,input_text_list,pos_extraction_list,collection_word_list,list_docs_retrived,prompt
0," Assume that a tree T has a number n
 s of “supply vertices” and all the other vertices are “demand vertices.” Each supply vertex is assigned a positive number called
 a supply, while each demand vertex is assigned a positive number called a demand. One wish to partition T into exactly n
 s subtrees by deleting edges from T so that each subtree contains exactly one supply vertex whose supply is no less than the sum of demands of all demand vertices
 in the subtree. The “partition problem” is a decision problem to ask whether T has such a partition. The “maximum partition problem” is an optimization version of the partition problem. In this paper,
 we give three algorithms for the problems. First is a linear-time algorithm for the partition problem. Second is a pseudo-polynomial-time
 algorithm for the maximum partition problem. Third is a fully polynomial-time approximation scheme (FPTAS) for the maximum
 partition problem.
 ","['n', 'other', 'positive', 'positive', 'n', 'less', 'maximum', 'optimization', 'lineartime', 'pseudopolynomialtime', 'maximum', 'polynomialtime', 'maximum']","['A', 'the', 'balanced', 'better', 'key', 'unate', 'balanced', 'better', 'key', 'unate', 'A', 'almost', 'better', 'than', 'without', 'Maximum', 'maximal', 'maximum', 'total', 'algorithm', 'Lineartime', 'lineartime', 'seriesparallel', 'Approximability', 'PolynomialTime', 'nonhamiltonian', 'vertexrankings', 'Maximum', 'maximal', 'maximum', 'total', 'Approximability', 'PolynomialTime', 'nonhamiltonian', 'vertexrankings', 'Maximum', 'maximal', 'maximum', 'total']","['A 1-tough nonhamiltonian maximal planar graph', 'A better than “best possible” algorithm to edge color multigraphs']","Paraphrase the following paper title without adding any other information: "" Assume that a tree T has a number n
 s of “supply vertices” and all the other vertices are “demand vertices.” Each supply vertex is assigned a positive number called
 a supply, while each demand vertex is assigned a positive number called a demand. One wish to partition T into exactly n
 s subtrees by deleting edges from T so that each subtree contains exactly one supply vertex whose supply is no less than the sum of demands of all demand vertices
 in the subtree. The “partition problem” is a decision problem to ask whether T has such a partition. The “maximum partition problem” is an optimization version of the partition problem. In this paper,
 we give three algorithms for the problems. First is a linear-time algorithm for the partition problem. Second is a pseudo-polynomial-time
 algorithm for the maximum partition problem. Third is a fully polynomial-time approximation scheme (FPTAS) for the maximum
 partition problem.
 "" based on the user's previous titles: ""['A 1-tough nonhamiltonian maximal planar graph', 'A better than “best possible” algorithm to edge color multigraphs']""."
1," In our earlier work, we found that feature space induced by tactile receptive fields (TRFs) are better than that by visual receptive fields (VRFs) in texture boundary detection tasks. This suggests that TRFs could be intimately associated with texture-like input. In this paper, we investigate how TRFs can develop in a cortical learning context. Our main hypothesis is that TRFs can be self-organized using the same cortical development mechanism found in the visual cortex, simply by exposing it to texture-like inputs (as opposed to natural-scene-like inputs). To test our hypothesis, we used the LISSOM model of visual cortical development. Our main results show that texture-like inputs lead to the self-organization of TRFs while natural-scene-like inputs lead to VRFs. These results suggest that TRFs can better represent texture than VRFs. We further analyzed the effectiveness of TRFs in representing texture, using kernel Fisher discriminant (KFD) and the results, along with texture classification performance, confirm that this is indeed the case. We expect these results to help us better understand the nature of texture, as a fundamentally tactile property.","['earlier', 'feature', 'tactile', 'receptive', 'better', 'visual', 'receptive', 'boundary', 'texturelike', 'cortical', 'main', 'selforganized', 'same', 'cortical', 'visual', 'texturelike', 'naturalscenelike', 'visual', 'cortical', 'main', 'texturelike', 'naturalscenelike', 'texture', 'better', 'tactile']","['than', 'the', 'functions', 'key', 'set', 'linear', 'and', 'best', 'better', 'optimal', 'than', 'color', 'depth', 'drawings', 'and', 'bounded', 'bounds', 'contours', 'edge', 'edgedisjoint', 'multicolorings', 'vertexrankings', 'planar', 'key', 'the', '4connected', 'Triconnected', 'lineartime', 'the', 'planar', 'color', 'depth', 'drawings', 'edgedisjoint', 'multicolorings', 'vertexrankings', 'SeriesParallel', 'degreeconstrained', 'edgedisjoint', 'fourconnected', 'color', 'depth', 'drawings', 'planar', 'key', 'the', 'edgedisjoint', 'multicolorings', 'vertexrankings', 'SeriesParallel', 'degreeconstrained', 'edgedisjoint', 'fourconnected', 'color', 'complexity', 'contours', 'depth', 'best', 'better', 'optimal', 'than', 'linear']","['Analogical cascade: a theory on the role of the thalamo-cortical loop in brain function.', 'Extrapolative Delay Compensation Through Facilitating Synapses and Its Relation to the Flash-Lag Effect']","Paraphrase the following paper title without adding any other information: "" In our earlier work, we found that feature space induced by tactile receptive fields (TRFs) are better than that by visual receptive fields (VRFs) in texture boundary detection tasks. This suggests that TRFs could be intimately associated with texture-like input. In this paper, we investigate how TRFs can develop in a cortical learning context. Our main hypothesis is that TRFs can be self-organized using the same cortical development mechanism found in the visual cortex, simply by exposing it to texture-like inputs (as opposed to natural-scene-like inputs). To test our hypothesis, we used the LISSOM model of visual cortical development. Our main results show that texture-like inputs lead to the self-organization of TRFs while natural-scene-like inputs lead to VRFs. These results suggest that TRFs can better represent texture than VRFs. We further analyzed the effectiveness of TRFs in representing texture, using kernel Fisher discriminant (KFD) and the results, along with texture classification performance, confirm that this is indeed the case. We expect these results to help us better understand the nature of texture, as a fundamentally tactile property."" based on the user's previous titles: ""['Analogical cascade: a theory on the role of the thalamo-cortical loop in brain function.', 'Extrapolative Delay Compensation Through Facilitating Synapses and Its Relation to the Flash-Lag Effect']""."
2," Clock distribution networks (CDNs) are costly in high-performance ASICs. This paper proposes a new approach: splitting clock domains at a very fine level, down to the level of a handful of gates. Each domain is synchronized with an inexpensive clock signal, generated locally. This is possible by adopting the paradigm of stochastic computation, where signal values are encoded as random bit streams. The design method is illustrated with the synthesis of circuits for applications in signal and image processing.","['costly', 'highperformance', 'new', 'fine', 'inexpensive', 'possible', 'stochastic', 'signal', 'random']","['Minimizing', 'necessary', 'possible', 'than', 'tradeoffs', 'Efficient', 'compact', 'computing', 'the', 'almost', 'balanced', 'better', 'Efficient', 'Simple', 'best', 'compact', 'best', 'necessary', 'optimal', 'possible', 'combinatorial', 'linear', 'circuits', 'threshold', 'combinatorial', 'linear', 'undirected']","['Efficient Use of Dynamically tagged Directories Through Compiler Analysis', 'Power and Area Efficient Sorting Networks Using Unary Processing']","Paraphrase the following paper title without adding any other information: "" Clock distribution networks (CDNs) are costly in high-performance ASICs. This paper proposes a new approach: splitting clock domains at a very fine level, down to the level of a handful of gates. Each domain is synchronized with an inexpensive clock signal, generated locally. This is possible by adopting the paradigm of stochastic computation, where signal values are encoded as random bit streams. The design method is illustrated with the synthesis of circuits for applications in signal and image processing."" based on the user's previous titles: ""['Efficient Use of Dynamically tagged Directories Through Compiler Analysis', 'Power and Area Efficient Sorting Networks Using Unary Processing']""."
3," Myerson's graph-restricted games are a well-known formalism for modeling cooperation that's subject to restrictions. In particular, Myerson considered a coalitional game in which cooperation is possible only through an underlying network of links between agents. A unique fair solution concept for graph-restricted games is called the Myerson value. One study generalized these results by considering probabilistic graphs in which agents can cooperate via links only to some extent, that is, with some probability. The authors' algorithm is based on the enumeration of all connected subgraphs in the graph. As a sample application of the new algorithm, they consider a probabilistic graph that represents likelihood of pairwise collaboration between political parties before the 2015 general elections in the UK.","['graphrestricted', 'wellknown', 'subject', 'particular', 'coalitional', 'possible', 'underlying', 'unique', 'fair', 'graphrestricted', 'probabilistic', 'sample', 'new', 'probabilistic', 'political', 'general']","['4Connected', '4connected', 'MultipleValued', 'seriesparallel', 'Various', 'best', 'colorings', 'combinatorial', 'areas', 'the', 'under', 'without', 'the', 'Hamiltonian', 'bipartition', 'combinatorial', 'matchings', 'best', 'necessary', 'optimal', 'possible', 'complexity', 'problem', 'problems', 'balanced', 'best', 'optimal', 'balanced', 'best', 'better', 'sufficient', '4Connected', '4connected', 'MultipleValued', 'seriesparallel', 'algorithm', 'algorithms', 'combinatorial', 'graph', 'partial', 'size', 'the', 'algorithm', 'algorithms', 'combinatorial', 'Hamiltonian', 'family', 'networks', 'power', 'generalized', 'the']","['Strategy/false-name proof protocols for combinatorial multi-attribute procurement auction: handling arbitrary utility of the buyer', 'Towards a combinatorial auction protocol among experts and amateurs: the case of single-skilled experts']","Paraphrase the following paper title without adding any other information: "" Myerson's graph-restricted games are a well-known formalism for modeling cooperation that's subject to restrictions. In particular, Myerson considered a coalitional game in which cooperation is possible only through an underlying network of links between agents. A unique fair solution concept for graph-restricted games is called the Myerson value. One study generalized these results by considering probabilistic graphs in which agents can cooperate via links only to some extent, that is, with some probability. The authors' algorithm is based on the enumeration of all connected subgraphs in the graph. As a sample application of the new algorithm, they consider a probabilistic graph that represents likelihood of pairwise collaboration between political parties before the 2015 general elections in the UK."" based on the user's previous titles: ""['Strategy/false-name proof protocols for combinatorial multi-attribute procurement auction: handling arbitrary utility of the buyer', 'Towards a combinatorial auction protocol among experts and amateurs: the case of single-skilled experts']""."
4," User-based collaborative filtering systems suggest interesting items to a user relying on similar-minded people called neighbors. The selection and weighting of these neighbors characterize the different recommendation approaches. While standard strategies perform a neighbor selection based on user similarities, trust-aware recommendation algorithms rely on other aspects indicative of user trust and reliability. In this article we restate the trust-aware recommendation problem, generalizing it in terms of performance prediction techniques, whose goal is to predict the performance of an information retrieval system in response to a particular query. We investigate how to adopt the preceding generalization to define a unified framework where we conduct an objective analysis of the effectiveness (predictive power) of neighbor scoring functions. The proposed framework enables discriminating whether recommendation performance improvements are caused by the used neighbor scoring functions or by the ways these functions are used in the recommendation computation. We evaluated our approach with several state-of-the-art and novel neighbor scoring functions on three publicly available datasets. By empirically comparing four neighbor quality metrics and thirteen performance predictors, we found strong predictive power for some of the predictors with respect to certain metrics. This result was then validated by checking the final performance of recommendation strategies where predictors are used for selecting and/or weighting user neighbors. As a result, we have found that, by measuring the predictive power of neighbor performance predictors, we are able to anticipate which predictors are going to perform better in neighbor-scoring-powered versions of a user-based collaborative filtering algorithm.","['Userbased', 'collaborative', 'interesting', 'similarminded', 'different', 'standard', 'user', 'trustaware', 'other', 'indicative', 'user', 'trustaware', 'particular', 'unified', 'objective', 'predictive', 'used', 'several', 'stateoftheart', 'novel', 'available', 'thirteen', 'strong', 'predictive', 'certain', 'final', 'andor', 'user', 'predictive', 'able', 'better', 'neighborscoringpowered', 'userbased', 'collaborative']","['Approximability', 'FourConnected', 'Multicommodity', 'MultipleValued', 'Sharing', 'balanced', 'sharing', 'And', 'Nicely', 'finding', 'possible', 'seriesparallel', 'Various', 'are', 'better', 'than', 'compact', 'protocols', 'regular', 'set', 'Web', 'algorithm', 'computing', 'ANDEXOR', 'Triconnected', 'lineartime', 'the', 'Given', 'are', 'is', 'Web', 'algorithm', 'computing', 'ANDEXOR', 'Triconnected', 'lineartime', 'the', 'balanced', 'generalized', 'uniform', 'necessary', 'optimal', 'possible', 'sufficient', 'algorithm', 'algorithms', 'for', 'using', 'Various', 'consecutive', 'the', 'Improvements', 'Sufficient', 'algorithms', 'computing', 'Characterization', 'algorithm', 'characterization', 'combinatorial', 'are', 'for', 'necessary', 'possible', 'Four', 'almost', 'consecutive', 'total', 'a', 'balanced', 'sufficient', 'algorithm', 'algorithms', 'the', 'the', 'and', 'Web', 'algorithm', 'computing', 'algorithm', 'algorithms', 'to', 'best', 'better', 'optimal', 'than', 'Approximability', 'EdgeRanking', 'MultipleValued', 'Triconnected', 'Approximability', 'Subtrees', 'vertexrankings', 'Sharing', 'balanced', 'sharing']","['Predicting the performance of recommender systems: an information theoretic approach', 'Alleviating the new user problem in collaborative filtering by exploiting personality information.']","Paraphrase the following paper title without adding any other information: "" User-based collaborative filtering systems suggest interesting items to a user relying on similar-minded people called neighbors. The selection and weighting of these neighbors characterize the different recommendation approaches. While standard strategies perform a neighbor selection based on user similarities, trust-aware recommendation algorithms rely on other aspects indicative of user trust and reliability. In this article we restate the trust-aware recommendation problem, generalizing it in terms of performance prediction techniques, whose goal is to predict the performance of an information retrieval system in response to a particular query. We investigate how to adopt the preceding generalization to define a unified framework where we conduct an objective analysis of the effectiveness (predictive power) of neighbor scoring functions. The proposed framework enables discriminating whether recommendation performance improvements are caused by the used neighbor scoring functions or by the ways these functions are used in the recommendation computation. We evaluated our approach with several state-of-the-art and novel neighbor scoring functions on three publicly available datasets. By empirically comparing four neighbor quality metrics and thirteen performance predictors, we found strong predictive power for some of the predictors with respect to certain metrics. This result was then validated by checking the final performance of recommendation strategies where predictors are used for selecting and/or weighting user neighbors. As a result, we have found that, by measuring the predictive power of neighbor performance predictors, we are able to anticipate which predictors are going to perform better in neighbor-scoring-powered versions of a user-based collaborative filtering algorithm."" based on the user's previous titles: ""['Predicting the performance of recommender systems: an information theoretic approach', 'Alleviating the new user problem in collaborative filtering by exploiting personality information.']""."
5," We have developed a deductive database system PACADE for analyzing 3-D and secondary structures of protein. The PACADE system consists of a relational database created from Protein Data Bank and a deductive engine DEE based on logic programming. It has the following features: (1) The system has an inference mechanism. This means by which users can easily write and check biological hypotheses using logical and declarative rules instead of procedural programs. (2) The relational database of the PACADE system stores data on both 3-D and secondary structures of protein. The integration of this two level structure makes feasible an abstract representation of the protein structure. We describe herein the design, functions, and implementation of this PACADE system.","['deductive', '3D', 'secondary', 'relational', 'deductive', 'logic', 'following', 'biological', 'logical', 'declarative', 'procedural', 'relational', '3D', 'secondary', 'feasible', 'abstract']","['Boolean', 'combinatorial', 'computability', 'linear', 'planar', 'key', 'sufficient', 'supply', 'Boolean', 'Cardinality', 'Hierarchical', 'cardinality', 'computability', 'Boolean', 'combinatorial', 'computability', 'linear', 'Boolean', 'Logic', 'circuits', 'combinatorial', 'the', 'characterization', 'combinatorial', 'decomposition', 'family', 'undirected', 'Logic', 'necessary', 'parallel', 'possible', 'Boolean', 'cardinality', 'computability', 'generalized', 'reducible', 'characterization', 'combinatorial', 'complexity', 'protocols', 'Boolean', 'Cardinality', 'Hierarchical', 'cardinality', 'computability', 'planar', 'key', 'sufficient', 'supply', 'necessary', 'optimal', 'possible', 'sufficient', 'Abstract', 'reducible']","['Development of the Overlapping Oligonucleotide Database and its application to signal sequence search of the human genome.', 'Bridging the Knowledge Gap between Research and Education through Textbooks']","Paraphrase the following paper title without adding any other information: "" We have developed a deductive database system PACADE for analyzing 3-D and secondary structures of protein. The PACADE system consists of a relational database created from Protein Data Bank and a deductive engine DEE based on logic programming. It has the following features: (1) The system has an inference mechanism. This means by which users can easily write and check biological hypotheses using logical and declarative rules instead of procedural programs. (2) The relational database of the PACADE system stores data on both 3-D and secondary structures of protein. The integration of this two level structure makes feasible an abstract representation of the protein structure. We describe herein the design, functions, and implementation of this PACADE system."" based on the user's previous titles: ""['Development of the Overlapping Oligonucleotide Database and its application to signal sequence search of the human genome.', 'Bridging the Knowledge Gap between Research and Education through Textbooks']""."
6," Machine learning is an increasingly used computational tool within human-computer interaction research. While most researchers currently utilize an iterative approach to refining classifier models and performance, we propose that ensemble classification techniques may be a viable and even preferable alternative. In ensemble learning, algorithms combine multiple classifiers to build one that is superior to its components. In this paper, we present EnsembleMatrix, an interactive visualization system that presents a graphical view of confusion matrices to help users understand relative merits of various classifiers. EnsembleMatrix allows users to directly interact with the visualizations in order to explore and build combination models. We evaluate the efficacy of the system and the approach in a user study. Results show that users are able to quickly combine multiple classifiers operating on multiple feature sets to produce an ensemble classifier with accuracy that approaches best-reported performance classifying images in the CalTech-101 dataset.","['used', 'computational', 'humancomputer', 'most', 'iterative', 'classifier', 'ensemble', 'viable', 'preferable', 'ensemble', 'algorithms', 'superior', 'interactive', 'graphical', 'relative', 'various', 'able', 'multiple', 'multiple', 'ensemble', 'bestreported']","['for', 'using', 'algorithms', 'computability', 'computing', 'Multicommodity', 'PolynomialTime', 'edgedisjoint', 'nonhamiltonian', 'best', 'the', 'algorithm', 'algorithm', 'Hamiltonian', 'Pair', 'characterization', 'uniform', 'necessary', 'optimal', 'possible', 'sufficient', 'better', 'necessary', 'optimal', 'possible', 'simply', 'Hamiltonian', 'Pair', 'characterization', 'uniform', 'algorithm', 'algorithms', 'best', 'better', 'optimal', 'than', 'linear', 'graphs', 'Given', 'approximation', 'optimal', 'size', 'Various', 'the', 'to', 'Various', 'consecutive', 'parallel', 'Various', 'consecutive', 'parallel', 'Hamiltonian', 'Pair', 'characterization', 'uniform', '4connected', 'MultipleValued', 'Triconnected', 'degreeconstrained']","['The best of two worlds: merging virtual and real for face to face collaboration', 'SoundWave: using the doppler effect to sense gestures']","Paraphrase the following paper title without adding any other information: "" Machine learning is an increasingly used computational tool within human-computer interaction research. While most researchers currently utilize an iterative approach to refining classifier models and performance, we propose that ensemble classification techniques may be a viable and even preferable alternative. In ensemble learning, algorithms combine multiple classifiers to build one that is superior to its components. In this paper, we present EnsembleMatrix, an interactive visualization system that presents a graphical view of confusion matrices to help users understand relative merits of various classifiers. EnsembleMatrix allows users to directly interact with the visualizations in order to explore and build combination models. We evaluate the efficacy of the system and the approach in a user study. Results show that users are able to quickly combine multiple classifiers operating on multiple feature sets to produce an ensemble classifier with accuracy that approaches best-reported performance classifying images in the CalTech-101 dataset."" based on the user's previous titles: ""['The best of two worlds: merging virtual and real for face to face collaboration', 'SoundWave: using the doppler effect to sense gestures']""."
7," We consider the problem of belief propagation in a network of communicating agents, modeled in the recently introduced Belief Revision Game (BRG) framework. In this setting, each agent expresses her belief through a propositional formula and revises her own belief at each step by considering the beliefs of her acquaintances, using belief change tools. In this paper, we investigate the extent to which BRGs satisfy some monotonicity properties, i.e., whether promoting some desired piece of belief to a given set of agents is actually always useful for making it accepted by all of them. We formally capture such a concept of promotion by a new family of belief change operators. We show that some basic monotonicity properties are not satisfied by BRGs in general, even when the agents merging-based revision policies are fully rational (in the AGM sense). We also identify some classes where they hold.","['propositional', 'own', 'belief', 'desired', 'useful', 'new', 'basic', 'general', 'mergingbased', 'rational']","['Boolean', 'cardinality', 'computability', 'generalized', 'reducible', 'into', 'the', 'demand', 'power', 'simply', 'the', 'necessary', 'optimal', 'possible', 'sufficient', 'better', 'finding', 'necessary', 'possible', 'sufficient', 'the', 'functions', 'key', 'necessary', 'generalized', 'the', 'MultipleValued', 'fourconnected', 'seriesparallel', 'approximation', 'balanced', 'reducible', 'undirected']","['On the Logic of Merging', 'Reasoning under inconsistency: the forgotten connective']","Paraphrase the following paper title without adding any other information: "" We consider the problem of belief propagation in a network of communicating agents, modeled in the recently introduced Belief Revision Game (BRG) framework. In this setting, each agent expresses her belief through a propositional formula and revises her own belief at each step by considering the beliefs of her acquaintances, using belief change tools. In this paper, we investigate the extent to which BRGs satisfy some monotonicity properties, i.e., whether promoting some desired piece of belief to a given set of agents is actually always useful for making it accepted by all of them. We formally capture such a concept of promotion by a new family of belief change operators. We show that some basic monotonicity properties are not satisfied by BRGs in general, even when the agents merging-based revision policies are fully rational (in the AGM sense). We also identify some classes where they hold."" based on the user's previous titles: ""['On the Logic of Merging', 'Reasoning under inconsistency: the forgotten connective']""."
8," A new active set algorithm (ASA) for large-scale box constrained optimization is introduced. The algorithm consists of a nonmonotone gradient projection step, an unconstrained optimization step, and a set of rules for switching between the two steps. Numerical experiments and comparisons are presented using box constrained problems in the CUTEr and MINPACK test problem libraries.","['new', 'active', 'largescale', 'algorithm', 'nonmonotone', 'unconstrained', 'Numerical']","['the', 'connected', 'designated', 'optimal', 'regular', 'generalized', 'parallel', 'partial', 'algorithm', 'BendOptimal', 'PolynomialTime', 'edgedisjoint', 'vertexrankings', 'bounded', 'generalized', 'orthogonal', 'undirected', 'Algorithms', 'Parametric']","['Two New Regularized Adaboost Algorithms', 'A sparse proximal implementation of the LP dual active set algorithm']","Paraphrase the following paper title without adding any other information: "" A new active set algorithm (ASA) for large-scale box constrained optimization is introduced. The algorithm consists of a nonmonotone gradient projection step, an unconstrained optimization step, and a set of rules for switching between the two steps. Numerical experiments and comparisons are presented using box constrained problems in the CUTEr and MINPACK test problem libraries."" based on the user's previous titles: ""['Two New Regularized Adaboost Algorithms', 'A sparse proximal implementation of the LP dual active set algorithm']""."
9," In this paper, we present the challenges involved in large-scale video transcoding application in public clouds. We introduce the architecture of an existing video transcoding system which is tightly coupled with an existing video sharing service. We examine the horizontal scalability of the video transcoding system on AWS EC2. With an online transaction processing (OLTP) model, the system achieves linear horizontal scalability up to 1,000 vCPU cores, but starts to experience performance degradation beyond that. We analyze the resource consumption pattern of the existing system, then introduce an improved architecture by adding a message queue layer. This effectively decouples the video transcoding system from the video sharing service and converts the OLTP model into a batch processing model. Large-scale evaluations on AWS EC2 indicate that the improved design maintains linear horizontal scalability at 10,100 vCPU cores. The hybrid design of the system allows it to be easily adapted for other batch processing use cases without the need to modify or recompile the application.","['largescale', 'public', 'horizontal', 'online', 'linear', 'improved', 'Largescale', 'improved', 'linear', 'horizontal', 'hybrid', 'other']","['generalized', 'parallel', 'partial', 'areas', 'demand', 'networks', 'secret', 'parallel', 'rectangle', 'rectangular', 'Web', 'best', 'exchange', 'networks', 'regular', 'linear', 'Improved', 'Improvements', 'better', 'optimal', 'Partitioning', 'Improved', 'Improvements', 'better', 'optimal', 'linear', 'parallel', 'rectangle', 'rectangular', 'compact', 'computing', 'planar', 'the']","['Resilient virtual communication networks using multi-commodity flow based local optimal mapping.', 'A parallel computing engine for a class of time critical processes.']","Paraphrase the following paper title without adding any other information: "" In this paper, we present the challenges involved in large-scale video transcoding application in public clouds. We introduce the architecture of an existing video transcoding system which is tightly coupled with an existing video sharing service. We examine the horizontal scalability of the video transcoding system on AWS EC2. With an online transaction processing (OLTP) model, the system achieves linear horizontal scalability up to 1,000 vCPU cores, but starts to experience performance degradation beyond that. We analyze the resource consumption pattern of the existing system, then introduce an improved architecture by adding a message queue layer. This effectively decouples the video transcoding system from the video sharing service and converts the OLTP model into a batch processing model. Large-scale evaluations on AWS EC2 indicate that the improved design maintains linear horizontal scalability at 10,100 vCPU cores. The hybrid design of the system allows it to be easily adapted for other batch processing use cases without the need to modify or recompile the application."" based on the user's previous titles: ""['Resilient virtual communication networks using multi-commodity flow based local optimal mapping.', 'A parallel computing engine for a class of time critical processes.']""."
10," The Redundant Arrays of Inexpensive DWS Nodes (RAIN) technique is a node-level data replication approach that introduces failover capabilities to DWS (Data Warehouse Striping) clusters. RAIN is based on the selective replication of fact tables' data across the cluster nodes and endows DWS clusters with the capability of providing query answers even when one or more nodes are unavailable. Two distinct replication modes are supported: simple redundancy (RAIN-0) and stripped redundancy (RAIN-S). In this demo we are going to show a DWS cluster using the RAIN technique, focusing on the execution of queries in the presence of nodes failures and on the process of recovering failed nodes.","['nodelevel', 'selective', 'more', 'unavailable', 'distinct', 'simple', 'failed']","['Approximability', 'edgedisjoint', 'edgerankings', 'vertexrankings', 'characterization', 'combinatorial', 'partial', 'partitioning', 'For', 'better', 'for', 'than', 'are', 'designated', 'necessary', 'possible', 'simply', 'are', 'key', 'orthogonal', 'parallel', 'Simple', 'compact', 'linear', 'simply', 'complete', 'necessary', 'sufficient', 'to']","['A data mining approach to identify key factors in dependability experiments', 'The Olap And Data Warehousing Approaches For Analysis And Sharing Of Results From Dependability Evaluation Experiments']","Paraphrase the following paper title without adding any other information: "" The Redundant Arrays of Inexpensive DWS Nodes (RAIN) technique is a node-level data replication approach that introduces failover capabilities to DWS (Data Warehouse Striping) clusters. RAIN is based on the selective replication of fact tables' data across the cluster nodes and endows DWS clusters with the capability of providing query answers even when one or more nodes are unavailable. Two distinct replication modes are supported: simple redundancy (RAIN-0) and stripped redundancy (RAIN-S). In this demo we are going to show a DWS cluster using the RAIN technique, focusing on the execution of queries in the presence of nodes failures and on the process of recovering failed nodes."" based on the user's previous titles: ""['A data mining approach to identify key factors in dependability experiments', 'The Olap And Data Warehousing Approaches For Analysis And Sharing Of Results From Dependability Evaluation Experiments']""."
11," With the rise of software complexity, software-related accidents represent a significant threat for computer-based systems. Software Fault Injection is a method to anticipate worst-case scenarios caused by faulty software through the deliberate injection of software faults. This survey provides a comprehensive overview of the state of the art on Software Fault Injection to support researchers and practitioners in the selection of the approach that best fits their dependability assessment goals, and it discusses how these approaches have evolved to achieve fault representativeness, efficiency, and usability. The survey includes a description of relevant applications of Software Fault Injection in the context of fault-tolerant systems.","['softwarerelated', 'significant', 'computerbased', 'worstcase', 'deliberate', 'comprehensive', 'best', 'relevant', 'faulttolerant']","['4Connected', '4connected', 'Triconnected', 'fourconnected', 'key', 'necessary', 'sufficient', 'algorithms', 'computing', 'protocols', 'approximation', 'optimal', 'tradeoffs', 'unate', 'balanced', 'possible', 'simply', 'undirected', 'balanced', 'best', 'complete', 'partial', 'Best', 'best', 'better', 'possible', 'shortest', 'key', 'necessary', 'sufficient', '4connected', 'SiteOriented', 'edgedisjoint', 'vertexrankings']","['A data mining approach to identify key factors in dependability experiments', 'Towards Identifying the Best Variables for Failure Prediction Using Injection of Realistic Software Faults']","Paraphrase the following paper title without adding any other information: "" With the rise of software complexity, software-related accidents represent a significant threat for computer-based systems. Software Fault Injection is a method to anticipate worst-case scenarios caused by faulty software through the deliberate injection of software faults. This survey provides a comprehensive overview of the state of the art on Software Fault Injection to support researchers and practitioners in the selection of the approach that best fits their dependability assessment goals, and it discusses how these approaches have evolved to achieve fault representativeness, efficiency, and usability. The survey includes a description of relevant applications of Software Fault Injection in the context of fault-tolerant systems."" based on the user's previous titles: ""['A data mining approach to identify key factors in dependability experiments', 'Towards Identifying the Best Variables for Failure Prediction Using Injection of Realistic Software Faults']""."
12," Japanese kanji recognition experiments are typically narrowly focused, and feature only native speakers as participants. It remains unclear how to apply their results to kanji similarity applications, especially when learners are much more likely to make similarity-based confusion errors. We describe an experiment to collect authentic human similarity judgements from participants of all levels of Japanese proficiency, from non-speaker to native. The data was used to construct simple similarity models for kanji based on pixel difference and radical cosine similarity, in order to work towards genuine confusability data. The latter model proved the best predictor of human responses.","['Japanese', 'native', 'unclear', 'likely', 'similaritybased', 'authentic', 'human', 'Japanese', 'simple', 'radical', 'genuine', 'latter', 'best', 'human']","['Doughnut', 'Drawings', 'Octagonal', 'forests', 'trees', 'Given', 'is', 'possible', 'are', 'better', 'possible', 'simply', 'than', 'seriesparallel', 'balanced', 'best', 'complete', 'possible', 'approximation', 'complexity', 'face', 'power', 'reducible', 'Doughnut', 'Drawings', 'Octagonal', 'Simple', 'compact', 'linear', 'simply', 'combinatorial', 'partial', 'reducible', 'undirected', 'best', 'finding', 'possible', 'sufficient', 'the', 'Best', 'best', 'better', 'possible', 'shortest', 'approximation', 'complexity', 'face', 'power', 'reducible']","['Automatic satire detection: are you having a laugh?', 'Language identification: the long and the short of the matter']","Paraphrase the following paper title without adding any other information: "" Japanese kanji recognition experiments are typically narrowly focused, and feature only native speakers as participants. It remains unclear how to apply their results to kanji similarity applications, especially when learners are much more likely to make similarity-based confusion errors. We describe an experiment to collect authentic human similarity judgements from participants of all levels of Japanese proficiency, from non-speaker to native. The data was used to construct simple similarity models for kanji based on pixel difference and radical cosine similarity, in order to work towards genuine confusability data. The latter model proved the best predictor of human responses."" based on the user's previous titles: ""['Automatic satire detection: are you having a laugh?', 'Language identification: the long and the short of the matter']""."
13," Given an $N$-relay Gaussian Half-Duplex (HD) diamond network, can we achieve a significant fraction of the capacity of the full network by operating only a subset of $k$ relays? This paper seeks to answer this question, which is also known as the {it network simplification} problem. Building upon the recent result of Cardone et al., this work proves that there always exists a subnetwork of $k=N-1$ relays that approximately (i.e., up to a constant gap) achieves a fraction $frac{N-1}{N}$ of the capacity of the full network. This fraction guarantee is shown to be tight, i.e., there exists a class of Gaussian HD diamond networks with $N$ relays where the best (i.e., the one with the largest capacity) subnetwork of $k=N-1$ relays has an approximate capacity of $frac{N-1}{N}$ of the capacity of the full network. Moreover, it is shown that any optimal schedule of the full network can be used by at least one of the ${N}choose{k}$ subnetworks of $k$ relays to achieve a worst performance guarantee of $frac{k}{N}$. The key step of the proof lies in the derivation of properties of submodular functions, which provide a combinatorial handle of the network simplification problem in Gaussian HD diamond networks.","['Gaussian', 'significant', 'full', 'k', 'recent', 'kN1', 'constant', 'frac', 'N1', 'full', 'tight', 'Gaussian', 'best', 'largest', 'kN1', 'approximate', 'frac', 'N1', 'full', 'optimal', 'full', 'least', 'k', 'worst', 'frac', 'key', 'submodular', 'combinatorial']","['Generalized', 'approximation', 'key', 'necessary', 'sufficient', 'complete', 'maximum', 'partial', 'A', 'cardinality', 'treewidth', 'unate', 'Given', 'consecutive', 'the', 'Boolean', 'algorithm', 'functions', 'almost', 'linear', 'maximal', 'regular', 'Hamiltonian', 'symmetric', 'treewidth', 'unate', 'A', 'NC', 'On2', 'complete', 'maximum', 'partial', 'balanced', 'bounded', 'compact', 'corners', 'Generalized', 'approximation', 'Best', 'best', 'better', 'possible', 'shortest', 'best', 'shortest', 'the', 'Boolean', 'algorithm', 'functions', 'approximation', 'maximum', 'optimal', 'Hamiltonian', 'symmetric', 'treewidth', 'unate', 'A', 'NC', 'On2', 'complete', 'maximum', 'partial', 'Optimal', 'maximal', 'maximum', 'optimal', 'complete', 'maximum', 'partial', 'almost', 'than', 'the', 'A', 'cardinality', 'treewidth', 'unate', 'Best', 'best', 'better', 'possible', 'shortest', 'Hamiltonian', 'symmetric', 'treewidth', 'unate', 'Key', 'key', 'combinatorial', 'computability', 'multigraphs', 'treewidth', 'Boolean', 'Combinatorial', 'algorithms', 'combinatorial', 'computability']","['On the structure of approximately optimal schedules for half-duplex diamond networks.', 'A simple relaying strategy for diamond networks']","Paraphrase the following paper title without adding any other information: "" Given an $N$-relay Gaussian Half-Duplex (HD) diamond network, can we achieve a significant fraction of the capacity of the full network by operating only a subset of $k$ relays? This paper seeks to answer this question, which is also known as the {it network simplification} problem. Building upon the recent result of Cardone et al., this work proves that there always exists a subnetwork of $k=N-1$ relays that approximately (i.e., up to a constant gap) achieves a fraction $frac{N-1}{N}$ of the capacity of the full network. This fraction guarantee is shown to be tight, i.e., there exists a class of Gaussian HD diamond networks with $N$ relays where the best (i.e., the one with the largest capacity) subnetwork of $k=N-1$ relays has an approximate capacity of $frac{N-1}{N}$ of the capacity of the full network. Moreover, it is shown that any optimal schedule of the full network can be used by at least one of the ${N}choose{k}$ subnetworks of $k$ relays to achieve a worst performance guarantee of $frac{k}{N}$. The key step of the proof lies in the derivation of properties of submodular functions, which provide a combinatorial handle of the network simplification problem in Gaussian HD diamond networks."" based on the user's previous titles: ""['On the structure of approximately optimal schedules for half-duplex diamond networks.', 'A simple relaying strategy for diamond networks']""."
14," We consider large-scale networks with n nodes, out of which k are in possession, (e.g., have sensed or collected in some other way) k information packets. In the scenarios in which network nodes are vulnerable because of, for example, limited energy or a hostile environment, it is desirable to disseminate the acquired information throughout the network so that each of the n nodes stores one (possibly coded) packet and the original k source packets can be recovered later in a computationally simple way from any (1 + \varepsilon)k nodes for some small \varepsilon 0. We developed two distributed algorithms for solving this problem based on simple random walks and Fountain codes. Unlike all previously developed schemes, our solution is truly distributed, that is, nodes do not know n, k or connectivity in the network, except in their own neighborhoods, and they do not maintain any routing tables. In the first algorithm, all the sensors have the knowledge of n and k. In the second algorithm, each sensor estimates these parameters through the random walk dissemination. We present analysis of the communication/transmission and encoding/decoding complexity of these two algorithms, and provide extensive simulation results as well.","['largescale', 'n', 'other', 'vulnerable', 'limited', 'hostile', 'desirable', 'n', 'original', 'simple', 'small', 'simple', 'random', 'distributed', 'own', 'routing', 'first', 'second', 'present', 'extensive']","['generalized', 'parallel', 'partial', 'A', 'the', 'are', 'areas', 'face', 'necessary', 'possible', 'maximum', 'partial', 'sufficient', 'exchange', 'face', 'into', 'unate', 'necessary', 'optimal', 'possible', 'sufficient', 'A', 'the', 'Simple', 'compact', 'linear', 'simply', 'Small', 'a', 'size', 'Simple', 'compact', 'linear', 'simply', 'combinatorial', 'linear', 'undirected', 'Distribution', 'designated', 'networks', 'weighted', 'into', 'the', 'Routing', 'protocols', 'routing', 'the', 'the', 'the', 'complete', 'depth', 'partial']","['When do the availability codes make the stored data more available?', 'On the secrecy capacity of the space-division multiplexed fiber optical communication systems']","Paraphrase the following paper title without adding any other information: "" We consider large-scale networks with n nodes, out of which k are in possession, (e.g., have sensed or collected in some other way) k information packets. In the scenarios in which network nodes are vulnerable because of, for example, limited energy or a hostile environment, it is desirable to disseminate the acquired information throughout the network so that each of the n nodes stores one (possibly coded) packet and the original k source packets can be recovered later in a computationally simple way from any (1 + \varepsilon)k nodes for some small \varepsilon 0. We developed two distributed algorithms for solving this problem based on simple random walks and Fountain codes. Unlike all previously developed schemes, our solution is truly distributed, that is, nodes do not know n, k or connectivity in the network, except in their own neighborhoods, and they do not maintain any routing tables. In the first algorithm, all the sensors have the knowledge of n and k. In the second algorithm, each sensor estimates these parameters through the random walk dissemination. We present analysis of the communication/transmission and encoding/decoding complexity of these two algorithms, and provide extensive simulation results as well."" based on the user's previous titles: ""['When do the availability codes make the stored data more available?', 'On the secrecy capacity of the space-division multiplexed fiber optical communication systems']""."
15," Peer-to-peer file-sharing systems have poor search performance for rare or poorly described files. These files lack a quality or variety of metadata making them hard to match with queries. A server can alleviate this problem by gathering the descriptors used by peers via what we call probe queries, and use these descriptors to improve its own. We consider probe query triggering mechanisms and criteria for selecting a file for which to probe in this work. Experimental results indicate that probe queries are effective in improving search performance.","['Peertopeer', 'filesharing', 'poor', 'rare', 'hard', 'probe', 'own', 'probe', 'Experimental', 'effective']","['5coloring', 'Approximability', 'EdgeDisjoint', 'vertexrankings', 'bandwidth', 'computing', 'embedding', 'networks', 'sharing', 'better', 'for', 'problems', 'almost', 'are', 'best', 'possible', 'best', 'better', 'finding', 'simply', 'planar', 'into', 'the', 'planar', 'Abstract', 'Characterization', 'Efficient', 'necessary', 'optimal']","['Extracting information networks from the blogosphere', 'Parallel and distributed computing for data mining']","Paraphrase the following paper title without adding any other information: "" Peer-to-peer file-sharing systems have poor search performance for rare or poorly described files. These files lack a quality or variety of metadata making them hard to match with queries. A server can alleviate this problem by gathering the descriptors used by peers via what we call probe queries, and use these descriptors to improve its own. We consider probe query triggering mechanisms and criteria for selecting a file for which to probe in this work. Experimental results indicate that probe queries are effective in improving search performance."" based on the user's previous titles: ""['Extracting information networks from the blogosphere', 'Parallel and distributed computing for data mining']""."
16, Designing revenue optimal auctions for selling an item to $n$ symmetric bidders is a fundamental problem in mechanism design. Myerson (1981) shows that the second price auction with an appropriate reserve price is optimal when bidders' values are drawn i.i.d. from a known regular distribution. A cornerstone in the prior-independent revenue maximization literature is a result by Bulow and Klemperer (1996) showing that the second price auction without a reserve achieves (n-1)/n of the optimal revenue in the worst case. We construct a randomized mechanism that strictly outperforms the second price auction in this setting. Our mechanism inflates the second highest bid with a probability that varies with $n$. For two bidders we improve the performance guarantee from 0.5 to 0.512 of the optimal revenue. We also resolve a question in the design of revenue optimal mechanisms that have access to a single sample from an unknown distribution. We show that a randomized mechanism strictly outperforms all deterministic mechanisms in terms of worst case guarantee.,"['optimal', 'n', 'symmetric', 'fundamental', 'second', 'appropriate', 'optimal', 'iid', 'regular', 'priorindependent', 'second', 'n1', 'optimal', 'worst', 'randomized', 'second', 'second', 'highest', 'n', 'optimal', 'optimal', 'single', 'unknown', 'randomized', 'deterministic', 'worst']","['Optimal', 'maximal', 'maximum', 'optimal', 'A', 'orthogonal', 'planar', 'symmetric', 'key', 'necessary', 'problem', 'the', 'necessary', 'optimal', 'possible', 'sufficient', 'Optimal', 'maximal', 'maximum', 'optimal', 'Eulerian', 'Generalized', 'cardinality', 'symmetric', 'unate', 'consecutive', 'regular', 'uniform', 'Multicommodity', 'edgedisjoint', 'vertexrankings', 'the', 'Combinatorial', 'matchings', 'orthogonal', 'treewidth', 'unate', 'Optimal', 'maximal', 'maximum', 'optimal', 'Best', 'best', 'better', 'possible', 'shortest', 'algorithm', 'symmetric', 'the', 'the', 'best', 'maximal', 'maximum', 'shortest', 'A', 'Optimal', 'maximal', 'maximum', 'optimal', 'Optimal', 'maximal', 'maximum', 'optimal', 'a', 'Given', 'Various', 'possible', 'secret', 'algorithm', 'symmetric', 'combinatorial', 'linear', 'Best', 'best', 'better', 'possible', 'shortest']","['How to approximate optimal auctions', 'A Simple and Approximately Optimal Mechanism for an Additive Buyer']","Paraphrase the following paper title without adding any other information: "" Designing revenue optimal auctions for selling an item to $n$ symmetric bidders is a fundamental problem in mechanism design. Myerson (1981) shows that the second price auction with an appropriate reserve price is optimal when bidders' values are drawn i.i.d. from a known regular distribution. A cornerstone in the prior-independent revenue maximization literature is a result by Bulow and Klemperer (1996) showing that the second price auction without a reserve achieves (n-1)/n of the optimal revenue in the worst case. We construct a randomized mechanism that strictly outperforms the second price auction in this setting. Our mechanism inflates the second highest bid with a probability that varies with $n$. For two bidders we improve the performance guarantee from 0.5 to 0.512 of the optimal revenue. We also resolve a question in the design of revenue optimal mechanisms that have access to a single sample from an unknown distribution. We show that a randomized mechanism strictly outperforms all deterministic mechanisms in terms of worst case guarantee."" based on the user's previous titles: ""['How to approximate optimal auctions', 'A Simple and Approximately Optimal Mechanism for an Additive Buyer']""."
17," In a distributed processing environment, the static placement of query operators may result in unsatisfactory system performance due to unpredictable factors such as changes of servers' load, data arrival rates, etc The problem is exacerbated for continuous (and long running) monitoring queries over data streams as any suboptimal placement will affect the system for a very long time In this paper, we formalize and analyze the operator placement problem in the context of a locally distributed continuous query system We also propose a solution, that is asynchronous and local, to dynamically manage the load across the system nodes Essentially, during runtime, we migrate query operators/fragments from overloaded nodes to lightly loaded ones to achieve better performance Heuristics are also proposed to maintain good data flow locality Results of a performance study shows the effectiveness of our technique.","['distributed', 'static', 'unsatisfactory', 'due', 'unpredictable', 'such', 'continuous', 'suboptimal', 'long', 'continuous', 'asynchronous', 'local', 'query', 'better', 'good']","['Distribution', 'designated', 'networks', 'weighted', 'linear', 'necessary', 'optimal', 'problems', 'Given', 'the', 'Given', 'complexity', 'optimal', 'possible', 'undirected', 'are', 'necessary', 'possible', 'without', 'linear', 'Optimal', 'better', 'optimal', 'tradeoffs', 'for', 'shortest', 'linear', 'Boolean', 'combinatorial', 'parallel', 'symmetric', 'the', 'Boolean', 'algorithm', 'cardinality', 'best', 'better', 'optimal', 'than', 'best', 'better', 'for', 'necessary', 'sufficient']","['Sense the physical, walkthrough the virtual, manage the co (existing) spaces: a database perspective', 'Query optimization for massively parallel data processing']","Paraphrase the following paper title without adding any other information: "" In a distributed processing environment, the static placement of query operators may result in unsatisfactory system performance due to unpredictable factors such as changes of servers' load, data arrival rates, etc The problem is exacerbated for continuous (and long running) monitoring queries over data streams as any suboptimal placement will affect the system for a very long time In this paper, we formalize and analyze the operator placement problem in the context of a locally distributed continuous query system We also propose a solution, that is asynchronous and local, to dynamically manage the load across the system nodes Essentially, during runtime, we migrate query operators/fragments from overloaded nodes to lightly loaded ones to achieve better performance Heuristics are also proposed to maintain good data flow locality Results of a performance study shows the effectiveness of our technique."" based on the user's previous titles: ""['Sense the physical, walkthrough the virtual, manage the co (existing) spaces: a database perspective', 'Query optimization for massively parallel data processing']""."
18," We propose a technique to control the temporal noise present in sketchy animations. Given an input animation drawn digitally, our approach works by combining motion extraction and inbetweening techniques to generate a reduced-noise sketchy animation registered to the input animation. The amount of noise is then controlled by a continuous parameter value. Our method can be applied to effectively reduce the temporal noise present in sequences of sketches to a desired rate, while preserving the geometric richness of the sketchy style in each frame. This provides the manipulation of temporal noise as an additional artistic parameter, e.g. to emphasize character emotions and scene atmosphere, and enables the display of sketchy content to broader audiences by producing animations with comfortable noise levels. We demonstrate the effectiveness of our approach on a series of rough hand-drawn animations.","['temporal', 'sketchy', 'reducednoise', 'continuous', 'temporal', 'desired', 'geometric', 'sketchy', 'temporal', 'additional', 'artistic', 'sketchy', 'comfortable', 'rough', 'handdrawn']","['linear', 'drawings', 'edgedisjoint', 'multicolorings', 'vertexrankings', 'linear', 'linear', 'necessary', 'optimal', 'possible', 'sufficient', 'linear', 'drawings', 'linear', 'for', 'necessary', 'sufficient', 'drawings', 'drawings', 'balanced', 'better', 'compact', 'optimal', 'approximation', 'better', 'contours', 'corners', 'edge', 'drawings']","['Locomotion skills for simulated quadrupeds', 'Smart Scribbles for Sketch Segmentation']","Paraphrase the following paper title without adding any other information: "" We propose a technique to control the temporal noise present in sketchy animations. Given an input animation drawn digitally, our approach works by combining motion extraction and inbetweening techniques to generate a reduced-noise sketchy animation registered to the input animation. The amount of noise is then controlled by a continuous parameter value. Our method can be applied to effectively reduce the temporal noise present in sequences of sketches to a desired rate, while preserving the geometric richness of the sketchy style in each frame. This provides the manipulation of temporal noise as an additional artistic parameter, e.g. to emphasize character emotions and scene atmosphere, and enables the display of sketchy content to broader audiences by producing animations with comfortable noise levels. We demonstrate the effectiveness of our approach on a series of rough hand-drawn animations."" based on the user's previous titles: ""['Locomotion skills for simulated quadrupeds', 'Smart Scribbles for Sketch Segmentation']""."
19," In this paper, we study Resource Constrained Best Upgrade Plan (BUP) computation in road network databases. Consider a transportation network (weighted graph) G where a subset of the edges are upgradable, i.e., for each such edge there is a cost, which if spent, the weight of the edge can be reduced to a specific new value. In the single-pair version of BUP, the input includes a source and a destination in G, and a budget B (resource constraint). The goal is to identify which upgradable edges should be upgraded so that the shortest path distance between source and destination (in the updated network) is minimized, without exceeding the available budget for the upgrade. In the multiple-pair version of BUP, a set Q of source-destination pairs is given, and the problem is to choose for upgrade those edges that lead to the smallest sum of shortest path distances across all pairs in Q, subject to budget constraint B. In addition to transportation networks, the BUP query arises in other domains too, such as telecommunications. We propose a framework for BUP processing and evaluate it with experiments on large, real road networks.","['weighted', 'upgradable', 'such', 'specific', 'new', 'singlepair', 'upgradable', 'shortest', 'updated', 'available', 'multiplepair', 'upgrade', 'smallest', 'shortest', 'subject', 'other', 'such', 'large', 'real']","['Weighted', 'weighted', 'Planar', 'compact', 'tradeoffs', 'are', 'necessary', 'possible', 'without', 'key', 'optimal', 'the', 'Noncrossing', 'fivecoloring', 'nonhamiltonian', 'Planar', 'compact', 'tradeoffs', 'Shortest', 'best', 'possible', 'shortest', 'Improved', 'Revised', 'balanced', 'complete', 'regular', 'are', 'for', 'necessary', 'possible', 'MultipleValued', 'fivecoloring', 'seriesparallel', 'Improved', 'Improvements', 'better', 'complete', 'necessary', 'best', 'compact', 'shortest', 'size', 'Shortest', 'best', 'possible', 'shortest', 'areas', 'the', 'under', 'without', 'the', 'are', 'necessary', 'possible', 'without', 'Small', 'a', 'size', 'best', 'better', 'possible', 'problem']","['Efficient verification of shortest path search via authenticated hints', 'Strong location privacy: A case study on shortest path queries.']","Paraphrase the following paper title without adding any other information: "" In this paper, we study Resource Constrained Best Upgrade Plan (BUP) computation in road network databases. Consider a transportation network (weighted graph) G where a subset of the edges are upgradable, i.e., for each such edge there is a cost, which if spent, the weight of the edge can be reduced to a specific new value. In the single-pair version of BUP, the input includes a source and a destination in G, and a budget B (resource constraint). The goal is to identify which upgradable edges should be upgraded so that the shortest path distance between source and destination (in the updated network) is minimized, without exceeding the available budget for the upgrade. In the multiple-pair version of BUP, a set Q of source-destination pairs is given, and the problem is to choose for upgrade those edges that lead to the smallest sum of shortest path distances across all pairs in Q, subject to budget constraint B. In addition to transportation networks, the BUP query arises in other domains too, such as telecommunications. We propose a framework for BUP processing and evaluate it with experiments on large, real road networks."" based on the user's previous titles: ""['Efficient verification of shortest path search via authenticated hints', 'Strong location privacy: A case study on shortest path queries.']""."
20," In data publishing, the owner delegates the role of satisfying user queries to a third-party publisher. As the publisher may be untrusted or susceptible to attacks, it could produce incorrect query results. In this paper, we introduce a scheme for users to verify that their query results are complete (i.e., no qualifying tuples are omitted) and authentic (i.e., all the result values originated from the owner). The scheme supports range selection on key and non-key attributes, project as well as join queries on relational databases. Moreover, the proposed scheme complies with access control policies, is computationally secure, and can be implemented efficiently.","['user', 'thirdparty', 'susceptible', 'incorrect', 'complete', 'authentic', 'key', 'nonkey', 'relational', 'secure']","['Web', 'algorithm', 'computing', 'On2', 'Web', 'networks', 'protocols', 'are', 'reducible', 'than', 'without', 'partial', 'problem', 'simply', 'Partial', 'complete', 'partial', 'total', 'balanced', 'best', 'complete', 'possible', 'Key', 'key', 'Boolean', 'Cardinality', 'Subgraphs', 'cardinality', 'key', 'Boolean', 'Cardinality', 'Hierarchical', 'cardinality', 'computability', 'Secure', 'connected', 'necessary', 'sufficient']","['Explaining inferences in Bayesian networks', 'Secure server-aided top-k monitoring.']","Paraphrase the following paper title without adding any other information: "" In data publishing, the owner delegates the role of satisfying user queries to a third-party publisher. As the publisher may be untrusted or susceptible to attacks, it could produce incorrect query results. In this paper, we introduce a scheme for users to verify that their query results are complete (i.e., no qualifying tuples are omitted) and authentic (i.e., all the result values originated from the owner). The scheme supports range selection on key and non-key attributes, project as well as join queries on relational databases. Moreover, the proposed scheme complies with access control policies, is computationally secure, and can be implemented efficiently."" based on the user's previous titles: ""['Explaining inferences in Bayesian networks', 'Secure server-aided top-k monitoring.']""."
21," The authors provide a complete method for describing and recognizing 3-D objects, using surface information. Their system takes as input dense range date and automatically produces a symbolic description of the objects in the scene in terms of their visible surface patches. This segmented representation may be viewed as a graph whose nodes capture information about the individual surface patches and whose links represent the relationships between them, such as occlusion and connectivity. On the basis of these relations, a graph for a given scene is decomposed into subgraphs corresponding to different objects. A model is represented by a set of such descriptions from multiple viewing angles, typically four to six. Models can therefore be acquired and represented automatically. Matching between the objects in a scene and the models is performed by three modules: the screener, in which the most likely candidate views for each object are found; the graph matcher, which compares the potential matching graphs and computes the 3-D transformation between them; and the analyzer, which takes a critical look at the results and proposes to split and merge object graphs.","['complete', '3D', 'symbolic', 'visible', 'segmented', 'nodes', 'individual', 'such', 'subgraphs', 'different', 'such', 'multiple', 'likely', 'potential', '3D', 'critical', 'object']","['Partial', 'complete', 'partial', 'total', 'planar', 'functions', 'key', 'power', 'reducible', 'are', 'connected', 'possible', 'without', 'planar', 'rectangular', 'Subgraphs', 'networks', 'subgraphs', 'necessary', 'simply', 'the', 'are', 'necessary', 'possible', 'without', 'Subgraphs', 'subgraphs', 'treewidth', 'Various', 'are', 'better', 'than', 'are', 'necessary', 'possible', 'without', 'Various', 'consecutive', 'parallel', 'are', 'better', 'possible', 'simply', 'than', 'maximum', 'possible', 'problems', 'planar', 'key', 'necessary', 'optimal', 'plane', 'rectangle']","['Conditional Bayesian networks for action detection', 'Three-Dimensional Descriptions Based on the Analysis of the Invariant and Quasi-Invariant Properties of Some Curved-Axis Generalized Cylinders']","Paraphrase the following paper title without adding any other information: "" The authors provide a complete method for describing and recognizing 3-D objects, using surface information. Their system takes as input dense range date and automatically produces a symbolic description of the objects in the scene in terms of their visible surface patches. This segmented representation may be viewed as a graph whose nodes capture information about the individual surface patches and whose links represent the relationships between them, such as occlusion and connectivity. On the basis of these relations, a graph for a given scene is decomposed into subgraphs corresponding to different objects. A model is represented by a set of such descriptions from multiple viewing angles, typically four to six. Models can therefore be acquired and represented automatically. Matching between the objects in a scene and the models is performed by three modules: the screener, in which the most likely candidate views for each object are found; the graph matcher, which compares the potential matching graphs and computes the 3-D transformation between them; and the analyzer, which takes a critical look at the results and proposes to split and merge object graphs."" based on the user's previous titles: ""['Conditional Bayesian networks for action detection', 'Three-Dimensional Descriptions Based on the Analysis of the Invariant and Quasi-Invariant Properties of Some Curved-Axis Generalized Cylinders']""."
22," Image classification and annotation are important problems in computer vision, but rarely considered together Intuitively, annotations provide evidence for the class label, and the class label provides evidence for annotations. For example, an image of class highway is more likely annotated with words ""road,"" ""car"" and ""traffic"" than words ""fish,"" ""boat,"" and ""scuba."" In this paper we develop a new probabilistic model for jointly modeling the image, its class label, and its annotations. Our model treats the class label as a global description of the image, and treats annotation terms as local descriptions of parts of the image. Its underlying probabilistic assumptions naturally integrate these two sources of information. We derive an approximate inference and estimation algorithms based on variational methods, as well as efficient approximations for classifying and annotating new images. We examine the performance of our model on two real-world image data sets, illustrating that a single model provides competitive annotation performance, and superior classification performance.","['important', 'likely', 'fish', 'new', 'probabilistic', 'global', 'local', 'underlying', 'probabilistic', 'approximate', 'variational', 'efficient', 'new', 'realworld', 'single', 'competitive', 'superior']","['best', 'key', 'necessary', 'are', 'better', 'possible', 'simply', 'than', 'forests', 'trees', 'the', 'algorithm', 'algorithms', 'combinatorial', 'complexity', 'computing', 'demand', 'networks', 'the', 'complexity', 'problem', 'problems', 'algorithm', 'algorithms', 'combinatorial', 'approximation', 'maximum', 'optimal', 'Eulerian', 'Hamiltonian', 'approximation', 'generalized', 'Efficient', 'optimal', 'the', 'approximation', 'computing', 'parallel', 'solvable', 'tradeoffs', 'a', 'balanced', 'best', 'edge', 'optimal', 'best', 'better', 'optimal', 'than']","['Efficient Euclidean Projections onto the Intersection of Norm Balls', 'Leveraging the Wisdom of the Crowd for Fine-Grained Recognition']","Paraphrase the following paper title without adding any other information: "" Image classification and annotation are important problems in computer vision, but rarely considered together Intuitively, annotations provide evidence for the class label, and the class label provides evidence for annotations. For example, an image of class highway is more likely annotated with words ""road,"" ""car"" and ""traffic"" than words ""fish,"" ""boat,"" and ""scuba."" In this paper we develop a new probabilistic model for jointly modeling the image, its class label, and its annotations. Our model treats the class label as a global description of the image, and treats annotation terms as local descriptions of parts of the image. Its underlying probabilistic assumptions naturally integrate these two sources of information. We derive an approximate inference and estimation algorithms based on variational methods, as well as efficient approximations for classifying and annotating new images. We examine the performance of our model on two real-world image data sets, illustrating that a single model provides competitive annotation performance, and superior classification performance."" based on the user's previous titles: ""['Efficient Euclidean Projections onto the Intersection of Norm Balls', 'Leveraging the Wisdom of the Crowd for Fine-Grained Recognition']""."
23," Distributed hard real-time systems require guaranteed communication. One common approach is to restrict network access by enforcing a time-division multiple access (TDMA) schedule.The typical data representation of offlinegenerated TDMA schedules is table-like structures. This representation, however, does not permit applications with dynamic communication demands, because the table-like structure prevents on-the-fly changes during execution. A common approach for applications with dynamic communication behavior is dynamic TDMA schedules. However, such schedules are hard to verify, because they are usually implemented in a programming language, which does not support verification. Network code is a behavioral model for specifying realtime communication schedules. It allows modeling arbitrary time-triggered communication schedules with on-thefly choices, and it is also apt for formal verification. In this work, we present network code and show how we can use a model checker to verify safety properties such as collision-free communication, schedulability, and guaranteed message reception. We also discuss its implementation in RTLinux and provide performance measurements.","['hard', 'realtime', 'guaranteed', 'common', 'multiple', 'typical', 'offlinegenerated', 'tablelike', 'dynamic', 'tablelike', 'onthefly', 'common', 'dynamic', 'dynamic', 'such', 'hard', 'behavioral', 'realtime', 'arbitrary', 'timetriggered', 'onthefly', 'apt', 'formal', 'such', 'collisionfree', 'guaranteed']","['best', 'better', 'finding', 'simply', 'algorithm', 'algorithms', 'Unconditionally', 'maximum', 'optimal', 'possible', 'are', 'problem', 'problems', 'Various', 'consecutive', 'parallel', 'the', '4connected', 'MultipleValued', 'Triconnected', 'fourconnected', 'BendOptimal', 'edgedisjoint', 'nonhamiltonian', 'vertexrankings', 'balanced', 'linear', 'BendOptimal', 'edgedisjoint', 'nonhamiltonian', 'vertexrankings', 'NPComplete', 'SeriesParallel', 'are', 'problem', 'problems', 'balanced', 'linear', 'balanced', 'linear', 'are', 'necessary', 'possible', 'without', 'best', 'better', 'finding', 'simply', 'combinatorial', 'problems', 'protocols', 'tradeoffs', 'algorithm', 'algorithms', 'approximation', 'generalized', 'linear', 'orthogonal', '4connected', 'SiteOriented', 'Triconnected', 'fivecoloring', 'NPComplete', 'SeriesParallel', 'An', 'better', 'necessary', 'simply', 'regular', 'uniform', 'without', 'are', 'necessary', 'possible', 'without', '5coloring', 'Matchings', 'Noncrossing', 'Unconditionally', 'maximum', 'optimal', 'possible']","['Automatic verification of linear controller software', 'On the parallel complexity of model checking in the modal mu-calculus']","Paraphrase the following paper title without adding any other information: "" Distributed hard real-time systems require guaranteed communication. One common approach is to restrict network access by enforcing a time-division multiple access (TDMA) schedule.The typical data representation of offlinegenerated TDMA schedules is table-like structures. This representation, however, does not permit applications with dynamic communication demands, because the table-like structure prevents on-the-fly changes during execution. A common approach for applications with dynamic communication behavior is dynamic TDMA schedules. However, such schedules are hard to verify, because they are usually implemented in a programming language, which does not support verification. Network code is a behavioral model for specifying realtime communication schedules. It allows modeling arbitrary time-triggered communication schedules with on-thefly choices, and it is also apt for formal verification. In this work, we present network code and show how we can use a model checker to verify safety properties such as collision-free communication, schedulability, and guaranteed message reception. We also discuss its implementation in RTLinux and provide performance measurements."" based on the user's previous titles: ""['Automatic verification of linear controller software', 'On the parallel complexity of model checking in the modal mu-calculus']""."
24," Our opinions and judgments are increasingly shaped by what we read on social media -- whether they be tweets and posts in social networks, blog posts, or review boards. These opinions could be about topics such as consumer products, politics, life style, or celebrities. Understanding how users in a network update opinions based on their neighbor's opinions, as well as what global opinion structure is implied when users iteratively update opinions, is important in the context of viral marketing and information dissemination, as well as targeting messages to users in the network. In this paper, we consider the problem of modeling how users update opinions based on their neighbors' opinions. We perform a set of online user studies based on the celebrated conformity experiments of Asch [1]. Our experiments are carefully crafted to derive quantitative insights into developing a model for opinion updates (as opposed to deriving psychological insights). We show that existing and widely studied theoretical models do not explain the entire gamut of experimental observations we make. This leads us to posit a new, nuanced model that we term the BVM. We present preliminary theoretical and simulation results on the convergence and structure of opinions in the entire network when users iteratively update their respective opinions according to the BVM. We show that consensus and polarization of opinions arise naturally in this model under easy to interpret initial conditions on the network.","['social', 'social', 'such', 'update', 'global', 'update', 'important', 'viral', 'online', 'celebrated', 'quantitative', 'psychological', 'theoretical', 'entire', 'experimental', 'new', 'nuanced', 'present', 'preliminary', 'entire', 'respective', 'easy', 'initial']","['computing', 'embedding', 'family', 'networks', 'sharing', 'computing', 'embedding', 'family', 'networks', 'sharing', 'are', 'necessary', 'possible', 'without', 'algorithm', 'complexity', 'computing', 'demand', 'networks', 'algorithm', 'best', 'key', 'necessary', 'Web', 'combinatorial', 'embedding', 'networks', 'sharing', 'Web', 'best', 'exchange', 'networks', 'regular', 'almost', 'consecutive', 'designated', 'characterization', 'combinatorial', 'generalized', 'linear', 'complexity', 'depth', 'problems', 'approximation', 'combinatorial', 'computability', 'generalized', 'maximal', 'the', 'algorithm', 'combinatorial', 'protocols', 'the', 'balanced', 'better', 'characterization', 'complexity', 'the', 'characterization', 'complete', 'drawings', 'partial', 'the', 'the', 'Simple', 'compact', 'necessary', 'possible', 'simply', 'the']","['Mining Videos from the Web for Electronic Textbooks.', 'Optimal auctions via the multiplicative weight method']","Paraphrase the following paper title without adding any other information: "" Our opinions and judgments are increasingly shaped by what we read on social media -- whether they be tweets and posts in social networks, blog posts, or review boards. These opinions could be about topics such as consumer products, politics, life style, or celebrities. Understanding how users in a network update opinions based on their neighbor's opinions, as well as what global opinion structure is implied when users iteratively update opinions, is important in the context of viral marketing and information dissemination, as well as targeting messages to users in the network. In this paper, we consider the problem of modeling how users update opinions based on their neighbors' opinions. We perform a set of online user studies based on the celebrated conformity experiments of Asch [1]. Our experiments are carefully crafted to derive quantitative insights into developing a model for opinion updates (as opposed to deriving psychological insights). We show that existing and widely studied theoretical models do not explain the entire gamut of experimental observations we make. This leads us to posit a new, nuanced model that we term the BVM. We present preliminary theoretical and simulation results on the convergence and structure of opinions in the entire network when users iteratively update their respective opinions according to the BVM. We show that consensus and polarization of opinions arise naturally in this model under easy to interpret initial conditions on the network."" based on the user's previous titles: ""['Mining Videos from the Web for Electronic Textbooks.', 'Optimal auctions via the multiplicative weight method']""."
25," This paper addresses the smallest grammar problem: What is the smallest context-free grammar that generates exactly one given string σ? This is a natural question about a fundamental object connected to many fields such as data compression, Kolmogorov complexity, pattern identification, and addition chains. Due to the problem's inherent complexity, our objective is to find an approximation algorithm which finds a small grammar for the input string. We focus attention on the approximation ratio of the algorithm (and implicitly, the worst case behavior) to establish provable performance guarantees and to address shortcomings in the classical measure of redundancy in the literature. Our first results are concern the hardness of approximating the smallest grammar problem. Most notably, we show that every efficient algorithm for the smallest grammar problem has approximation ratio at least 8569/8568 unless P=NP. We then bound approximation ratios for several of the best known grammar-based compression algorithms, including LZ78, B ISECTION, SEQUENTIAL, LONGEST MATCH, GREEDY, and RE-PAIR. Among these, the best upper bound we show is O(n12/). We finish by presenting two novel algorithms with exponentially better ratios of O(log3n) and O(log(n/m*)), where m* is the size of the smallest grammar for that input. The latter algorithm highlights a connection between grammar-based compression and LZ77.","['smallest', 'smallest', 'contextfree', 'natural', 'fundamental', 'many', 'such', 'pattern', 'inherent', 'small', 'worst', 'provable', 'classical', 'first', 'smallest', 'Most', 'smallest', 'least', 'several', 'best', 'known', 'grammarbased', 'best', 'upper', 'finish', 'novel', 'better', 'nm', 'smallest', 'latter', 'grammarbased']","['best', 'compact', 'shortest', 'size', 'best', 'compact', 'shortest', 'size', 'BendOptimal', 'VertexRankings', 'edgedisjoint', 'vertexrankings', 'colorings', 'forests', 'undirected', 'key', 'necessary', 'problem', 'Various', 'are', 'the', 'are', 'necessary', 'possible', 'without', 'color', 'contours', 'rectangle', 'size', 'complexity', 'problem', 'problems', 'tradeoffs', 'Small', 'a', 'size', 'Best', 'best', 'better', 'possible', 'shortest', 'computability', 'maximal', 'possible', 'reducible', 'solvable', 'combinatorial', 'linear', 'the', 'best', 'compact', 'shortest', 'size', 'Almost', 'For', 'Various', 'best', 'compact', 'shortest', 'size', 'almost', 'than', 'the', 'Various', 'consecutive', 'the', 'Best', 'best', 'better', 'possible', 'shortest', 'are', 'best', 'designated', 'is', 'possible', '4connected', 'Triconnected', 'fourconnected', 'nonhamiltonian', 'Best', 'best', 'better', 'possible', 'shortest', 'Lower', 'the', 'color', 'complete', 'corners', 'edge', 'shortest', 'Characterization', 'algorithm', 'characterization', 'combinatorial', 'best', 'better', 'optimal', 'than', 'planar', 'best', 'compact', 'shortest', 'size', 'the', '4connected', 'Triconnected', 'fourconnected', 'nonhamiltonian']","['Computing shortest paths with uncertainty', 'Estimating corpus size via queries']","Paraphrase the following paper title without adding any other information: "" This paper addresses the smallest grammar problem: What is the smallest context-free grammar that generates exactly one given string σ? This is a natural question about a fundamental object connected to many fields such as data compression, Kolmogorov complexity, pattern identification, and addition chains. Due to the problem's inherent complexity, our objective is to find an approximation algorithm which finds a small grammar for the input string. We focus attention on the approximation ratio of the algorithm (and implicitly, the worst case behavior) to establish provable performance guarantees and to address shortcomings in the classical measure of redundancy in the literature. Our first results are concern the hardness of approximating the smallest grammar problem. Most notably, we show that every efficient algorithm for the smallest grammar problem has approximation ratio at least 8569/8568 unless P=NP. We then bound approximation ratios for several of the best known grammar-based compression algorithms, including LZ78, B ISECTION, SEQUENTIAL, LONGEST MATCH, GREEDY, and RE-PAIR. Among these, the best upper bound we show is O(n12/). We finish by presenting two novel algorithms with exponentially better ratios of O(log3n) and O(log(n/m*)), where m* is the size of the smallest grammar for that input. The latter algorithm highlights a connection between grammar-based compression and LZ77."" based on the user's previous titles: ""['Computing shortest paths with uncertainty', 'Estimating corpus size via queries']""."
26," In this paper, we present P-Hera, a peer-to-peer (P2P) infrastructure for scalable and secure content hosting. PHera allows the users and content owners to dynamically establish trust using fine-grained access control. In P-Hera, resource owners can specify fine-grained restrictions on who can access their resources and which user can access which part of data. We differentiate our work with traditional works of fine-grained access control on Web services, as our system in addition to handling access constrains of the service provider (which is the case in Web services), it also handles security constrains regarding actions performed on data: replication and modification. We believe this is of immense significance for wide-range of applications such as data Grids, Information Grids and Web Content Delivery Networks. In addition to presenting the overall system architecture, we also study the problem of evaluating these fine-grained access policies in depth and propose a novel means of organizing these policies that can result in faster evaluation. We demonstrate the effectiveness of our approach using prototype implementation.","['present', 'scalable', 'secure', 'finegrained', 'finegrained', 'traditional', 'finegrained', 'immense', 'such', 'overall', 'finegrained', 'novel', 'faster', 'prototype']","['the', 'Efficient', 'computing', 'optimal', 'Secure', 'connected', 'necessary', 'sufficient', 'Hierarchical', 'balanced', 'partitioning', 'symmetric', 'Hierarchical', 'balanced', 'partitioning', 'symmetric', 'the', 'using', 'Hierarchical', 'balanced', 'partitioning', 'symmetric', 'complexity', 'depth', 'sufficient', 'total', 'are', 'necessary', 'possible', 'without', 'complexity', 'optimal', 'the', 'total', 'Hierarchical', 'balanced', 'partitioning', 'symmetric', 'Characterization', 'algorithm', 'characterization', 'combinatorial', 'better', 'possible', 'shortest', 'than', 'without', 'drawings']","['An Efficient Time-Bound Hierarchical Key Management Scheme for Secure Broadcasting', 'Hierarchical data placement for navigational multimedia applications']","Paraphrase the following paper title without adding any other information: "" In this paper, we present P-Hera, a peer-to-peer (P2P) infrastructure for scalable and secure content hosting. PHera allows the users and content owners to dynamically establish trust using fine-grained access control. In P-Hera, resource owners can specify fine-grained restrictions on who can access their resources and which user can access which part of data. We differentiate our work with traditional works of fine-grained access control on Web services, as our system in addition to handling access constrains of the service provider (which is the case in Web services), it also handles security constrains regarding actions performed on data: replication and modification. We believe this is of immense significance for wide-range of applications such as data Grids, Information Grids and Web Content Delivery Networks. In addition to presenting the overall system architecture, we also study the problem of evaluating these fine-grained access policies in depth and propose a novel means of organizing these policies that can result in faster evaluation. We demonstrate the effectiveness of our approach using prototype implementation."" based on the user's previous titles: ""['An Efficient Time-Bound Hierarchical Key Management Scheme for Secure Broadcasting', 'Hierarchical data placement for navigational multimedia applications']""."
27," Exceptions in database systems can be used for two different purposes: to store data not conforming to the description provided by the database schema, that is, exceptional data; and to handle exceptional situations during processing, that is, the usual execution exceptions of programming languages. In this paper we survey approaches to both kinds of exceptions in OODBMSs, we discuss some uses of exceptions peculiar to databases, and relate exceptions with triggers, a typical database functionality.","['different', 'exceptional', 'exceptional', 'usual', 'relate', 'typical']","['Various', 'are', 'better', 'than', 'best', 'maximum', 'optimal', 'sufficient', 'best', 'maximum', 'optimal', 'sufficient', 'better', 'regular', 'simply', 'than', 'the', 'connected', 'to', 'the']","['An abstraction-based approach to measuring the structural similarity between two unordered XML documents', 'Reverting the effects of XQuery update expressions']","Paraphrase the following paper title without adding any other information: "" Exceptions in database systems can be used for two different purposes: to store data not conforming to the description provided by the database schema, that is, exceptional data; and to handle exceptional situations during processing, that is, the usual execution exceptions of programming languages. In this paper we survey approaches to both kinds of exceptions in OODBMSs, we discuss some uses of exceptions peculiar to databases, and relate exceptions with triggers, a typical database functionality."" based on the user's previous titles: ""['An abstraction-based approach to measuring the structural similarity between two unordered XML documents', 'Reverting the effects of XQuery update expressions']""."
28,"   In this paper we introduce and experimentally compare alternative algorithms to join uncertain relations. Different algorithms are based on specific principles, e.g., sorting, indexing, or building intermediate relational tables to apply traditional approaches. As a consequence their performance is affected by different features of the input data, and each algorithm is shown to be more efficient than the others in specific cases. In this way statistics explicitly representing the amount and kind of uncertainty in the input uncertain relations can be used to choose the most efficient algorithm. ","['compare', 'alternative', 'uncertain', 'specific', 'intermediate', 'relational', 'traditional', 'different', 'efficient', 'specific', 'uncertain', 'efficient']","['Find', 'best', 'to', 'best', 'better', 'optimal', 'possible', 'Given', 'are', 'finding', 'possible', 'key', 'optimal', 'decomposition', 'maximal', 'partial', 'shortest', 'Boolean', 'Cardinality', 'Hierarchical', 'cardinality', 'computability', 'the', 'using', 'Various', 'are', 'better', 'than', 'Efficient', 'optimal', 'key', 'optimal', 'Given', 'are', 'finding', 'possible', 'Efficient', 'optimal']","['A Rewriting Technique for the Analysis and the Optimization of Active Databases', 'Datalog for the web 2.0: the case of social network data management']","Paraphrase the following paper title without adding any other information: ""   In this paper we introduce and experimentally compare alternative algorithms to join uncertain relations. Different algorithms are based on specific principles, e.g., sorting, indexing, or building intermediate relational tables to apply traditional approaches. As a consequence their performance is affected by different features of the input data, and each algorithm is shown to be more efficient than the others in specific cases. In this way statistics explicitly representing the amount and kind of uncertainty in the input uncertain relations can be used to choose the most efficient algorithm. "" based on the user's previous titles: ""['A Rewriting Technique for the Analysis and the Optimization of Active Databases', 'Datalog for the web 2.0: the case of social network data management']""."
29," Quality of service (QoS) and, in particular, reliability and a bounded low latency are essential attributes of safety-critical wireless systems for medical applications. However, wireless links are typically prone to bursts of errors, with characteristics which vary over time.We propose a wireless system suitable for real-time remote patient monitoring in which the necessary reliability and guaranteed latency are both achieved by an efficient error control scheme. We have paired an example remote electrocardiography application to this wireless system. We also developed a tool chain that uses a formal description of the proposed wireless medical system architecture in the architecture analysis and design language to assess various combinations of system parameters: we can determine the QoS in terms of packet-delivery ratio and the service latency, and also the size of jitter buffer required for seamless ECG monitoring. A realistic assessment, based on data from the MIT-BIT arrhythmia database, shows that the proposed wireless system can achieve an appropriate level of QoS for real-time ECG monitoring if link-level error control is correctly implemented. Additionally, we present guidelines for the design of energy-efficient link-level error control, derived from energy data, obtained from simulations.","['particular', 'low', 'essential', 'safetycritical', 'medical', 'wireless', 'prone', 'suitable', 'realtime', 'remote', 'necessary', 'guaranteed', 'efficient', 'formal', 'medical', 'various', 'seamless', 'realistic', 'appropriate', 'realtime', 'linklevel', 'energyefficient', 'linklevel']","['the', 'Lower', 'maximum', 'optimal', 'key', 'necessary', 'sufficient', 'seriesparallel', 'computing', 'family', 'prescribed', 'protocols', 'bandwidth', 'computing', 'connected', 'networks', 'are', 'problems', 'simply', 'than', 'for', 'algorithm', 'algorithms', 'areas', 'computing', 'connected', 'power', 'Necessary', 'Sufficient', 'necessary', 'possible', 'sufficient', 'Unconditionally', 'maximum', 'optimal', 'possible', 'Efficient', 'optimal', 'regular', 'uniform', 'without', 'computing', 'family', 'prescribed', 'protocols', 'Various', 'the', 'balanced', 'complete', 'optimal', 'approximation', 'balanced', 'better', 'possible', 'necessary', 'optimal', 'possible', 'sufficient', 'algorithm', 'algorithms', 'Approximability', 'edgedisjoint', 'edgerankings', 'vertexrankings', 'Efficient', 'compact', 'optimal', 'Approximability', 'edgedisjoint', 'edgerankings', 'vertexrankings']","['Cyber-physical systems: the next computing revolution', 'Feedback fault tolerance of real-time embedded systems: issues and possible solutions']","Paraphrase the following paper title without adding any other information: "" Quality of service (QoS) and, in particular, reliability and a bounded low latency are essential attributes of safety-critical wireless systems for medical applications. However, wireless links are typically prone to bursts of errors, with characteristics which vary over time.We propose a wireless system suitable for real-time remote patient monitoring in which the necessary reliability and guaranteed latency are both achieved by an efficient error control scheme. We have paired an example remote electrocardiography application to this wireless system. We also developed a tool chain that uses a formal description of the proposed wireless medical system architecture in the architecture analysis and design language to assess various combinations of system parameters: we can determine the QoS in terms of packet-delivery ratio and the service latency, and also the size of jitter buffer required for seamless ECG monitoring. A realistic assessment, based on data from the MIT-BIT arrhythmia database, shows that the proposed wireless system can achieve an appropriate level of QoS for real-time ECG monitoring if link-level error control is correctly implemented. Additionally, we present guidelines for the design of energy-efficient link-level error control, derived from energy data, obtained from simulations."" based on the user's previous titles: ""['Cyber-physical systems: the next computing revolution', 'Feedback fault tolerance of real-time embedded systems: issues and possible solutions']""."
30," In relevance feedback of image retrieval, selective sampling is often used to alleviate the burden of labeling by selecting only the most informative data to label. Traditional data selection scheme often selects a batch of data at a time and label them all together, which neglects the data's correlation and thus jeopardizes the effectiveness. In this paper, we propose a novel Dynamic Certainty Propagation (DCP) scheme for informative data selection. For each unlabeled data, we define the notion of certainty to quantify our confidence in its predicted label. Every time, we only label one single data point with the lowest degree of certainty. Then we update the rest unlabeled data's certainty dynamically according to their correlation. This one-by-one labeling offers us extra guidance from the last labeled data for the next labeling. Experiments show that the DCP scheme outperforms the traditional method evidently.","['selective', 'informative', 'novel', 'informative', 'unlabeled', 'predicted', 'single', 'lowest', 'onebyone', 'extra', 'last', 'labeled', 'next', 'traditional']","['characterization', 'combinatorial', 'partial', 'partitioning', 'Nicely', 'balanced', 'graphs', 'sharing', 'Characterization', 'algorithm', 'characterization', 'combinatorial', 'Nicely', 'balanced', 'graphs', 'sharing', 'Labeling', 'rectangle', 'using', 'Given', 'approximation', 'possible', 'threshold', 'a', 'best', 'maximum', 'optimal', 'shortest', 'Lineartime', 'edgedisjoint', 'onebit', 'vertexrankings', 'for', 'maximum', 'necessary', 'sufficient', 'consecutive', 'the', 'Labeling', 'connected', 'designated', 'the', 'the', 'using']","['Optical Flow in the Scale Space', 'Generalized optical flow in the scale space']","Paraphrase the following paper title without adding any other information: "" In relevance feedback of image retrieval, selective sampling is often used to alleviate the burden of labeling by selecting only the most informative data to label. Traditional data selection scheme often selects a batch of data at a time and label them all together, which neglects the data's correlation and thus jeopardizes the effectiveness. In this paper, we propose a novel Dynamic Certainty Propagation (DCP) scheme for informative data selection. For each unlabeled data, we define the notion of certainty to quantify our confidence in its predicted label. Every time, we only label one single data point with the lowest degree of certainty. Then we update the rest unlabeled data's certainty dynamically according to their correlation. This one-by-one labeling offers us extra guidance from the last labeled data for the next labeling. Experiments show that the DCP scheme outperforms the traditional method evidently."" based on the user's previous titles: ""['Optical Flow in the Scale Space', 'Generalized optical flow in the scale space']""."
31," We consider the verification of safety (strict serializability and abort consistency) and liveness (obstruction and livelock freedom) for the hybrid transactional memory framework FlexTM. This framework allows for flexible implementations of transactional memories based on an adaptation of the MESI coherence protocol. FlexTM allows for both eager and lazy conflict resolution strategies. Like in the case of Software Transactional Memories, the verification problem is not trivial as the number of concurrent transactions, their size, and the number of accessed shared variables cannot be a priori bounded. This complexity is exacerbated by aspects that are specific to hardware and hybrid transactional memories. Our work takes into account intricate behaviours such as cache line based conflict detection, false sharing, invisible reads or non-transactional instructions. We carry out the first automatic verification of a hybrid transactional memory and establish, by adopting a small model approach, challenging properties such as strict serializability, abort consistency, and obstruction freedom for both an eager and a lazy conflict resolution strategies. We also detect an example that refutes livelock freedom. To achieve this, our prototype tool makes use of the latest antichain based techniques to handle systems with tens of thousands of states.","['strict', 'hybrid', 'transactional', 'flexible', 'transactional', 'eager', 'lazy', 'trivial', 'concurrent', 'accessed', 'specific', 'hybrid', 'transactional', 'intricate', 'such', 'false', 'invisible', 'nontransactional', 'first', 'automatic', 'hybrid', 'transactional', 'small', 'such', 'strict', 'lazy', 'prototype', 'latest']","['prescribed', 'protocols', 'uniform', 'compact', 'computing', 'planar', 'Relationship', 'complexity', 'computing', 'exchange', 'flows', 'balanced', 'compact', 'optimal', 'Relationship', 'complexity', 'computing', 'exchange', 'flows', 'to', 'better', 'linear', 'simply', 'generalized', 'necessary', 'reducible', 'simply', 'solvable', 'Consecutive', 'consecutive', 'parallel', 'Web', 'connected', 'designated', 'using', 'key', 'optimal', 'compact', 'computing', 'planar', 'Relationship', 'complexity', 'computing', 'exchange', 'flows', 'Simple', 'complexity', 'contours', 'drawings', 'are', 'necessary', 'possible', 'without', 'Boolean', 'generalized', 'partial', 'simply', 'almost', 'secret', 'simply', 'without', 'multicolorings', 'nonhamiltonian', 'vertexrankings', 'the', 'algorithms', 'partial', 'regular', 'compact', 'computing', 'planar', 'Relationship', 'complexity', 'computing', 'exchange', 'flows', 'Small', 'a', 'size', 'are', 'necessary', 'possible', 'without', 'prescribed', 'protocols', 'uniform', 'better', 'linear', 'simply', 'drawings', 'best', 'the']","['Shared memory computing on clusters with symmetric multiprocessors and system area networks', 'Reducing the complexity of the register file in dynamic superscalar processors']","Paraphrase the following paper title without adding any other information: "" We consider the verification of safety (strict serializability and abort consistency) and liveness (obstruction and livelock freedom) for the hybrid transactional memory framework FlexTM. This framework allows for flexible implementations of transactional memories based on an adaptation of the MESI coherence protocol. FlexTM allows for both eager and lazy conflict resolution strategies. Like in the case of Software Transactional Memories, the verification problem is not trivial as the number of concurrent transactions, their size, and the number of accessed shared variables cannot be a priori bounded. This complexity is exacerbated by aspects that are specific to hardware and hybrid transactional memories. Our work takes into account intricate behaviours such as cache line based conflict detection, false sharing, invisible reads or non-transactional instructions. We carry out the first automatic verification of a hybrid transactional memory and establish, by adopting a small model approach, challenging properties such as strict serializability, abort consistency, and obstruction freedom for both an eager and a lazy conflict resolution strategies. We also detect an example that refutes livelock freedom. To achieve this, our prototype tool makes use of the latest antichain based techniques to handle systems with tens of thousands of states."" based on the user's previous titles: ""['Shared memory computing on clusters with symmetric multiprocessors and system area networks', 'Reducing the complexity of the register file in dynamic superscalar processors']""."
32," In this work, we propose and assess a technique called bypass processing for optimizing the evaluation of disjunctive queries with expensive predicates. The technique is particularly useful for optimizing selection predicates that contain terms whose evaluation costs vary tremendously; e.g., the evaluation of a nested subquery or the invocation of a user-defined function in an object-oriented or extended relational model may be orders of magnitude more expensive than an attribute access (and comparison). The idea of bypass processing consists of avoiding the evaluation of such expensive terms whenever the outcome of the entire selection predicate can already be induced by testing other, less expensive terms. In order to validate the viability of bypass evaluation, we extend a previously developed optimizer architecture and incorporate three alternative optimization algorithms for generating bypass processing plans.","['disjunctive', 'expensive', 'useful', 'nested', 'userdefined', 'objectoriented', 'extended', 'relational', 'expensive', 'attribute', 'such', 'expensive', 'entire', 'other', 'expensive', 'alternative']","['linear', 'multigraphs', 'best', 'better', 'compact', 'necessary', 'than', 'better', 'finding', 'necessary', 'possible', 'sufficient', 'Hierarchical', 'cardinality', 'subgraphs', 'Boolean', 'Cardinality', 'cardinality', 'functions', 'routing', 'Boolean', 'algorithms', 'combinatorial', 'Extended', 'Boolean', 'Cardinality', 'Hierarchical', 'cardinality', 'computability', 'best', 'better', 'compact', 'necessary', 'than', 'cardinality', 'key', 'simply', 'are', 'necessary', 'possible', 'without', 'best', 'better', 'compact', 'necessary', 'than', 'the', 'the', 'best', 'better', 'compact', 'necessary', 'than', 'best', 'better', 'optimal', 'possible']","['Heterogeneity-conscious parallel query execution: getting a better mileage while driving faster!', 'Index-Assisted Hierarchical Computations in Main-Memory RDBMS.']","Paraphrase the following paper title without adding any other information: "" In this work, we propose and assess a technique called bypass processing for optimizing the evaluation of disjunctive queries with expensive predicates. The technique is particularly useful for optimizing selection predicates that contain terms whose evaluation costs vary tremendously; e.g., the evaluation of a nested subquery or the invocation of a user-defined function in an object-oriented or extended relational model may be orders of magnitude more expensive than an attribute access (and comparison). The idea of bypass processing consists of avoiding the evaluation of such expensive terms whenever the outcome of the entire selection predicate can already be induced by testing other, less expensive terms. In order to validate the viability of bypass evaluation, we extend a previously developed optimizer architecture and incorporate three alternative optimization algorithms for generating bypass processing plans."" based on the user's previous titles: ""['Heterogeneity-conscious parallel query execution: getting a better mileage while driving faster!', 'Index-Assisted Hierarchical Computations in Main-Memory RDBMS.']""."
33," In this paper, we design and evaluate SORS- a scalable online ridesharing system, where drivers and passengers send their requests for a ride in advance, possibly on a short notice. SORS is modular and consists of two main, loosely coupled, components: the Constraint Satisfier and the Matching Module. The Constraint Satisfier takes as input information about the desired trajectories and spatio-temporal constraints of drivers and passengers and it returns a list of feasible (driver, passenger) pairs. We use a road networks data structure, optimized for the specific spatio-temporal queries in the context of ridesharing, and we show that our Constraint Satisfier has a 4.65x more scalable query time than a general-purpose database. We represent the feasible pairs of drivers and passengers as a weighted bipartite graph with edge weight being the length of the shared trip of the pair, which captures the revenue in real-world ridesharing systems, such as Lyft Carpool. The Matching Module then takes as input this weighted bipartite graph and returns the maximum weighted matching (MWM), using an algorithm that solves the problem online and efficiently, by incrementally updating the matching solution in real-time. We show that our algorithm achieves 51% larger weight (i.e., total revenue) compared to greedy heuristics used by many real systems today. We also evaluate the SORS system as a whole, using mobile datasets to extract driver trajectories and passenger locations in urban environments. We show that SORS can provide a ridesharing recommendation to individual users within a sub-second query response time, even at high workloads.","['scalable', 'short', 'modular', 'main', 'desired', 'spatiotemporal', 'feasible', 'specific', 'spatiotemporal', 'scalable', 'generalpurpose', 'feasible', 'weighted', 'shared', 'realworld', 'such', 'bipartite', 'maximum', 'weighted', 'matching', 'realtime', 'larger', 'total', 'many', 'real', 'mobile', 'urban', 'individual', 'subsecond', 'high']","['Efficient', 'computing', 'optimal', 'Shortest', 'consecutive', 'shortest', 'compact', 'linear', 'key', 'the', 'necessary', 'optimal', 'possible', 'sufficient', 'linear', 'necessary', 'optimal', 'possible', 'sufficient', 'key', 'optimal', 'linear', 'Efficient', 'computing', 'optimal', 'algorithms', 'combinatorial', 'computing', 'necessary', 'optimal', 'possible', 'sufficient', 'Weighted', 'weighted', 'Sharing', 'connected', 'sharing', 'approximation', 'computing', 'parallel', 'solvable', 'tradeoffs', 'are', 'necessary', 'possible', 'without', 'bipartition', 'matchings', 'subgraphs', 'Maximum', 'maximal', 'maximum', 'total', 'Weighted', 'weighted', 'color', 'finding', 'matchings', 'algorithm', 'algorithms', 'Small', 'better', 'size', 'than', 'Total', 'total', 'Various', 'are', 'the', 'best', 'better', 'possible', 'problem', 'Connected', 'Web', 'compact', 'computing', 'networks', 'areas', 'forests', 'grid', 'necessary', 'simply', 'the', 'maximal', 'maximum', 'optimal', 'Lower', 'demand', 'maximum']","['An analytical design of optimal playout schedulers for packet video receivers', 'If you are not paying for it, you are the product: how much do advertisers pay to reach you?']","Paraphrase the following paper title without adding any other information: "" In this paper, we design and evaluate SORS- a scalable online ridesharing system, where drivers and passengers send their requests for a ride in advance, possibly on a short notice. SORS is modular and consists of two main, loosely coupled, components: the Constraint Satisfier and the Matching Module. The Constraint Satisfier takes as input information about the desired trajectories and spatio-temporal constraints of drivers and passengers and it returns a list of feasible (driver, passenger) pairs. We use a road networks data structure, optimized for the specific spatio-temporal queries in the context of ridesharing, and we show that our Constraint Satisfier has a 4.65x more scalable query time than a general-purpose database. We represent the feasible pairs of drivers and passengers as a weighted bipartite graph with edge weight being the length of the shared trip of the pair, which captures the revenue in real-world ridesharing systems, such as Lyft Carpool. The Matching Module then takes as input this weighted bipartite graph and returns the maximum weighted matching (MWM), using an algorithm that solves the problem online and efficiently, by incrementally updating the matching solution in real-time. We show that our algorithm achieves 51% larger weight (i.e., total revenue) compared to greedy heuristics used by many real systems today. We also evaluate the SORS system as a whole, using mobile datasets to extract driver trajectories and passenger locations in urban environments. We show that SORS can provide a ridesharing recommendation to individual users within a sub-second query response time, even at high workloads."" based on the user's previous titles: ""['An analytical design of optimal playout schedulers for packet video receivers', 'If you are not paying for it, you are the product: how much do advertisers pay to reach you?']""."
34," One of the most widely considered cache replacement policies is least recently used (LRU) based on which many other policies have been developed. LRU has been studied analytically in the literature under the assumption that the object requests are independent. However, such an assumption does not seem to be in agreement with recent studies of Web-traces, which indicate the existence of short term correlations among the requests. This paper introduces an approximate analysis that fairly accurately predicts the hit ratio of the LRU policy in the case of short term correlations. The approximation approach is based on the relation between the working set model and LRU, while the request generation process is assumed to follow a recently proposed model for Web-traces, which captures short term correlations among the requests. The accuracy of the introduced approximate analysis is validated for synthetic as well as real Web-traces.","['least', 'many', 'other', 'independent', 'such', 'recent', 'short', 'approximate', 'short', 'short', 'introduced', 'synthetic', 'real']","['almost', 'than', 'the', 'Various', 'are', 'the', 'the', 'Independent', 'connected', 'sufficient', 'are', 'necessary', 'possible', 'without', 'Given', 'consecutive', 'the', 'Shortest', 'consecutive', 'shortest', 'approximation', 'maximum', 'optimal', 'Shortest', 'consecutive', 'shortest', 'Shortest', 'consecutive', 'shortest', 'designated', 'into', 'set', 'using', 'algorithms', 'colorings', 'combinatorial', 'using', 'best', 'better', 'possible', 'problem']","['On the approximation of the output process of multiuser random-access communication networks', 'A generic characterization of the overheads imposed by IPsec and associated cryptographic algorithms']","Paraphrase the following paper title without adding any other information: "" One of the most widely considered cache replacement policies is least recently used (LRU) based on which many other policies have been developed. LRU has been studied analytically in the literature under the assumption that the object requests are independent. However, such an assumption does not seem to be in agreement with recent studies of Web-traces, which indicate the existence of short term correlations among the requests. This paper introduces an approximate analysis that fairly accurately predicts the hit ratio of the LRU policy in the case of short term correlations. The approximation approach is based on the relation between the working set model and LRU, while the request generation process is assumed to follow a recently proposed model for Web-traces, which captures short term correlations among the requests. The accuracy of the introduced approximate analysis is validated for synthetic as well as real Web-traces."" based on the user's previous titles: ""['On the approximation of the output process of multiuser random-access communication networks', 'A generic characterization of the overheads imposed by IPsec and associated cryptographic algorithms']""."
35," An orthogonal drawing of a graph is a drawing such that nodes are placed on grid points and edges are drawn as sequences of vertical and horizontal segments. In this paper we present linear time algorithms that produce orthogonal drawings of graphs with n nodes. If the maximum degree is four, then the drawing produced by our algorithm needs area no greater than 0.8n
2 and no more than 1.9n bends. Notice that our upper bound on the bends is below the lower bound for planar orthogonal drawings of planar graphs. To the best of our knowledge, this is the first algorithm for orthogonal drawings that needs area less than n
2. If the maximum degree is three, then the drawing produced by our algorithm needs (n/2+1)×n/2 area and at most n/2+3 bends. These upper bounds match the upper bounds known for triconnected planar graphs of degree 3.","['orthogonal', 'such', 'grid', 'vertical', 'horizontal', 'present', 'linear', 'orthogonal', 'n', 'maximum', 'greater', 'more', 'upper', 'lower', 'planar', 'orthogonal', 'best', 'first', 'orthogonal', 'less', 'maximum', 'most', 'n23', 'upper', 'upper', 'triconnected', 'degree']","['Orthogonal', 'orthogonal', 'are', 'necessary', 'possible', 'without', 'Grid', 'grid', 'linear', 'rectangular', 'parallel', 'rectangle', 'rectangular', 'the', 'linear', 'Orthogonal', 'orthogonal', 'A', 'Maximum', 'maximal', 'maximum', 'total', 'better', 'maximal', 'maximum', 'than', 'For', 'better', 'for', 'than', 'Lower', 'the', 'Lower', 'than', 'the', 'planar', 'Orthogonal', 'orthogonal', 'Best', 'best', 'better', 'possible', 'shortest', 'the', 'Orthogonal', 'orthogonal', 'almost', 'better', 'than', 'without', 'Maximum', 'maximal', 'maximum', 'total', 'best', 'the', 'Combinatorial', 'Generalized', 'computability', 'symmetric', 'unate', 'Lower', 'the', 'Lower', 'the', '4Connected', '4connected', 'Triconnected', 'fourconnected', 'complete', 'degrees', 'maximum', 'partial']","['Lower bounds and parallel algorithms for planar orthogonal grid drawings', 'Lower bounds for planar orthogonal drawings of graphs']","Paraphrase the following paper title without adding any other information: "" An orthogonal drawing of a graph is a drawing such that nodes are placed on grid points and edges are drawn as sequences of vertical and horizontal segments. In this paper we present linear time algorithms that produce orthogonal drawings of graphs with n nodes. If the maximum degree is four, then the drawing produced by our algorithm needs area no greater than 0.8n
2 and no more than 1.9n bends. Notice that our upper bound on the bends is below the lower bound for planar orthogonal drawings of planar graphs. To the best of our knowledge, this is the first algorithm for orthogonal drawings that needs area less than n
2. If the maximum degree is three, then the drawing produced by our algorithm needs (n/2+1)×n/2 area and at most n/2+3 bends. These upper bounds match the upper bounds known for triconnected planar graphs of degree 3."" based on the user's previous titles: ""['Lower bounds and parallel algorithms for planar orthogonal grid drawings', 'Lower bounds for planar orthogonal drawings of graphs']""."
36," This paper describes a framework that serves as a reference for classifying user interfaces supporting multiple targets, or multiple contexts of use in the field of context-aware computing. In this framework, a context of use is decomposed into three facets: the end users of the interactive system, the hardware and software computing platform with which the users have to carry out their interactiv...","['user', 'multiple', 'multiple', 'contextaware', 'interactive']","['Web', 'algorithm', 'computing', 'Various', 'consecutive', 'parallel', 'Various', 'consecutive', 'parallel', 'edgecoloring', 'edgedisjoint', 'linear']","['A layout inference algorithm for Graphical User Interfaces.', 'Model-Based Approaches to Reengineering Web Pages']","Paraphrase the following paper title without adding any other information: "" This paper describes a framework that serves as a reference for classifying user interfaces supporting multiple targets, or multiple contexts of use in the field of context-aware computing. In this framework, a context of use is decomposed into three facets: the end users of the interactive system, the hardware and software computing platform with which the users have to carry out their interactiv..."" based on the user's previous titles: ""['A layout inference algorithm for Graphical User Interfaces.', 'Model-Based Approaches to Reengineering Web Pages']""."
37," Intuition suggests that random testing of object-oriented programs should exhibit a significant difference in the number of faults detected by two different runs of equal duration. As a consequence, random testing would be rather unpredictable. We evaluate the variance of the number of faults detected by random testing over time. We present the results of an empirical study that is based on 1215 hours of randomly testing 27 Eiffel classes, each with 30 seeds of the random number generator. Analyzing over 6 million failures triggered during the experiments, the study provides evidence that the relative number of faults detected by random testing over time is predictable but that different runs of the random test case generator detect different faults. The study also shows that random testing quickly finds faults: the first failure is likely to be triggered within 30 seconds.","['random', 'objectoriented', 'significant', 'different', 'equal', 'random', 'unpredictable', 'empirical', 'random', 'relative', 'predictable', 'different', 'random', 'different', 'random', 'first', 'likely']","['combinatorial', 'linear', 'undirected', 'Boolean', 'algorithms', 'combinatorial', 'key', 'necessary', 'sufficient', 'Various', 'are', 'better', 'than', 'maximal', 'maximum', 'sufficient', 'than', 'combinatorial', 'linear', 'undirected', 'Given', 'complexity', 'optimal', 'possible', 'undirected', 'approximation', 'characterization', 'combinatorial', 'generalized', 'graphs', 'combinatorial', 'linear', 'undirected', 'Given', 'approximation', 'optimal', 'size', 'linear', 'Various', 'are', 'better', 'than', 'combinatorial', 'linear', 'undirected', 'Various', 'are', 'better', 'than', 'combinatorial', 'linear', 'undirected', 'the', 'are', 'better', 'possible', 'simply', 'than']","['On the Effectiveness of Test Extraction without Overhead', 'Idea: Unwinding Based Model-Checking and Testing for Non-Interference on EFSMs.']","Paraphrase the following paper title without adding any other information: "" Intuition suggests that random testing of object-oriented programs should exhibit a significant difference in the number of faults detected by two different runs of equal duration. As a consequence, random testing would be rather unpredictable. We evaluate the variance of the number of faults detected by random testing over time. We present the results of an empirical study that is based on 1215 hours of randomly testing 27 Eiffel classes, each with 30 seeds of the random number generator. Analyzing over 6 million failures triggered during the experiments, the study provides evidence that the relative number of faults detected by random testing over time is predictable but that different runs of the random test case generator detect different faults. The study also shows that random testing quickly finds faults: the first failure is likely to be triggered within 30 seconds."" based on the user's previous titles: ""['On the Effectiveness of Test Extraction without Overhead', 'Idea: Unwinding Based Model-Checking and Testing for Non-Interference on EFSMs.']""."
38," The Network Simulator-3 (ns-3) is rapidly developing into a flexible and easy-to-use tool suitable for wireless network simulation. Since energy consumption is a key issue for wireless devices, wireless network researchers often need to investigate the energy consumption at a battery powered node or in the overall network, while running network simulations. This requires the underlying simulator to support energy consumption and energy source modeling. Currently however, ns-3 does not provide any support for modeling energy consumption or energy sources. In this paper, we introduce an integrated energy framework for ns-3, with models for energy source as well as energy consumption. We present the design and implementation of the overall framework and the specific models therein. Further, we show how the proposed framework can be used in ns-3 to simulate energy-aware protocols in a wireless network.","['ns3', 'flexible', 'easytouse', 'key', 'wireless', 'overall', 'underlying', 'ns3', 'integrated', 'ns3', 'overall', 'specific', 'ns3', 'energyaware']","['Boolean', 'Combinatorial', 'cardinality', 'subgraphs', 'unate', 'balanced', 'compact', 'optimal', '4Connected', 'BendOptimal', 'SeriesParallel', 'Key', 'key', 'bandwidth', 'computing', 'connected', 'networks', 'complexity', 'optimal', 'the', 'total', 'complexity', 'problem', 'problems', 'Boolean', 'Combinatorial', 'cardinality', 'subgraphs', 'unate', 'balanced', 'connected', 'embedding', 'Boolean', 'Combinatorial', 'cardinality', 'subgraphs', 'unate', 'complexity', 'optimal', 'the', 'total', 'key', 'optimal', 'Boolean', 'Combinatorial', 'cardinality', 'subgraphs', 'unate', '4connected', 'NPComplete', 'PolynomialTime', 'SiteOriented', 'VariablePriority']","['Combinatorial Algorithms for Control of Biological Regulatory Networks.', 'Key distribution for secure multimedia multicasts via data embedding']","Paraphrase the following paper title without adding any other information: "" The Network Simulator-3 (ns-3) is rapidly developing into a flexible and easy-to-use tool suitable for wireless network simulation. Since energy consumption is a key issue for wireless devices, wireless network researchers often need to investigate the energy consumption at a battery powered node or in the overall network, while running network simulations. This requires the underlying simulator to support energy consumption and energy source modeling. Currently however, ns-3 does not provide any support for modeling energy consumption or energy sources. In this paper, we introduce an integrated energy framework for ns-3, with models for energy source as well as energy consumption. We present the design and implementation of the overall framework and the specific models therein. Further, we show how the proposed framework can be used in ns-3 to simulate energy-aware protocols in a wireless network."" based on the user's previous titles: ""['Combinatorial Algorithms for Control of Biological Regulatory Networks.', 'Key distribution for secure multimedia multicasts via data embedding']""."
39," We present a pioneering study on machine dating of manuscripts written by an individual. Analysis of handwriting style forms the core of the proposed method. A general framework is presented for automatic time stamping of handwritten manuscripts. Initially, it is hypothesized that a manuscript can be dated, to a certain level of accuracy, by looking at the way it is written. The hypothesis is then verified with real samples of known dates. Experiments on a database containing manuscripts of Gustave Flaubert (1821-1880), the famous French novelist, reports about 62% accuracy while dating the manuscripts within a range of five calendar years with respect to their exact year of writing. (C) 2008 SPIE and IS&T.","['general', 'automatic', 'handwritten', 'certain', 'real', '18211880', 'famous', 'French', 'exact']","['generalized', 'the', 'algorithms', 'partial', 'regular', 'drawings', 'the', 'best', 'better', 'possible', 'problem', '4Connected', 'Multicolorings', 'SiteOriented', 'best', 'the', 'Doughnut', 'Expressions', 'Independent', 'Schnyder', 'almost', 'the']","['Towards a better understanding of random forests through the study of strength and correlation', 'Digitizing cultural heritage manuscripts: the Bovary project']","Paraphrase the following paper title without adding any other information: "" We present a pioneering study on machine dating of manuscripts written by an individual. Analysis of handwriting style forms the core of the proposed method. A general framework is presented for automatic time stamping of handwritten manuscripts. Initially, it is hypothesized that a manuscript can be dated, to a certain level of accuracy, by looking at the way it is written. The hypothesis is then verified with real samples of known dates. Experiments on a database containing manuscripts of Gustave Flaubert (1821-1880), the famous French novelist, reports about 62% accuracy while dating the manuscripts within a range of five calendar years with respect to their exact year of writing. (C) 2008 SPIE and IS&T."" based on the user's previous titles: ""['Towards a better understanding of random forests through the study of strength and correlation', 'Digitizing cultural heritage manuscripts: the Bovary project']""."
40," Performing random walks in networks is a fundamental primitive that has found numerous applications in communication networks such as token management, load balancing, network topology discovery and construction, search, and peer-to-peer membership management. While several such algorithms are ubiquitous, and use numerous random walk samples, the walks themselves have always been performed naively.In this paper, we focus on the problem of performing random walk sampling efficiently in a distributed network. Given bandwidth constraints, the goal is to minimize the number of rounds and messages required to obtain several random walk samples in a continuous online fashion. We present the first round and message optimal distributed algorithms that present a significant improvement on all previous approaches. The theoretical analysis and comprehensive simulations of our algorithms show that they perform very well in different types of networks of differing topologies.In particular, our results show how several random walks can be performed continuously (when source nodes are provided only at runtime, i.e., online), such that each walk of length ¿ can be performed exactly in just O ¿ ( ¿ D ) rounds11Throughout this paper, O ¿ hides polylogarithmic factors in the number of nodes in the network. (where D is the diameter of the network), and O ( ¿ ) messages. This significantly improves upon both, the naive technique that requires O ( ¿ ) rounds and O ( ¿ ) messages, and the sophisticated algorithm of Das Sarma et¿al. (2013) that has the same round complexity as this paper but requires ¿ ( m ¿ ) messages (where m is the number of edges in the network). Our theoretical results are corroborated through extensive simulations on various topological data sets. Our algorithms are fully decentralized, lightweight, and easily implementable, and can serve as building blocks in the design of topologically-aware networks. We consider the problem of sampling nodes through random walks in a distributed network.We present algorithm to compute several random walk samples in a continuous online fashion.Our algorithm minimizes (almost optimal) the number of rounds and messages.Our algorithm is effective and efficient as shown by experimental evaluation on various topological networks.Our algorithm performs very well on various metrics for all parameter ranges.","['random', 'fundamental', 'numerous', 'such', 'token', 'peertopeer', 'several', 'such', 'ubiquitous', 'numerous', 'distributed', 'bandwidth', 'several', 'random', 'continuous', 'online', 'first', 'optimal', 'significant', 'previous', 'theoretical', 'comprehensive', 'different', 'several', 'random', 'such', 'polylogarithmic', 'naive', 'sophisticated', 'same', 'theoretical', 'extensive', 'various', 'topological', 'lightweight', 'implementable', 'topologicallyaware', 'random', 'distributed', 'networkWe', 'present', 'several', 'random', 'continuous', 'effective', 'efficient', 'experimental', 'various', 'topological', 'various']","['combinatorial', 'linear', 'undirected', 'key', 'necessary', 'problem', 'Various', 'with', 'are', 'necessary', 'possible', 'without', 'Queue', 'cards', 'exchange', 'threshold', 'ANDEXOR', 'edgedisjoint', 'nonhamiltonian', 'vertexrankings', 'Various', 'consecutive', 'the', 'are', 'necessary', 'possible', 'without', 'almost', 'compact', 'computing', 'networks', 'Various', 'with', 'Distribution', 'designated', 'networks', 'weighted', 'Bandwidth', 'bandwidth', 'networks', 'tradeoffs', 'Various', 'consecutive', 'the', 'combinatorial', 'linear', 'undirected', 'linear', 'Web', 'best', 'exchange', 'networks', 'regular', 'the', 'Optimal', 'maximal', 'maximum', 'optimal', 'key', 'necessary', 'sufficient', 'consecutive', 'the', 'approximation', 'combinatorial', 'computability', 'generalized', 'maximal', 'balanced', 'best', 'complete', 'partial', 'Various', 'are', 'better', 'than', 'Various', 'consecutive', 'the', 'combinatorial', 'linear', 'undirected', 'are', 'necessary', 'possible', 'without', 'Boolean', 'combinatorial', 'computability', 'subgraphs', 'treewidth', 'generalized', 'simply', 'solvable', 'symmetric', 'undirected', 'Simple', 'algorithms', 'compact', 'complexity', 'the', 'approximation', 'combinatorial', 'computability', 'generalized', 'maximal', 'complete', 'depth', 'partial', 'Various', 'the', 'Hamiltonian', 'subgraphs', 'balanced', 'compact', 'optimal', 'algorithms', 'approximation', 'possible', 'solvable', 'Approximability', 'NPComplete', 'PolynomialTime', 'edgedisjoint', 'lineartime', 'combinatorial', 'linear', 'undirected', 'Distribution', 'designated', 'networks', 'weighted', 'Subtrees', 'VertexRankings', 'vertexrankings', 'the', 'Various', 'consecutive', 'the', 'combinatorial', 'linear', 'undirected', 'linear', 'Efficient', 'necessary', 'optimal', 'Efficient', 'optimal', 'algorithm', 'combinatorial', 'protocols', 'Various', 'the', 'Hamiltonian', 'subgraphs', 'Various', 'the']","['On the complexity of information spreading in dynamic networks', 'Brief announcement: a fast distributed approximation algorithm for minimum spanning trees in the SINR model']","Paraphrase the following paper title without adding any other information: "" Performing random walks in networks is a fundamental primitive that has found numerous applications in communication networks such as token management, load balancing, network topology discovery and construction, search, and peer-to-peer membership management. While several such algorithms are ubiquitous, and use numerous random walk samples, the walks themselves have always been performed naively.In this paper, we focus on the problem of performing random walk sampling efficiently in a distributed network. Given bandwidth constraints, the goal is to minimize the number of rounds and messages required to obtain several random walk samples in a continuous online fashion. We present the first round and message optimal distributed algorithms that present a significant improvement on all previous approaches. The theoretical analysis and comprehensive simulations of our algorithms show that they perform very well in different types of networks of differing topologies.In particular, our results show how several random walks can be performed continuously (when source nodes are provided only at runtime, i.e., online), such that each walk of length ¿ can be performed exactly in just O ¿ ( ¿ D ) rounds11Throughout this paper, O ¿ hides polylogarithmic factors in the number of nodes in the network. (where D is the diameter of the network), and O ( ¿ ) messages. This significantly improves upon both, the naive technique that requires O ( ¿ ) rounds and O ( ¿ ) messages, and the sophisticated algorithm of Das Sarma et¿al. (2013) that has the same round complexity as this paper but requires ¿ ( m ¿ ) messages (where m is the number of edges in the network). Our theoretical results are corroborated through extensive simulations on various topological data sets. Our algorithms are fully decentralized, lightweight, and easily implementable, and can serve as building blocks in the design of topologically-aware networks. We consider the problem of sampling nodes through random walks in a distributed network.We present algorithm to compute several random walk samples in a continuous online fashion.Our algorithm minimizes (almost optimal) the number of rounds and messages.Our algorithm is effective and efficient as shown by experimental evaluation on various topological networks.Our algorithm performs very well on various metrics for all parameter ranges."" based on the user's previous titles: ""['On the complexity of information spreading in dynamic networks', 'Brief announcement: a fast distributed approximation algorithm for minimum spanning trees in the SINR model']""."
41," Efficiently solving large-scale sparse linear systems is important for robot mapping and navigation. Recently, the subgraph-preconditioned conjugate gradient method has been proposed to combine the advantages of two reigning paradigms, direct and iterative methods, to improve the efficiency of the solver. Yet the question of how to pick a good subgraph is still an open problem. In this paper, we propose a new metric to measure the quality of a spanning tree preconditioner based on support theory. We use this metric to develop an algorithm to find good subgraph preconditioners and apply them to solve the SLAM problem. The results show that although the proposed algorithm is not fast enough, the new metric is effective and resulting subgraph preconditioners significantly improve the efficiency of the state-of-the-art solver.","['largescale', 'linear', 'important', 'subgraphpreconditioned', 'direct', 'iterative', 'good', 'open', 'new', 'metric', 'metric', 'good', 'new', 'metric', 'effective', 'subgraph', 'stateoftheart']","['generalized', 'parallel', 'partial', 'linear', 'best', 'key', 'necessary', 'Approximability', 'PolynomialTime', 'nonhamiltonian', 'vertexrankings', 'parallel', 'partial', 'possible', 'algorithm', 'best', 'better', 'for', 'necessary', 'sufficient', 'around', 'exchange', 'into', 'the', 'the', 'approximation', 'graphs', 'approximation', 'graphs', 'best', 'better', 'for', 'necessary', 'sufficient', 'the', 'approximation', 'graphs', 'Efficient', 'necessary', 'optimal', 'Subgraphs', 'subgraphs', 'treewidth', 'Improvements', 'Sufficient', 'algorithms', 'computing']","['Simple deterministic approximation algorithms for counting matchings', 'Randomized greedy: new variants of some classic approximation algorithms']","Paraphrase the following paper title without adding any other information: "" Efficiently solving large-scale sparse linear systems is important for robot mapping and navigation. Recently, the subgraph-preconditioned conjugate gradient method has been proposed to combine the advantages of two reigning paradigms, direct and iterative methods, to improve the efficiency of the solver. Yet the question of how to pick a good subgraph is still an open problem. In this paper, we propose a new metric to measure the quality of a spanning tree preconditioner based on support theory. We use this metric to develop an algorithm to find good subgraph preconditioners and apply them to solve the SLAM problem. The results show that although the proposed algorithm is not fast enough, the new metric is effective and resulting subgraph preconditioners significantly improve the efficiency of the state-of-the-art solver."" based on the user's previous titles: ""['Simple deterministic approximation algorithms for counting matchings', 'Randomized greedy: new variants of some classic approximation algorithms']""."
42," Human motion modeling for animation and VR has reached a level of capturing real motion data from people using various visual and non-visual sensors. However, most of the available systems require special devices and environment, or even attach extra things onto an actor. To realize a personal system, we developed general type software to acquire arbitrary motion from a video sequence. A 3D articulate human model with changeable size, color and surface shape is constructed and personalized to fit with a focused figure in the video.Either the personal model is then driven automatically or manually to match with the moving body in the consecutive image frames. This matching starts from key frames that contain key poses. A smooth motion is then interpolated in between key frames. In addition, the evaluation of the generated motion is enhanced by image correlation. We provide various methods to make the matching feasible in order to reduce the modeling time. This approach is suitable for personal use to meet wide needs of human motion acquisition.","['real', 'various', 'visual', 'nonvisual', 'most', 'available', 'special', 'extra', 'personal', 'general', 'arbitrary', 'human', 'changeable', 'focused', 'personal', 'consecutive', 'key', 'key', 'smooth', 'key', 'generated', 'various', 'feasible', 'suitable', 'personal', 'wide', 'human']","['best', 'better', 'possible', 'problem', 'Various', 'the', 'color', 'depth', 'drawings', 'complexity', 'computing', 'functions', 'linear', 'orthogonal', 'best', 'the', 'are', 'for', 'necessary', 'possible', 'for', 'regular', 'secret', 'for', 'maximum', 'necessary', 'sufficient', 'and', 'family', 'sharing', 'generalized', 'the', 'approximation', 'generalized', 'linear', 'orthogonal', 'approximation', 'complexity', 'face', 'power', 'reducible', 'Shapes', 'color', 'compact', 'reducible', 'simply', 'balanced', 'on', 'and', 'family', 'sharing', 'Consecutive', 'consecutive', 'regular', 'shortest', 'Key', 'key', 'Key', 'key', 'balanced', 'compact', 'contours', 'linear', 'Key', 'key', 'algorithms', 'flows', 'using', 'Various', 'the', 'necessary', 'optimal', 'possible', 'sufficient', 'for', 'and', 'family', 'sharing', 'Spanning', 'depth', 'approximation', 'complexity', 'face', 'power', 'reducible']","['Key views for visualizing large spaces', 'Locating key views for image indexing of spaces']","Paraphrase the following paper title without adding any other information: "" Human motion modeling for animation and VR has reached a level of capturing real motion data from people using various visual and non-visual sensors. However, most of the available systems require special devices and environment, or even attach extra things onto an actor. To realize a personal system, we developed general type software to acquire arbitrary motion from a video sequence. A 3D articulate human model with changeable size, color and surface shape is constructed and personalized to fit with a focused figure in the video.Either the personal model is then driven automatically or manually to match with the moving body in the consecutive image frames. This matching starts from key frames that contain key poses. A smooth motion is then interpolated in between key frames. In addition, the evaluation of the generated motion is enhanced by image correlation. We provide various methods to make the matching feasible in order to reduce the modeling time. This approach is suitable for personal use to meet wide needs of human motion acquisition."" based on the user's previous titles: ""['Key views for visualizing large spaces', 'Locating key views for image indexing of spaces']""."
43," The emergence of location-aware services calls for new real time spatio-temporal query processing algorithms that deal with large numbers of mobile objects and queries. Online query response is an important characterization of location-aware services. A delay in the answer to a query gives invalid and obsolete results, simply because moving objects can change their locations before the query responds. To handle large numbers of spatio-temporal queries efficiently, we propose the idea of sharing as a means to achieve scalability. In this paper, we introduce several types of sharing in the context of continuous spatio-temporal queries. Examples of sharing in the context of real-time spatio-temporal database systems include sharing the execution, sharing the underlying space, sharing the sliding time windows, and sharing the objects of interest. We demonstrate how sharing can be integrated into query predicates, e.g., selection and spatial join processing. The goal of this paper is to outline research directions and approaches that will lead to scalable and efficient location-aware services.","['locationaware', 'new', 'real', 'spatiotemporal', 'large', 'mobile', 'important', 'locationaware', 'invalid', 'obsolete', 'large', 'spatiotemporal', 'several', 'continuous', 'spatiotemporal', 'realtime', 'spatiotemporal', 'underlying', 'spatial', 'scalable', 'efficient', 'locationaware']","['edgecoloring', 'fourconnected', 'the', 'best', 'better', 'possible', 'problem', 'linear', 'Small', 'a', 'size', 'Connected', 'Web', 'compact', 'computing', 'networks', 'best', 'key', 'necessary', 'edgecoloring', 'fourconnected', 'necessary', 'partition', 'simply', 'sufficient', 'unate', 'almost', 'necessary', 'possible', 'simply', 'Small', 'a', 'size', 'linear', 'Various', 'consecutive', 'the', 'linear', 'linear', 'algorithm', 'algorithms', 'linear', 'complexity', 'problem', 'problems', 'linear', 'Efficient', 'computing', 'optimal', 'Efficient', 'optimal', 'edgecoloring', 'fourconnected']","['Power: a metric for evaluating watermarking algorithms', 'Efficient subsequence search in databases']","Paraphrase the following paper title without adding any other information: "" The emergence of location-aware services calls for new real time spatio-temporal query processing algorithms that deal with large numbers of mobile objects and queries. Online query response is an important characterization of location-aware services. A delay in the answer to a query gives invalid and obsolete results, simply because moving objects can change their locations before the query responds. To handle large numbers of spatio-temporal queries efficiently, we propose the idea of sharing as a means to achieve scalability. In this paper, we introduce several types of sharing in the context of continuous spatio-temporal queries. Examples of sharing in the context of real-time spatio-temporal database systems include sharing the execution, sharing the underlying space, sharing the sliding time windows, and sharing the objects of interest. We demonstrate how sharing can be integrated into query predicates, e.g., selection and spatial join processing. The goal of this paper is to outline research directions and approaches that will lead to scalable and efficient location-aware services."" based on the user's previous titles: ""['Power: a metric for evaluating watermarking algorithms', 'Efficient subsequence search in databases']""."
44," In many areas of life, we now have almost complete electronic archives reaching back for well over two decades. This includes, for example, the body of research papers in computer science, all news articles written in the US, and most people's personal email. However, we have only rather limited methods for analyzing and understanding these collections. While keyword-based retrieval systems allow efficient access to individual documents in archives, we still lack methods for understanding a corpus as a whole. In this paper, we explore methods that provide a temporal summary of such corpora in terms of landmark documents, authors, and topics. In particular, we explicitly model the temporal nature of influence between documents and re-interpret summarization as a coverage problem over words anchored in time. The resulting models provide monotone sub-modular objectives for computing informative and non-redundant summaries over time, which can be efficiently optimized with greedy algorithms. Our empirical study shows the effectiveness of our approach over several baselines.","['many', 'complete', 'electronic', 'most', 'personal', 'limited', 'keywordbased', 'efficient', 'individual', 'temporal', 'such', 'particular', 'temporal', 'reinterpret', 'resulting', 'submodular', 'informative', 'nonredundant', 'empirical', 'several']","['Various', 'are', 'the', 'Partial', 'complete', 'partial', 'total', 'Digital', 'circuits', 'computing', 'best', 'the', 'and', 'family', 'sharing', 'maximum', 'partial', 'sufficient', 'Subtrees', 'edgedisjoint', 'fivecoloring', 'vertexrankings', 'Efficient', 'optimal', 'necessary', 'simply', 'the', 'linear', 'are', 'necessary', 'possible', 'without', 'the', 'linear', 'to', 'partial', 'the', 'combinatorial', 'computability', 'multigraphs', 'treewidth', 'Nicely', 'balanced', 'graphs', 'sharing', 'combinatorial', 'maximal', 'subgraphs', 'approximation', 'characterization', 'combinatorial', 'generalized', 'graphs', 'Various', 'consecutive', 'the']","['Improving Recommender Systems Beyond the Algorithm.', 'Estimating the Generalization Performance of an SVM Efficiently']","Paraphrase the following paper title without adding any other information: "" In many areas of life, we now have almost complete electronic archives reaching back for well over two decades. This includes, for example, the body of research papers in computer science, all news articles written in the US, and most people's personal email. However, we have only rather limited methods for analyzing and understanding these collections. While keyword-based retrieval systems allow efficient access to individual documents in archives, we still lack methods for understanding a corpus as a whole. In this paper, we explore methods that provide a temporal summary of such corpora in terms of landmark documents, authors, and topics. In particular, we explicitly model the temporal nature of influence between documents and re-interpret summarization as a coverage problem over words anchored in time. The resulting models provide monotone sub-modular objectives for computing informative and non-redundant summaries over time, which can be efficiently optimized with greedy algorithms. Our empirical study shows the effectiveness of our approach over several baselines."" based on the user's previous titles: ""['Improving Recommender Systems Beyond the Algorithm.', 'Estimating the Generalization Performance of an SVM Efficiently']""."
45," The introduction of multicore processors on desktops and other personal computing platforms has given rise to multiple interesting end-user application possibilities. One important trend is the increased presence of resource hungry applications like gaming and multimedia applications. One of the key distinguishing factors of these applications is that they are amenable to variable semantics (ie, multiple possibilities of results) unlike traditional applications wherein a fixed, unique answer is expected. For example, varying degrees of image processing improves picture quality, different model complexities used in game physics allow different degrees of realism during game play, and so on. The goal of this paper is to demonstrate that scalable semantics in applications such as video games can be enriched with optional tasks that can be launched and thus adapt to the amount of available resources at runtime. We propose a C/C++ API that allows the programmer to define how the current semantics of a program can be opportunistically enriched, as well as the underlying runtime system that orchestrates the different computations We show how this infrastructure can be used to enrich a well known game called Quake 3. Our results show that it is possible to perform significant enrichment without degrading the application's performance by utilizing additional cores.","['other', 'personal', 'interesting', 'enduser', 'important', 'increased', 'hungry', 'key', 'amenable', 'variable', 'multiple', 'traditional', 'fixed', 'unique', 'different', 'different', 'scalable', 'such', 'video', 'optional', 'available', 'current', 'different', 'possible', 'significant', 'additional']","['the', 'and', 'family', 'sharing', 'And', 'Nicely', 'finding', 'possible', 'complexity', 'computing', 'demand', 'tradeoffs', 'best', 'key', 'necessary', 'Improved', 'demand', 'maximum', 'demand', 'supply', 'Key', 'key', 'better', 'necessary', 'reducible', 'simply', 'sufficient', 'linear', 'Various', 'consecutive', 'parallel', 'the', 'using', 'problem', 'problems', 'solvable', 'balanced', 'best', 'optimal', 'Various', 'are', 'better', 'than', 'Various', 'are', 'better', 'than', 'Efficient', 'computing', 'optimal', 'are', 'necessary', 'possible', 'without', 'embedding', 'graph', 'An', 'Necessary', 'complete', 'necessary', 'are', 'for', 'necessary', 'possible', 'the', 'Various', 'are', 'better', 'than', 'best', 'necessary', 'optimal', 'possible', 'key', 'necessary', 'sufficient', 'for', 'necessary', 'sufficient']","['An Empirical Study of the I Test for Exact Data Dependence', 'A profile-driven statistical analysis framework for the design optimization of soft real-time applications']","Paraphrase the following paper title without adding any other information: "" The introduction of multicore processors on desktops and other personal computing platforms has given rise to multiple interesting end-user application possibilities. One important trend is the increased presence of resource hungry applications like gaming and multimedia applications. One of the key distinguishing factors of these applications is that they are amenable to variable semantics (ie, multiple possibilities of results) unlike traditional applications wherein a fixed, unique answer is expected. For example, varying degrees of image processing improves picture quality, different model complexities used in game physics allow different degrees of realism during game play, and so on. The goal of this paper is to demonstrate that scalable semantics in applications such as video games can be enriched with optional tasks that can be launched and thus adapt to the amount of available resources at runtime. We propose a C/C++ API that allows the programmer to define how the current semantics of a program can be opportunistically enriched, as well as the underlying runtime system that orchestrates the different computations We show how this infrastructure can be used to enrich a well known game called Quake 3. Our results show that it is possible to perform significant enrichment without degrading the application's performance by utilizing additional cores."" based on the user's previous titles: ""['An Empirical Study of the I Test for Exact Data Dependence', 'A profile-driven statistical analysis framework for the design optimization of soft real-time applications']""."
46,"  Software engineering and evolution techniques have recently started to exploit the natural language information in source code. A key step in doing so is splitting identifiers into their constituent words. While simple in concept, identifier splitting raises several challenging issues, leading to a range of splitting techniques. Consequently, the research community would benefit from a dataset (i.e., a gold set) that facilitates comparative studies of identifier splitting techniques. A gold set of 2,663 split identifiers was constructed from 8,522 individual human splitting judgements and can be obtained from www.cs.loyola.edu/~binkley/ludiso. This set's construction and observations aimed at its effective use are described. 

","['natural', 'key', 'simple', 'identifier', 'several', 'comparative', 'individual', 'human', 'effective']","['colorings', 'forests', 'undirected', 'Key', 'key', 'Simple', 'compact', 'linear', 'simply', 'algorithm', 'designated', 'routing', 'Various', 'consecutive', 'the', 'characterization', 'depth', 'generalized', 'partial', 'necessary', 'simply', 'the', 'approximation', 'complexity', 'face', 'power', 'reducible', 'Efficient', 'necessary', 'optimal']","['The use of shared forests in tree adjoining grammar parsing', 'eGIFT: Mining Gene Information from the Literature.']","Paraphrase the following paper title without adding any other information: ""  Software engineering and evolution techniques have recently started to exploit the natural language information in source code. A key step in doing so is splitting identifiers into their constituent words. While simple in concept, identifier splitting raises several challenging issues, leading to a range of splitting techniques. Consequently, the research community would benefit from a dataset (i.e., a gold set) that facilitates comparative studies of identifier splitting techniques. A gold set of 2,663 split identifiers was constructed from 8,522 individual human splitting judgements and can be obtained from www.cs.loyola.edu/~binkley/ludiso. This set's construction and observations aimed at its effective use are described. 

"" based on the user's previous titles: ""['The use of shared forests in tree adjoining grammar parsing', 'eGIFT: Mining Gene Information from the Literature.']""."
47," In this paper, we improve the bounds for computing a network decomposition, which is a basic notion in distributed graph algorithms, distributively and deterministically. Our algorithm computes an $(n^{\epsilon(n)},(n^{\epsilon(n)})$-decomposition in $O(n^{\epsilon(n)})$ time, where $\epsilon(n)=O(1/ \sqrt{\log n})$. As a corollary we obtain improved deterministic bounds for distributively computing several graph structures such as maximal independent sets and $\Delta$-vertex colorings. We also show that the class of graphs $\cal G$ whose maximum degree is $O(n^{\delta(n)})$, where $\delta(n)=O(1/\log \log n)$, is complete for the task of computing a near-optimal decomposition, i.e., a $(\log n \log n)$-decomposition, in $O($polylog$(n))$ time. This is a corollary of a more general characterization, which pinpoints the weak points of existing network decomposition algorithms. Completeness is to be intended in the following sense: if we have an algorithm $\cal A$ that computes an optimal decomposition in $O($polylog$(n))$ time for graphs in $\cal G$, then we can compute an optimal decomposition in $O($polylog $(n))$ time for all graphs.","['basic', 'distributed', 'n', 'n', 'n', 'n', 'n', 'n', 'corollary', 'improved', 'deterministic', 'several', 'graph', 'such', 'maximal', 'independent', 'Delta', 'vertex', 'graphs', 'cal', 'maximum', 'n', 'complete', 'nearoptimal', 'general', 'weak', 'following', 'algorithm', 'cal', 'optimal', 'cal', 'optimal']","['functions', 'key', 'necessary', 'Distribution', 'designated', 'networks', 'weighted', 'A', 'A', 'A', 'A', 'A', 'A', 'Given', 'generalized', 'problem', 'reducible', 'Improved', 'Improvements', 'better', 'optimal', 'combinatorial', 'linear', 'Various', 'consecutive', 'the', 'graph', 'graphs', 'are', 'necessary', 'possible', 'without', 'Maximal', 'maximal', 'maximum', 'optimal', 'Independent', 'connected', 'sufficient', 'Linear', 'Plane', 'bipartition', 'subgraphs', 'symmetric', 'graphs', 'Mod', 'and', 'degrees', 'unate', 'Maximum', 'maximal', 'maximum', 'total', 'A', 'Partial', 'complete', 'partial', 'total', 'seriesparallel', 'generalized', 'the', 'balanced', 'better', 'power', 'unate', 'the', 'algorithm', 'Mod', 'and', 'degrees', 'unate', 'Optimal', 'maximal', 'maximum', 'optimal', 'Mod', 'and', 'degrees', 'unate', 'Optimal', 'maximal', 'maximum', 'optimal']","['A constructive algorithm for the Lovász local lemma on permutations', 'P5: A Protocol for Scalable Anonymous Communication']","Paraphrase the following paper title without adding any other information: "" In this paper, we improve the bounds for computing a network decomposition, which is a basic notion in distributed graph algorithms, distributively and deterministically. Our algorithm computes an $(n^{\epsilon(n)},(n^{\epsilon(n)})$-decomposition in $O(n^{\epsilon(n)})$ time, where $\epsilon(n)=O(1/ \sqrt{\log n})$. As a corollary we obtain improved deterministic bounds for distributively computing several graph structures such as maximal independent sets and $\Delta$-vertex colorings. We also show that the class of graphs $\cal G$ whose maximum degree is $O(n^{\delta(n)})$, where $\delta(n)=O(1/\log \log n)$, is complete for the task of computing a near-optimal decomposition, i.e., a $(\log n \log n)$-decomposition, in $O($polylog$(n))$ time. This is a corollary of a more general characterization, which pinpoints the weak points of existing network decomposition algorithms. Completeness is to be intended in the following sense: if we have an algorithm $\cal A$ that computes an optimal decomposition in $O($polylog$(n))$ time for graphs in $\cal G$, then we can compute an optimal decomposition in $O($polylog $(n))$ time for all graphs."" based on the user's previous titles: ""['A constructive algorithm for the Lovász local lemma on permutations', 'P5: A Protocol for Scalable Anonymous Communication']""."
48," The lack of good ""correlation"" between pre-silicon simulated delays and measured delays on silicon (silicon data) has spurred efforts on so-called silicon debug. The identification of speed-limiting paths, or simply speedpaths, in silicon debug is a crucial step, required for both ""fixing"" failing paths and for accurate learning from silicon data. We propose using characterized, pre-silicon, variational timing models to identify speedpaths that can best explain the observed delays from silicon measurements. Delays of all logic paths are written as affine functions of process parameters, called hyperplanes, and a branch and bound approach is then applied to find the ""best"" path combinations. Our method has been tested on a set of ISCAS-89 circuits and the results show that it accurately identifies the speedpaths in most cases, and that this is achieved in a very efficient manner.","['good', 'presilicon', 'simulated', 'silicon', 'socalled', 'speedlimiting', 'speedpaths', 'crucial', 'accurate', 'presilicon', 'variational', 'observed', 'logic', 'affine', 'best', 'most', 'efficient']","['best', 'better', 'for', 'necessary', 'sufficient', 'VertexRankings', 'edgedisjoint', 'vertexrankings', 'approximation', 'parallel', 'using', 'planar', 'the', 'Approximability', 'NonCrossing', 'nonhamiltonian', 'vertexrankings', 'ktrees', 'multicolorings', 'finding', 'key', 'necessary', 'approximation', 'balanced', 'better', 'complete', 'VertexRankings', 'edgedisjoint', 'vertexrankings', 'Eulerian', 'Hamiltonian', 'approximation', 'generalized', 'Given', 'are', 'designated', 'maximal', 'prescribed', 'Boolean', 'Logic', 'circuits', 'combinatorial', 'orthogonal', 'planar', 'symmetric', 'Best', 'best', 'better', 'possible', 'shortest', 'best', 'the', 'Efficient', 'optimal']","['Statistical estimation of the switching activity in digital circuits', 'On the need for statistical timing analysis']","Paraphrase the following paper title without adding any other information: "" The lack of good ""correlation"" between pre-silicon simulated delays and measured delays on silicon (silicon data) has spurred efforts on so-called silicon debug. The identification of speed-limiting paths, or simply speedpaths, in silicon debug is a crucial step, required for both ""fixing"" failing paths and for accurate learning from silicon data. We propose using characterized, pre-silicon, variational timing models to identify speedpaths that can best explain the observed delays from silicon measurements. Delays of all logic paths are written as affine functions of process parameters, called hyperplanes, and a branch and bound approach is then applied to find the ""best"" path combinations. Our method has been tested on a set of ISCAS-89 circuits and the results show that it accurately identifies the speedpaths in most cases, and that this is achieved in a very efficient manner."" based on the user's previous titles: ""['Statistical estimation of the switching activity in digital circuits', 'On the need for statistical timing analysis']""."
49," The facial performance of an individual is inherently rich in subtle deformation and timing details. Although these subtleties make the performance realistic and compelling, they often elude both motion capture and hand animation. We present a technique for adding fine-scale details and expressiveness to low-resolution art-directed facial performances, such as those created manually using a rig, via marker-based capture, by fitting a morphable model to a video, or through Kinect reconstruction using recent faceshift technology. We employ a high-resolution facial performance capture system to acquire a representative performance of an individual in which he or she explores the full range of facial expressiveness. From the captured data, our system extracts an expressiveness model that encodes subtle spatial and temporal deformation details specific to that particular individual. Once this model has been built, these details can be transferred to low-resolution art-directed performances. We demonstrate results on various forms of input; after our enhancement, the resulting animations exhibit the same nuances and fine spatial details as the captured performance, with optional temporal enhancement to match the dynamics of the actor. Finally, we show that our technique outperforms the current state-of-the-art in example-based facial animation.","['facial', 'rich', 'subtle', 'realistic', 'finescale', 'lowresolution', 'artdirected', 'facial', 'such', 'markerbased', 'morphable', 'recent', 'highresolution', 'facial', 'representative', 'full', 'facial', 'expressiveness', 'subtle', 'spatial', 'temporal', 'specific', 'particular', 'lowresolution', 'artdirected', 'various', 'same', 'fine', 'captured', 'optional', 'temporal', 'current', 'examplebased', 'facial']","['bipartition', 'contours', 'face', 'partial', 'and', 'balanced', 'balanced', 'color', 'complexity', 'contours', 'approximation', 'balanced', 'better', 'possible', 'linear', 'Approximability', 'VertexRankings', 'vertexrankings', '4Connected', '4connected', 'Triconnected', 'fourconnected', 'bipartition', 'contours', 'face', 'partial', 'are', 'necessary', 'possible', 'without', '4Connected', 'FourConnected', 'MultipleValued', 'fourconnected', 'planar', 'Given', 'consecutive', 'the', 'planar', 'bipartition', 'contours', 'face', 'partial', 'Independent', 'approximation', 'characterization', 'designated', 'complete', 'maximum', 'partial', 'bipartition', 'contours', 'face', 'partial', 'Expressions', 'complexity', 'computability', 'depth', 'balanced', 'color', 'complexity', 'contours', 'linear', 'linear', 'key', 'optimal', 'the', 'Approximability', 'VertexRankings', 'vertexrankings', '4Connected', '4connected', 'Triconnected', 'fourconnected', 'Various', 'the', 'the', 'almost', 'balanced', 'better', 'almost', 'connected', 'into', 'An', 'Necessary', 'complete', 'necessary', 'linear', 'the', '4Connected', '4connected', 'Approximability', 'fivecoloring', 'fourconnected', 'bipartition', 'contours', 'face', 'partial']","['State of the Art in Artistic Editing of Appearance, Lighting and Material', 'Filtering color mapped textures and surfaces']","Paraphrase the following paper title without adding any other information: "" The facial performance of an individual is inherently rich in subtle deformation and timing details. Although these subtleties make the performance realistic and compelling, they often elude both motion capture and hand animation. We present a technique for adding fine-scale details and expressiveness to low-resolution art-directed facial performances, such as those created manually using a rig, via marker-based capture, by fitting a morphable model to a video, or through Kinect reconstruction using recent faceshift technology. We employ a high-resolution facial performance capture system to acquire a representative performance of an individual in which he or she explores the full range of facial expressiveness. From the captured data, our system extracts an expressiveness model that encodes subtle spatial and temporal deformation details specific to that particular individual. Once this model has been built, these details can be transferred to low-resolution art-directed performances. We demonstrate results on various forms of input; after our enhancement, the resulting animations exhibit the same nuances and fine spatial details as the captured performance, with optional temporal enhancement to match the dynamics of the actor. Finally, we show that our technique outperforms the current state-of-the-art in example-based facial animation."" based on the user's previous titles: ""['State of the Art in Artistic Editing of Appearance, Lighting and Material', 'Filtering color mapped textures and surfaces']""."
50," Accurate simulation of light transport in participating media is expensive, due to the many scattering events. However, the band-limiting effect of scattering in media makes this kind of light transport very suitable for adaptive sampling and reconstruction techniques. In this work we present a novel algorithm that adaptively samples radiance from sparse points in the medium using up-to second-order occlusion-aware derivatives to determine when interpolation is appropriate. We derive our metric from each point's incoming light field. We use a proxy triangulation-based representation of the radiance reflected by the surrounding medium and geometry to efficiently compute the first- and second-order derivatives of the radiance at the cache points while accounting for occlusion changes. We validate the quality of our approach on a self-contained two-dimensional model for light transport in media. Then we show how our results generalize to practical three-dimensional scenarios, where we show much better results while reducing computation time up to a 30% compared to previous work.","['light', 'expensive', 'due', 'many', 'scattering', 'bandlimiting', 'light', 'suitable', 'adaptive', 'novel', 'sparse', 'upto', 'secondorder', 'occlusionaware', 'appropriate', 'metric', 'incoming', 'light', 'proxy', 'triangulationbased', 'first', 'secondorder', 'selfcontained', 'twodimensional', 'light', 'practical', 'threedimensional', 'better', 'previous']","['balanced', 'color', 'compact', 'power', 'best', 'better', 'compact', 'necessary', 'than', 'Given', 'the', 'Various', 'are', 'the', 'Hamiltonian', 'approximation', 'decomposition', 'orthogonal', 'Approximability', 'Noncrossing', 'vertexrankings', 'balanced', 'color', 'compact', 'power', 'for', 'algorithm', 'algorithms', 'Characterization', 'algorithm', 'characterization', 'combinatorial', 'approximation', 'compact', 'linear', 'Minimum', 'around', 'maximum', 'Approximability', 'edgedisjoint', 'nonhamiltonian', 'vertexrankings', '5coloring', 'PolynomialTime', 'Triconnected', 'multicolorings', 'necessary', 'optimal', 'possible', 'sufficient', 'approximation', 'graphs', 'routing', 'the', 'balanced', 'color', 'compact', 'power', 'approximation', 'exchange', 'routing', 'using', 'Triconnected', 'fivecoloring', 'vertexrankings', 'the', 'Approximability', 'edgedisjoint', 'nonhamiltonian', 'vertexrankings', 'compact', 'linear', 'reducible', 'planar', 'balanced', 'color', 'compact', 'power', 'compact', 'necessary', 'optimal', 'possible', 'planar', 'best', 'better', 'optimal', 'than', 'consecutive', 'the']","['Irradiance gradients in the presence of participating media and occlusions', 'State of the Art in Artistic Editing of Appearance, Lighting and Material']","Paraphrase the following paper title without adding any other information: "" Accurate simulation of light transport in participating media is expensive, due to the many scattering events. However, the band-limiting effect of scattering in media makes this kind of light transport very suitable for adaptive sampling and reconstruction techniques. In this work we present a novel algorithm that adaptively samples radiance from sparse points in the medium using up-to second-order occlusion-aware derivatives to determine when interpolation is appropriate. We derive our metric from each point's incoming light field. We use a proxy triangulation-based representation of the radiance reflected by the surrounding medium and geometry to efficiently compute the first- and second-order derivatives of the radiance at the cache points while accounting for occlusion changes. We validate the quality of our approach on a self-contained two-dimensional model for light transport in media. Then we show how our results generalize to practical three-dimensional scenarios, where we show much better results while reducing computation time up to a 30% compared to previous work."" based on the user's previous titles: ""['Irradiance gradients in the presence of participating media and occlusions', 'State of the Art in Artistic Editing of Appearance, Lighting and Material']""."
51," Enabling a digital actor to move autonomously in a virtual environment is a challenging problem that has attracted much attention in recent years. The systems proposed in several researches have been able to plan the walking motions of a humanoid on an uneven terrain. In this paper, we aim to design a planning system that can generate various types of motions for a humanoid with a unified planning approach. Based on our previous work, we add two additional motion abilities: leaping and moving obstacles into the system. In previous work, the order of moving obstacles is determined first, and then the corresponding paths for the pushing/pulling motions are generated. In this work, we take a unified approach that accounts for all types of motions at the same time. We have implemented a planning system with this unified approach for a humanoid moving in a layered virtual environment. Several simulation examples are demonstrated in this paper to illustrate the effectiveness of the system.","['digital', 'virtual', 'much', 'recent', 'several', 'able', 'walking', 'uneven', 'various', 'unified', 'previous', 'additional', 'previous', 'corresponding', 'unified', 'same', 'unified', 'layered', 'virtual', 'Several']","['Digital', 'computing', 'linear', 'computing', 'exchange', 'networks', 'parallel', 'And', 'better', 'than', 'without', 'Given', 'consecutive', 'the', 'Various', 'consecutive', 'the', 'to', 'Walk', 'around', 'finding', 'path', 'paths', 'corners', 'linear', 'parallel', 'Various', 'the', 'balanced', 'generalized', 'uniform', 'consecutive', 'the', 'for', 'necessary', 'sufficient', 'consecutive', 'the', 'designated', 'the', 'balanced', 'generalized', 'uniform', 'the', 'balanced', 'generalized', 'uniform', 'complexity', 'planar', 'computing', 'exchange', 'networks', 'parallel', 'For', 'Four', 'Various']","['Using intelligent 3D animated character as the interface for interactive digital TV system', 'Thinking hard together: the long and short of collaborative idea generation in scientific inquiry']","Paraphrase the following paper title without adding any other information: "" Enabling a digital actor to move autonomously in a virtual environment is a challenging problem that has attracted much attention in recent years. The systems proposed in several researches have been able to plan the walking motions of a humanoid on an uneven terrain. In this paper, we aim to design a planning system that can generate various types of motions for a humanoid with a unified planning approach. Based on our previous work, we add two additional motion abilities: leaping and moving obstacles into the system. In previous work, the order of moving obstacles is determined first, and then the corresponding paths for the pushing/pulling motions are generated. In this work, we take a unified approach that accounts for all types of motions at the same time. We have implemented a planning system with this unified approach for a humanoid moving in a layered virtual environment. Several simulation examples are demonstrated in this paper to illustrate the effectiveness of the system."" based on the user's previous titles: ""['Using intelligent 3D animated character as the interface for interactive digital TV system', 'Thinking hard together: the long and short of collaborative idea generation in scientific inquiry']""."
52," A content-based tile retrieval system based on the underlying multispectral Markov random field representation is introduced. Single tiles are represented by our approved textural features derived from especially efficient Markovian statistics and supplemented with Local Binary Patterns (LBP) features representing occasional tile inhomogeneities. Markovian features are on top of that also invariant to illumination colour and robust to illumination direction variations, therefore an arbitrary illuminated tiles do not negatively influence the retrieval result. The presented computer-aided tile consulting system retrieves tiles from recent tile production digital catalogues, so that the retrieved tiles have as similar pattern and/or colours to a query tile as possible. The system is verified on a large commercial tile database in a psychovisual experiment.","['contentbased', 'underlying', 'multispectral', 'Single', 'approved', 'textural', 'efficient', 'Markovian', 'occasional', 'Markovian', 'arbitrary', 'illuminated', 'presented', 'computeraided', 'recent', 'digital', 'retrieved', 'similar', 'possible', 'large', 'commercial', 'psychovisual']","['fourconnected', 'multicolorings', 'vertexrankings', 'complexity', 'problem', 'problems', 'algorithms', 'decomposition', 'orthogonal', 'Extended', 'Small', 'Revised', 'designated', 'necessary', 'prescribed', 'under', 'characterization', 'color', 'complexity', 'depth', 'Efficient', 'optimal', 'Generalized', 'a', 'partial', 'regular', 'Generalized', 'approximation', 'generalized', 'linear', 'orthogonal', 'connected', 'designated', 'rectangular', 'are', 'balanced', 'set', 'simply', '4Connected', '4connected', 'fourconnected', 'seriesparallel', 'Given', 'consecutive', 'the', 'Digital', 'computing', 'linear', 'Using', 'designated', 'using', 'weighted', 'parallel', 'using', 'with', 'best', 'necessary', 'optimal', 'possible', 'Small', 'a', 'size', 'areas', 'networks', 'supply', 'algorithm', 'algorithms']","['Bidirectional texture function modeling: a state of the art survey.', 'On optimal resampling of view and illumination dependent textures']","Paraphrase the following paper title without adding any other information: "" A content-based tile retrieval system based on the underlying multispectral Markov random field representation is introduced. Single tiles are represented by our approved textural features derived from especially efficient Markovian statistics and supplemented with Local Binary Patterns (LBP) features representing occasional tile inhomogeneities. Markovian features are on top of that also invariant to illumination colour and robust to illumination direction variations, therefore an arbitrary illuminated tiles do not negatively influence the retrieval result. The presented computer-aided tile consulting system retrieves tiles from recent tile production digital catalogues, so that the retrieved tiles have as similar pattern and/or colours to a query tile as possible. The system is verified on a large commercial tile database in a psychovisual experiment."" based on the user's previous titles: ""['Bidirectional texture function modeling: a state of the art survey.', 'On optimal resampling of view and illumination dependent textures']""."
53," In this paper, we present approaches used in text summarization, showing how they can be adapted for speech summarization and where they fall short. Informal style and apparent lack of structure in speech mean that the typical approaches used for text summarization must be extended for use with speech. We illustrate how features derived from speech can help determine summary content within two ongoing summarization projects at Columbia University.","['text', 'short', 'Informal', 'apparent', 'speech', 'typical', 'text', 'summary']","['color', 'graphs', 'Shortest', 'consecutive', 'shortest', 'Efficient', 'Orderly', 'Parallel', 'Given', 'the', 'characterization', 'contours', 'embedding', 'exchange', 'functions', 'the', 'color', 'graphs', 'graph', 'graphs']","['Tailoring explanations for the user', 'Predicting the semantic orientation of adjectives']","Paraphrase the following paper title without adding any other information: "" In this paper, we present approaches used in text summarization, showing how they can be adapted for speech summarization and where they fall short. Informal style and apparent lack of structure in speech mean that the typical approaches used for text summarization must be extended for use with speech. We illustrate how features derived from speech can help determine summary content within two ongoing summarization projects at Columbia University."" based on the user's previous titles: ""['Tailoring explanations for the user', 'Predicting the semantic orientation of adjectives']""."
54, Sentence structure is considered to be an important component of the overall linguistic quality of text. Yet few empirical studies have sought to characterize how and to what extent structural features determine fluency and linguistic quality. We report the results of experiments on the predictive power of syntactic phrasing statistics and other structural features for these aspects of text. Manual assessments of sentence fluency for machine translation evaluation and text quality for summarization evaluation are used as gold-standard. We find that many structural features related to phrase length are weakly but significantly correlated with fluency and classifiers based on the entire suite of structural features can achieve high accuracy in pairwise comparison of sentence fluency and in distinguishing machine translations from human translations. We also test the hypothesis that the learned models capture general fluency properties applicable to human-authored text. The results from our experiments do not support the hypothesis. At the same time structural features and models based on them prove to be robust for automatic evaluation of the linguistic quality of multidocument summaries.,"['important', 'overall', 'linguistic', 'few', 'empirical', 'extent', 'structural', 'linguistic', 'predictive', 'syntactic', 'other', 'structural', 'Manual', 'text', 'goldstandard', 'many', 'structural', 'entire', 'structural', 'high', 'human', 'learned', 'general', 'applicable', 'humanauthored', 'same', 'structural', 'robust', 'automatic', 'linguistic']","['best', 'key', 'necessary', 'complexity', 'optimal', 'the', 'total', 'combinatorial', 'complexity', 'computability', 'multigraphs', 'a', 'approximation', 'characterization', 'combinatorial', 'generalized', 'graphs', 'the', 'complexity', 'planar', 'problems', 'combinatorial', 'complexity', 'computability', 'multigraphs', 'algorithm', 'algorithms', 'Boolean', 'combinatorial', 'computability', 'multigraphs', 'the', 'complexity', 'planar', 'problems', 'Functions', 'Revised', 'Transmission', 'Uniform', 'color', 'graphs', 'Approximability', 'edgedisjoint', 'nonhamiltonian', 'vertexrankings', 'Various', 'are', 'the', 'complexity', 'planar', 'problems', 'the', 'complexity', 'planar', 'problems', 'Lower', 'demand', 'maximum', 'approximation', 'complexity', 'face', 'power', 'reducible', 'And', 'using', 'generalized', 'the', 'approximation', 'are', 'designated', 'necessary', 'prescribed', '4Connected', '4connected', 'SiteOriented', 'Triconnected', 'the', 'complexity', 'planar', 'problems', 'balanced', 'compact', 'optimal', 'algorithms', 'partial', 'regular', 'combinatorial', 'complexity', 'computability', 'multigraphs']","['An assessment of the accuracy of automatic evaluation in summarization', 'Automatic text summarization of newswire: lessons learned from the document understanding conference']","Paraphrase the following paper title without adding any other information: "" Sentence structure is considered to be an important component of the overall linguistic quality of text. Yet few empirical studies have sought to characterize how and to what extent structural features determine fluency and linguistic quality. We report the results of experiments on the predictive power of syntactic phrasing statistics and other structural features for these aspects of text. Manual assessments of sentence fluency for machine translation evaluation and text quality for summarization evaluation are used as gold-standard. We find that many structural features related to phrase length are weakly but significantly correlated with fluency and classifiers based on the entire suite of structural features can achieve high accuracy in pairwise comparison of sentence fluency and in distinguishing machine translations from human translations. We also test the hypothesis that the learned models capture general fluency properties applicable to human-authored text. The results from our experiments do not support the hypothesis. At the same time structural features and models based on them prove to be robust for automatic evaluation of the linguistic quality of multidocument summaries."" based on the user's previous titles: ""['An assessment of the accuracy of automatic evaluation in summarization', 'Automatic text summarization of newswire: lessons learned from the document understanding conference']""."
55, A cycle double cover (CDC) of an undirected graph is a collection of the graph's cycles such that every edge of the graph belongs to exactly two cycles. We describe a constructive method for generating all the cubic graphs that have a 6-CDC (a CDC in which every cycle has length 6). We also prove that all such graphs have a Hamiltonian cycle.,"['double', 'undirected', 'such', 'constructive', 'cubic', '6CDC', 'such', 'Hamiltonian']","['maximum', 'partial', 'total', 'multigraphs', 'undirected', 'are', 'necessary', 'possible', 'without', 'balanced', 'better', 'necessary', 'possible', 'Cubic', 'linear', '5coloring', 'ANDEXOR', 'edgecoloring', 'fivecoloring', 'are', 'necessary', 'possible', 'without', 'Eulerian', 'Hamiltonian', 'subgraphs']","['Fast linear system solution by neural networks', 'Generating all the acyclic orientations of an undirected graph']","Paraphrase the following paper title without adding any other information: "" A cycle double cover (CDC) of an undirected graph is a collection of the graph's cycles such that every edge of the graph belongs to exactly two cycles. We describe a constructive method for generating all the cubic graphs that have a 6-CDC (a CDC in which every cycle has length 6). We also prove that all such graphs have a Hamiltonian cycle."" based on the user's previous titles: ""['Fast linear system solution by neural networks', 'Generating all the acyclic orientations of an undirected graph']""."
56," Selective regression testing strategies attempt to choose an appropriate subset of test cases from among a previously run test suite for a software system, based on information about the changes made to the system to create new versions. Although there has been a significant amount of research in recent years on the design of such strategies, there has been very little investigation of their cost-effectiveness. This paper presents some computationally efficient predictors of the cost-effectiveness of the two main classes of selective regression testing approaches. These predictors are computed from data about the coverage relationship between the system under test and its test suite. The paper then describes case studies in which these predictors were used to predict the cost-effectiveness of applying two different regression testing strategies to two software systems. In one case study, the TESTTUBE method selected an average of 88.1 percent of the available test cases in each version, while the predictor predicted that 87.3 percent of the test cases would be selected on average.","['Selective', 'appropriate', 'new', 'significant', 'recent', 'such', 'little', 'efficient', 'main', 'selective', 'different', 'available']","['Partial', 'necessary', 'optimal', 'possible', 'sufficient', 'the', 'key', 'necessary', 'sufficient', 'Given', 'consecutive', 'the', 'are', 'necessary', 'possible', 'without', 'a', 'Efficient', 'optimal', 'key', 'the', 'characterization', 'combinatorial', 'partial', 'partitioning', 'Various', 'are', 'better', 'than', 'are', 'for', 'necessary', 'possible']","['Assessing the Suitability of a Standard Design Method for Modeling Software Architectures', 'A Methodology for the Design of Ada Transformation Tools in a DIANA Environment']","Paraphrase the following paper title without adding any other information: "" Selective regression testing strategies attempt to choose an appropriate subset of test cases from among a previously run test suite for a software system, based on information about the changes made to the system to create new versions. Although there has been a significant amount of research in recent years on the design of such strategies, there has been very little investigation of their cost-effectiveness. This paper presents some computationally efficient predictors of the cost-effectiveness of the two main classes of selective regression testing approaches. These predictors are computed from data about the coverage relationship between the system under test and its test suite. The paper then describes case studies in which these predictors were used to predict the cost-effectiveness of applying two different regression testing strategies to two software systems. In one case study, the TESTTUBE method selected an average of 88.1 percent of the available test cases in each version, while the predictor predicted that 87.3 percent of the test cases would be selected on average."" based on the user's previous titles: ""['Assessing the Suitability of a Standard Design Method for Modeling Software Architectures', 'A Methodology for the Design of Ada Transformation Tools in a DIANA Environment']""."
57," This technical note studies the consensus problem for cooperative agents with nonlinear dynamics in a directed network. Both local and global consensus are defined and investigated. Techniques for studying the synchronization in such complex networks are exploited to establish various sufficient conditions for reaching consensus. The local consensus problem is first studied via a combination of the tools of complex analysis, local consensus manifold approach, and Lyapunov methods. A generalized algebraic connectivity is then proposed to study the global consensus problem in strongly connected networks and also in a broad class of networks containing spanning trees, for which ideas from algebraic graph theory, matrix theory, and Lyapunov methods are utilized.","['technical', 'cooperative', 'nonlinear', 'directed', 'local', 'global', 'such', 'complex', 'various', 'sufficient', 'local', 'complex', 'local', 'generalized', 'global', 'broad', 'algebraic', 'utilized']","['complexity', 'drawings', 'necessary', 'problems', 'balanced', 'exchange', 'sharing', 'symmetric', 'linear', 'multigraphs', 'undirected', 'the', 'complexity', 'computing', 'demand', 'networks', 'are', 'necessary', 'possible', 'without', 'combinatorial', 'complexity', 'problems', 'solvable', 'Various', 'the', 'Sufficient', 'maximal', 'necessary', 'sufficient', 'the', 'combinatorial', 'complexity', 'problems', 'solvable', 'the', 'Generalized', 'generalized', 'complexity', 'computing', 'demand', 'networks', 'a', 'generalized', 'Boolean', 'combinatorial', 'computability', 'multigraphs', 'designated', 'necessary', 'sufficient', 'using']","['Some necessary and sufficient conditions for second-order consensus in multi-agent dynamical systems', 'Convexification of the range-only station keeping problem']","Paraphrase the following paper title without adding any other information: "" This technical note studies the consensus problem for cooperative agents with nonlinear dynamics in a directed network. Both local and global consensus are defined and investigated. Techniques for studying the synchronization in such complex networks are exploited to establish various sufficient conditions for reaching consensus. The local consensus problem is first studied via a combination of the tools of complex analysis, local consensus manifold approach, and Lyapunov methods. A generalized algebraic connectivity is then proposed to study the global consensus problem in strongly connected networks and also in a broad class of networks containing spanning trees, for which ideas from algebraic graph theory, matrix theory, and Lyapunov methods are utilized."" based on the user's previous titles: ""['Some necessary and sufficient conditions for second-order consensus in multi-agent dynamical systems', 'Convexification of the range-only station keeping problem']""."
58," We present a GPU based implementation of Reyes-style adaptive surface subdivision, known in Reyes terminology as the Bound/Split and Dice stages. The performance of this task is important for the Reyes pipeline to map efficiently to graphics hardware, but its recursive nature and irregular and unbounded memory requirements present a challenge to an efficient implementation. Our solution begins by characterizing Reyes subdivision as a work queue with irregular computation, targeted to a massively parallel GPU. We propose efficient solutions to these general problems by casting our solution in terms of the fundamental primitives of prefix-sum and reduction, often encountered in parallel and GPGPU environments. Our results indicate that real-time Reyes subdivision can indeed be obtained on today's GPUs. We are able to subdivide a complex model to subpixel accuracy within 15 ms. Our measured performance is several times better than that of Pixar's RenderMan. Our implementation scales well with the input size and depth of subdivision. We also address concerns of memory size and bandwidth, and analyze the feasibility of conventional ideas on screen-space buckets.","['adaptive', 'important', 'recursive', 'unbounded', 'efficient', 'irregular', 'parallel', 'efficient', 'general', 'fundamental', 'realtime', 'able', 'complex', 'measured', 'several', 'conventional', 'screenspace']","['algorithm', 'algorithms', 'best', 'key', 'necessary', 'algorithm', 'bounded', 'bounds', 'combinatorial', 'undirected', 'Efficient', 'optimal', 'planar', 'rectangular', 'regular', 'symmetric', 'Parallel', 'orthogonal', 'parallel', 'Efficient', 'optimal', 'generalized', 'the', 'key', 'necessary', 'problem', 'algorithm', 'algorithms', 'to', 'combinatorial', 'complexity', 'problems', 'solvable', 'maximal', 'threshold', 'using', 'weighted', 'Various', 'consecutive', 'the', 'linear', 'bandwidth', 'grid', 'rectangle']","['Efficient maximal poisson-disk sampling', 'Toward techniques for auto-tuning GPU algorithms']","Paraphrase the following paper title without adding any other information: "" We present a GPU based implementation of Reyes-style adaptive surface subdivision, known in Reyes terminology as the Bound/Split and Dice stages. The performance of this task is important for the Reyes pipeline to map efficiently to graphics hardware, but its recursive nature and irregular and unbounded memory requirements present a challenge to an efficient implementation. Our solution begins by characterizing Reyes subdivision as a work queue with irregular computation, targeted to a massively parallel GPU. We propose efficient solutions to these general problems by casting our solution in terms of the fundamental primitives of prefix-sum and reduction, often encountered in parallel and GPGPU environments. Our results indicate that real-time Reyes subdivision can indeed be obtained on today's GPUs. We are able to subdivide a complex model to subpixel accuracy within 15 ms. Our measured performance is several times better than that of Pixar's RenderMan. Our implementation scales well with the input size and depth of subdivision. We also address concerns of memory size and bandwidth, and analyze the feasibility of conventional ideas on screen-space buckets."" based on the user's previous titles: ""['Efficient maximal poisson-disk sampling', 'Toward techniques for auto-tuning GPU algorithms']""."
59," Using geosocial applications, such as FourSquare, millions of people interact with their surroundings through their friends and their recommendations. Without adequate privacy protection, however, these systems can be easily misused, for example, to track users or target them for home invasion. In this paper, we introduce LocX, a novel alternative that provides significantly improved location privacy without adding uncertainty into query results or relying on strong assumptions about server security. Our key insight is to apply secure user-specific, distance-preserving coordinate transformations to all location data shared with the server. The friends of a user share this user's secrets so they can apply the same transformation. This allows all location queries to be evaluated correctly by the server, but our privacy mechanisms guarantee that servers are unable to see or infer the actual location data from the transformed data or from the data access. We show that LocX provides privacy even against a powerful adversary model, and we use prototype measurements to show that it provides privacy with very little performance overhead, making it suitable for today's mobile devices.","['geosocial', 'such', 'adequate', 'novel', 'alternative', 'query', 'strong', 'server', 'key', 'secure', 'userspecific', 'distancepreserving', 'same', 'unable', 'actual', 'powerful', 'adversary', 'prototype', 'little', 'suitable', 'mobile']","['Graph', 'computing', 'networks', 'are', 'necessary', 'possible', 'without', 'Sufficient', 'necessary', 'optimal', 'sufficient', 'Characterization', 'algorithm', 'characterization', 'combinatorial', 'best', 'better', 'optimal', 'possible', 'Boolean', 'algorithm', 'cardinality', 'a', 'balanced', 'sufficient', 'Bandwidth', 'Web', 'bandwidth', 'computing', 'routing', 'Key', 'key', 'Secure', 'connected', 'necessary', 'sufficient', 'Lineartime', 'edgedisjoint', 'lineartime', 'vertexrankings', 'distanceedgecolorings', 'edgecoloring', 'fourconnected', 'the', 'necessary', 'simply', 'sufficient', 'to', 'the', 'balanced', 'best', 'compact', 'power', 'algorithm', 'characterization', 'face', 'secret', 'drawings', 'a', 'for', 'Connected', 'Web', 'compact', 'computing', 'networks']","['Secure Input for Web Applications', 'Using Generalization and Characterization Techniques in the Anomaly-based Detection of Web Attacks']","Paraphrase the following paper title without adding any other information: "" Using geosocial applications, such as FourSquare, millions of people interact with their surroundings through their friends and their recommendations. Without adequate privacy protection, however, these systems can be easily misused, for example, to track users or target them for home invasion. In this paper, we introduce LocX, a novel alternative that provides significantly improved location privacy without adding uncertainty into query results or relying on strong assumptions about server security. Our key insight is to apply secure user-specific, distance-preserving coordinate transformations to all location data shared with the server. The friends of a user share this user's secrets so they can apply the same transformation. This allows all location queries to be evaluated correctly by the server, but our privacy mechanisms guarantee that servers are unable to see or infer the actual location data from the transformed data or from the data access. We show that LocX provides privacy even against a powerful adversary model, and we use prototype measurements to show that it provides privacy with very little performance overhead, making it suitable for today's mobile devices."" based on the user's previous titles: ""['Secure Input for Web Applications', 'Using Generalization and Characterization Techniques in the Anomaly-based Detection of Web Attacks']""."
60," We propose a novel tracking algorithm based on the Wang-Landau Monte Carlo sampling method which efficiently deals with the abrupt motions. Abrupt motions could cause conventional tracking methods to fail since they violate the motion smoothness constraint. To address this problem, we introduce the Wang-Landau algorithm that has been recently proposed in statistical physics, and integrate this algorithm into the Markov Chain Monte Carlo based tracking method. Our tracking method alleviates the motion smoothness constraint utilizing both the likelihood term and the density of states term, which is estimated by the Wang-Landau algorithm. The likelihood term helps to improve the accuracy in tracking smooth motions, while the density of states term captures abrupt motions robustly. Experimental results reveal that our approach efficiently samples the object's states even in a whole state space without loss of time. Therefore, it tracks the object of which motion is drastically changing, accurately and robustly.","['novel', 'abrupt', 'conventional', 'statistical', 'likelihood', 'likelihood', 'smooth', 'abrupt', 'Experimental', 'whole']","['Characterization', 'algorithm', 'characterization', 'combinatorial', 'linear', 'linear', 'graphs', 'Given', 'maximal', 'maximum', 'possible', 'Given', 'maximal', 'maximum', 'possible', 'balanced', 'compact', 'contours', 'linear', 'linear', 'Abstract', 'Characterization', 'the']","['Shape from shading with a linear triangular element surface model', 'PaletteNet: Image Recolorization with Given Color Palette.']","Paraphrase the following paper title without adding any other information: "" We propose a novel tracking algorithm based on the Wang-Landau Monte Carlo sampling method which efficiently deals with the abrupt motions. Abrupt motions could cause conventional tracking methods to fail since they violate the motion smoothness constraint. To address this problem, we introduce the Wang-Landau algorithm that has been recently proposed in statistical physics, and integrate this algorithm into the Markov Chain Monte Carlo based tracking method. Our tracking method alleviates the motion smoothness constraint utilizing both the likelihood term and the density of states term, which is estimated by the Wang-Landau algorithm. The likelihood term helps to improve the accuracy in tracking smooth motions, while the density of states term captures abrupt motions robustly. Experimental results reveal that our approach efficiently samples the object's states even in a whole state space without loss of time. Therefore, it tracks the object of which motion is drastically changing, accurately and robustly."" based on the user's previous titles: ""['Shape from shading with a linear triangular element surface model', 'PaletteNet: Image Recolorization with Given Color Palette.']""."
61," The problem of allowing user-centric control within multimedia presentations is important to document engineering when the presentations are specified as structured multimedia documents. In this paper we investigate the problem in the context of end-user ""real-time"" editing of interactive video programs.","['usercentric', 'multimedia', 'important', 'structured', 'enduser', 'realtime', 'interactive']","['BoxRectangular', 'edgedisjoint', 'seriesparallel', 'Web', 'bandwidth', 'computing', 'embedding', 'networks', 'best', 'key', 'necessary', 'balanced', 'linear', 'weighted', 'complexity', 'computing', 'demand', 'tradeoffs', 'algorithm', 'algorithms', 'linear']","['Performance evaluation of inference services for ubiquitous computing', 'Documenting social networks']","Paraphrase the following paper title without adding any other information: "" The problem of allowing user-centric control within multimedia presentations is important to document engineering when the presentations are specified as structured multimedia documents. In this paper we investigate the problem in the context of end-user ""real-time"" editing of interactive video programs."" based on the user's previous titles: ""['Performance evaluation of inference services for ubiquitous computing', 'Documenting social networks']""."
62," A modern technique for robotic sound source detection using a dataset Head-Related Transfer Functions (HRTFs) is presented. To ensure fast detection, the HRTFs are reduced using three different techniques, namely; Diffuse-Field Equalization, Balanced Model Truncation, and Principle Component Analysis. A new criterion introduced is to be satisfied by a set of output signals from the microphones of a dummy robot head. This criterion is then used to find the sound source location in accordance with the reduced HRTF datasets. The suggested method is verified through simulated examples and further tested in a household environment. This novel technique provides estimates of azimuth and elevation angles in free space by using only two microphones. It also uses a simple algorithm compared to the more complicated algorithms used in similar localization processes.","['modern', 'robotic', 'HeadRelated', 'fast', 'different', 'new', 'dummy', 'suggested', 'simulated', 'novel', 'free', 'simple', 'complicated', 'similar']","['compact', 'the', 'algorithms', 'linear', 'Lineartime', 'MultipleValued', 'Noncrossing', 'Efficient', 'Simple', 'compact', 'possible', 'Various', 'are', 'better', 'than', 'the', 'plane', 'rectangular', 'using', 'Given', 'possible', 'simply', 'using', 'approximation', 'parallel', 'using', 'Characterization', 'algorithm', 'characterization', 'combinatorial', 'exchange', 'for', 'Simple', 'compact', 'linear', 'simply', 'complexity', 'problem', 'problems', 'parallel', 'using', 'with']","['A fast solution to the approximation of 3D scattered point data from stereo images using triangular meshes', 'HRTF customization using multiway array analysis']","Paraphrase the following paper title without adding any other information: "" A modern technique for robotic sound source detection using a dataset Head-Related Transfer Functions (HRTFs) is presented. To ensure fast detection, the HRTFs are reduced using three different techniques, namely; Diffuse-Field Equalization, Balanced Model Truncation, and Principle Component Analysis. A new criterion introduced is to be satisfied by a set of output signals from the microphones of a dummy robot head. This criterion is then used to find the sound source location in accordance with the reduced HRTF datasets. The suggested method is verified through simulated examples and further tested in a household environment. This novel technique provides estimates of azimuth and elevation angles in free space by using only two microphones. It also uses a simple algorithm compared to the more complicated algorithms used in similar localization processes."" based on the user's previous titles: ""['A fast solution to the approximation of 3D scattered point data from stereo images using triangular meshes', 'HRTF customization using multiway array analysis']""."
63," Process mining is a process management technique to extract knowledge from the event logs recorded by an information system. We show how applying an appropriate semantic lifting to the event and workflow log may help to discover the process that is actually being executed. In particular, we show how it is possible to extract not only knowledge about the structure of the process, but also to verify if some non-functional properties, such as security properties, hold during the process execution.","['appropriate', 'semantic', 'workflow', 'particular', 'possible', 'nonfunctional', 'such']","['necessary', 'optimal', 'possible', 'sufficient', 'Boolean', 'algorithms', 'computability', 'Scheduling', 'algorithms', 'protocols', 'the', 'best', 'necessary', 'optimal', 'possible', 'functions', 'simply', 'undirected', 'without', 'are', 'necessary', 'possible', 'without']","['Semantics-aware querying in the WWW: the WG-Log Web query system', 'Towards the Certification of Cloud Services']","Paraphrase the following paper title without adding any other information: "" Process mining is a process management technique to extract knowledge from the event logs recorded by an information system. We show how applying an appropriate semantic lifting to the event and workflow log may help to discover the process that is actually being executed. In particular, we show how it is possible to extract not only knowledge about the structure of the process, but also to verify if some non-functional properties, such as security properties, hold during the process execution."" based on the user's previous titles: ""['Semantics-aware querying in the WWW: the WG-Log Web query system', 'Towards the Certification of Cloud Services']""."
64," SOAP has been widely adopted as a simple, robust and extensible XML-based protocol for the exchange of messages among web services. Unfortunately, SOAP communications have two major performance-related drawbacks: i) verbosity, related to XML, that leads to increased network traffic, and ii) high computational burden of XML parsing and processing, that leads to high latency. In this paper, we address these two issues and introduce a novel framework for Differential SOAP Multicasting (DSM). The main idea consists in identifying the common pattern and differences between SOAP messages, modeled as trees, so as to multicast similar messages together. Our method is based on the well known concept of Tree Edit Distance, built upon a novel filter-differencing architecture to reduce message aggregation time, identifying only those messages which are relevant (i.e., similar enough) for similarity evaluation. In addition, our technique exploits a dedicated differencing output format specifically designed to carry the minimum amount of diff information, in the multicast message, so as to minimize the multicast message size, and therefore reducing the network traffic. The battery of simulation experiments conducted to evaluate our approach shows the relevance of our method in comparison with traditional and dedicated multicasting techniques.","['robust', 'extensible', 'XMLbased', 'web', 'major', 'performancerelated', 'high', 'computational', 'high', 'main', 'common', 'similar', 'novel', 'filterdifferencing', 'relevant', 'similar', 'dedicated', 'minimum', 'multicast', 'traditional', 'dedicated']","['balanced', 'compact', 'optimal', 'compact', 'embedding', 'orthogonal', 'planar', '4connected', 'MultiWeighted', 'MultipleValued', 'degreeconstrained', 'fourconnected', 'Web', 'bandwidth', 'computing', 'graph', 'networks', 'key', 'the', '4Connected', '4connected', 'fourconnected', 'seriesparallel', 'Lower', 'demand', 'maximum', 'algorithms', 'computability', 'computing', 'Lower', 'demand', 'maximum', 'key', 'the', 'are', 'problem', 'problems', 'parallel', 'using', 'with', 'Characterization', 'algorithm', 'characterization', 'combinatorial', 'Approximability', 'Noncrossing', 'PolynomialTime', 'key', 'necessary', 'sufficient', 'parallel', 'using', 'with', 'connected', 'designated', 'for', 'Minimum', 'maximum', 'Routing', 'bandwidth', 'networks', 'routing', 'the', 'using', 'connected', 'designated', 'for']","['Toward Enhancing Web Accessibility for Blind Users through the Semantic Web', 'Active Server for the Management of Structured Documents Link Integrity']","Paraphrase the following paper title without adding any other information: "" SOAP has been widely adopted as a simple, robust and extensible XML-based protocol for the exchange of messages among web services. Unfortunately, SOAP communications have two major performance-related drawbacks: i) verbosity, related to XML, that leads to increased network traffic, and ii) high computational burden of XML parsing and processing, that leads to high latency. In this paper, we address these two issues and introduce a novel framework for Differential SOAP Multicasting (DSM). The main idea consists in identifying the common pattern and differences between SOAP messages, modeled as trees, so as to multicast similar messages together. Our method is based on the well known concept of Tree Edit Distance, built upon a novel filter-differencing architecture to reduce message aggregation time, identifying only those messages which are relevant (i.e., similar enough) for similarity evaluation. In addition, our technique exploits a dedicated differencing output format specifically designed to carry the minimum amount of diff information, in the multicast message, so as to minimize the multicast message size, and therefore reducing the network traffic. The battery of simulation experiments conducted to evaluate our approach shows the relevance of our method in comparison with traditional and dedicated multicasting techniques."" based on the user's previous titles: ""['Toward Enhancing Web Accessibility for Blind Users through the Semantic Web', 'Active Server for the Management of Structured Documents Link Integrity']""."
65," We consider the problem of recovering from a single node failure in a storage system based on an (n, k) MDS code. In such a scenario, a straightforward solution is to perform a complete decoding, even though the data to be recovered only amount to 1/kth of the entire data. This paper presents techniques that can reduce the network traffic incurred. The techniques perform algebraic alignment so that the effective dimension of unwanted information is reduced.","['single', 'such', 'straightforward', 'complete', 'entire', 'algebraic', 'effective', 'unwanted']","['a', 'are', 'necessary', 'possible', 'without', 'Simple', 'balanced', 'possible', 'simply', 'Partial', 'complete', 'partial', 'total', 'the', 'Boolean', 'combinatorial', 'computability', 'multigraphs', 'Efficient', 'necessary', 'optimal', 'Minimizing', 'problems', 'without']","['On the mixing time of Markov Chain Monte Carlo for integer least-square problems', 'Geographic Gossip: Efficient Averaging for Sensor Networks']","Paraphrase the following paper title without adding any other information: "" We consider the problem of recovering from a single node failure in a storage system based on an (n, k) MDS code. In such a scenario, a straightforward solution is to perform a complete decoding, even though the data to be recovered only amount to 1/kth of the entire data. This paper presents techniques that can reduce the network traffic incurred. The techniques perform algebraic alignment so that the effective dimension of unwanted information is reduced."" based on the user's previous titles: ""['On the mixing time of Markov Chain Monte Carlo for integer least-square problems', 'Geographic Gossip: Efficient Averaging for Sensor Networks']""."
66, Most interactive segmentation methods are defined at a 2-D level requiring sometimes extensive interaction when the object of interest is difficult to be outlined by simple means of segmentation. This may become tedious if the data happens to be a 3-D scene of which a part of the differentiating features of a structure is buried in 3-D contextual information. We propose a concept of extending interactive or semi-automatic segmentation to 3-D where coherence information may be exploited more efficiently. A non-analysed scene may be evaluated using volume rendering techniques with which the user interacts to extract the structures that he or she is interested in. The approach is based on a concept of a co-operative system that communicates with the user for finding the best rendition and analysis. Within the concept we present a new method of estimating rendering parameters if the object of interest is only partially analysed.,"['Most', 'interactive', '2D', 'extensive', 'difficult', 'simple', 'tedious', '3D', '3D', 'contextual', 'interactive', 'semiautomatic', 'nonanalysed', 'interested', 'cooperative', 'best', 'new']","['Almost', 'For', 'Various', 'linear', 'planar', 'complete', 'depth', 'partial', 'Finding', 'finding', 'necessary', 'Simple', 'compact', 'linear', 'simply', 'almost', 'complete', 'complexity', 'necessary', 'simply', 'planar', 'planar', 'complexity', 'embedding', 'key', 'linear', 'Parametric', 'compact', 'linear', '4Connected', '4connected', 'Triconnected', 'degreeconstrained', 'are', 'finding', 'sharing', 'simply', 'balanced', 'exchange', 'sharing', 'symmetric', 'Best', 'best', 'better', 'possible', 'shortest', 'the']","['A Genetic Algorithm For Constrained Seismic Horizon Correlation', 'Segmentation of the Date in Entries of Historical Church Registers']","Paraphrase the following paper title without adding any other information: "" Most interactive segmentation methods are defined at a 2-D level requiring sometimes extensive interaction when the object of interest is difficult to be outlined by simple means of segmentation. This may become tedious if the data happens to be a 3-D scene of which a part of the differentiating features of a structure is buried in 3-D contextual information. We propose a concept of extending interactive or semi-automatic segmentation to 3-D where coherence information may be exploited more efficiently. A non-analysed scene may be evaluated using volume rendering techniques with which the user interacts to extract the structures that he or she is interested in. The approach is based on a concept of a co-operative system that communicates with the user for finding the best rendition and analysis. Within the concept we present a new method of estimating rendering parameters if the object of interest is only partially analysed."" based on the user's previous titles: ""['A Genetic Algorithm For Constrained Seismic Horizon Correlation', 'Segmentation of the Date in Entries of Historical Church Registers']""."
67," The MathLang project aims at computerizing mathematical texts according to various degrees of formalisations, and without any prior commitment to a particular logical framework (e.g., having to choose either set theory or category theory or type theory, ...","['mathematical', 'various', 'prior', 'particular', 'logical']","['algorithms', 'combinatorial', 'computability', 'Various', 'the', 'the', 'without', 'the', 'Logic', 'necessary', 'parallel', 'possible']","['Extending Maple to the Grid: Design and Implementation', 'SymGrid: A Framework for Symbolic Computation on the Grid']","Paraphrase the following paper title without adding any other information: "" The MathLang project aims at computerizing mathematical texts according to various degrees of formalisations, and without any prior commitment to a particular logical framework (e.g., having to choose either set theory or category theory or type theory, ..."" based on the user's previous titles: ""['Extending Maple to the Grid: Design and Implementation', 'SymGrid: A Framework for Symbolic Computation on the Grid']""."
68," CONTEXT: Systematic literature reviews largely rely upon using the titles and abstracts of primary studies as the basis for determining their relevance. However, our experience indicates that the abstracts for software engineering papers are frequently of such poor quality they cannot be used to determine the relevance of papers. Both medicine and psychology recommend the use of structured abstracts to improve the quality of abstracts. AIM: This study investigates whether structured abstracts are more complete and easier to understand than non-structured abstracts for software engineering papers that describe experiments. METHOD: We constructed structured abstracts for a random selection of 25 papers describing software engineering experiments. The original abstract was assessed for clarity (assessed subjectively on a scale of 1 to 10) and completeness (measured with a questionnaire of 18 items) by the researcher who constructed the structured version. The structured abstract was reviewed for clarity and completeness by another member of the research team. We used a paired 't' test to compare the word length, clarity and completeness of the original and structured abstracts. RESULTS: The structured abstracts were significantly longer than the original abstracts (size difference =106.4 words with 95% confidence interval 78.1 to 134.7). However, the structured abstracts had a higher clarity score (clarity difference= 1.47 with 95% confidence interval 0.47 to 2.41) and were more complete (completeness difference=3.39 with 95% confidence intervals 4.76 to 7.56). CONCLUSIONS: The results of this study are consistent with previous research on structured abstracts. However, in this study, the subjective estimates of completeness and clarity were made by the research team. Future work will solicit assessments of the structured and original abstracts from independent sources (students and researchers).","['Systematic', 'rely', 'primary', 'such', 'poor', 'structured', 'more', 'complete', 'easier', 'nonstructured', 'structured', 'random', 'original', 'structured', 'structured', 'paired', 'original', 'structured', 'structured', 'longer', 'original', 'structured', 'higher', 'more', 'complete', 'completeness', 'consistent', 'previous', 'structured', 'subjective', 'Future', 'structured', 'original', 'independent']","['Characterization', 'Hierarchical', 'Optimal', 'Partial', 'on', 'simply', 'using', 'key', 'the', 'are', 'necessary', 'possible', 'without', 'better', 'for', 'problems', 'balanced', 'linear', 'weighted', 'For', 'better', 'for', 'than', 'Partial', 'complete', 'partial', 'total', 'better', 'finding', 'necessary', 'possible', 'than', '4Connected', '4connected', 'fourconnected', 'multicolorings', 'balanced', 'linear', 'weighted', 'combinatorial', 'linear', 'undirected', 'the', 'balanced', 'linear', 'weighted', 'balanced', 'linear', 'weighted', 'with', 'the', 'balanced', 'linear', 'weighted', 'balanced', 'linear', 'weighted', 'better', 'simply', 'than', 'without', 'the', 'balanced', 'linear', 'weighted', 'Lower', 'better', 'maximum', 'than', 'For', 'better', 'for', 'than', 'Partial', 'complete', 'partial', 'total', 'complete', 'complexity', 'computability', 'partial', 'balanced', 'optimal', 'uniform', 'consecutive', 'the', 'balanced', 'linear', 'weighted', 'generalized', 'reducible', 'simply', 'tradeoffs', 'weighted', 'Connected', 'Energy', 'Transformation', 'balanced', 'linear', 'weighted', 'the', 'Independent', 'connected', 'sufficient']","['Service-based software: the future for flexible software', 'Investigating the applicability of the evidence-based paradigm to software engineering']","Paraphrase the following paper title without adding any other information: "" CONTEXT: Systematic literature reviews largely rely upon using the titles and abstracts of primary studies as the basis for determining their relevance. However, our experience indicates that the abstracts for software engineering papers are frequently of such poor quality they cannot be used to determine the relevance of papers. Both medicine and psychology recommend the use of structured abstracts to improve the quality of abstracts. AIM: This study investigates whether structured abstracts are more complete and easier to understand than non-structured abstracts for software engineering papers that describe experiments. METHOD: We constructed structured abstracts for a random selection of 25 papers describing software engineering experiments. The original abstract was assessed for clarity (assessed subjectively on a scale of 1 to 10) and completeness (measured with a questionnaire of 18 items) by the researcher who constructed the structured version. The structured abstract was reviewed for clarity and completeness by another member of the research team. We used a paired 't' test to compare the word length, clarity and completeness of the original and structured abstracts. RESULTS: The structured abstracts were significantly longer than the original abstracts (size difference =106.4 words with 95% confidence interval 78.1 to 134.7). However, the structured abstracts had a higher clarity score (clarity difference= 1.47 with 95% confidence interval 0.47 to 2.41) and were more complete (completeness difference=3.39 with 95% confidence intervals 4.76 to 7.56). CONCLUSIONS: The results of this study are consistent with previous research on structured abstracts. However, in this study, the subjective estimates of completeness and clarity were made by the research team. Future work will solicit assessments of the structured and original abstracts from independent sources (students and researchers)."" based on the user's previous titles: ""['Service-based software: the future for flexible software', 'Investigating the applicability of the evidence-based paradigm to software engineering']""."
69," Finding common solutions to recurring design problems or creating computer programs is one way to improve communication between designers of software products and multiple users. Nowadays, one of the practices used to find these solutions is through patterns, which have their origin in architecture and have been used since the 90s in the software world. Still, it is not easy to decide which of the existing patterns are more appropriate for use in videogames design, so that it can contribute to the ease of use of videogames. For this reason, this paper presents a proposal to obtain a set of interaction patterns focused on ease of use, which are related to the design of videogames supported in Smartphones. This, from gathering information about interaction patterns of traditional software and entertainment software.","['common', 'multiple', 'easy', 'appropriate', 'traditional']","['are', 'problem', 'problems', 'Various', 'consecutive', 'parallel', 'Simple', 'compact', 'necessary', 'possible', 'simply', 'necessary', 'optimal', 'possible', 'sufficient', 'the', 'using']","['SSP: A Simple Software Process for Small-Size Software Development Projects', 'Analyzing and Evaluating Collaborative Processes using Case Studies in the Software Development Context']","Paraphrase the following paper title without adding any other information: "" Finding common solutions to recurring design problems or creating computer programs is one way to improve communication between designers of software products and multiple users. Nowadays, one of the practices used to find these solutions is through patterns, which have their origin in architecture and have been used since the 90s in the software world. Still, it is not easy to decide which of the existing patterns are more appropriate for use in videogames design, so that it can contribute to the ease of use of videogames. For this reason, this paper presents a proposal to obtain a set of interaction patterns focused on ease of use, which are related to the design of videogames supported in Smartphones. This, from gathering information about interaction patterns of traditional software and entertainment software."" based on the user's previous titles: ""['SSP: A Simple Software Process for Small-Size Software Development Projects', 'Analyzing and Evaluating Collaborative Processes using Case Studies in the Software Development Context']""."
70," In recent work, Flanagan and Qadeer proposed atomicity declarations as a light-weight mechanism for specifying non-interference properties in concurrent programming languages such as Java, and they provided a type and eect system to verify atomicity properties. While verification of atomicity specifications via a static type system has several advantages (scalability, compositional checking), we show that verifica- tion via model-checking also has several advantages (fewer unchecked annotations, greater coverage of Java idioms, stronger verification). In particular, we show that by adapting the Bogor model-checker, we nat- urally address several properties that are dicult to check with a static type system.","['recent', 'lightweight', 'such', 'static', 'several', 'compositional', 'verifica', 'several', 'fewer', 'unchecked', 'greater', 'stronger', 'particular', 'address', 'several', 'static']","['Given', 'consecutive', 'the', 'balanced', 'compact', 'optimal', 'are', 'necessary', 'possible', 'without', 'linear', 'Various', 'consecutive', 'the', 'characterization', 'combinatorial', 'complexity', 'decomposition', 'linear', 'Routing', 'necessary', 'protocols', 'unate', 'Various', 'consecutive', 'the', 'better', 'consecutive', 'numbers', 'than', 'bounds', 'power', 'problem', 'undirected', 'better', 'maximal', 'maximum', 'than', 'balanced', 'better', 'necessary', 'than', 'the', 'problem', 'problems', 'routing', 'Various', 'consecutive', 'the', 'linear']","['Software model checking: the Bandera approach', 'Foundations of the Bandera abstraction tools']","Paraphrase the following paper title without adding any other information: "" In recent work, Flanagan and Qadeer proposed atomicity declarations as a light-weight mechanism for specifying non-interference properties in concurrent programming languages such as Java, and they provided a type and eect system to verify atomicity properties. While verification of atomicity specifications via a static type system has several advantages (scalability, compositional checking), we show that verifica- tion via model-checking also has several advantages (fewer unchecked annotations, greater coverage of Java idioms, stronger verification). In particular, we show that by adapting the Bogor model-checker, we nat- urally address several properties that are dicult to check with a static type system."" based on the user's previous titles: ""['Software model checking: the Bandera approach', 'Foundations of the Bandera abstraction tools']""."
71," The attribute of stability is regarded by some as an important attribute of software. Some claims regarding software design quality imply that what are called interfaces in Java are stable. This paper introduces some new metrics for investigating such claims, and presents some preliminary measurements from these metrics, which indicate that developers do not consistently develop stable interfaces.","['important', 'stable', 'new', 'such', 'preliminary', 'stable']","['best', 'key', 'necessary', 'balanced', 'better', 'optimal', 'the', 'are', 'necessary', 'possible', 'without', 'characterization', 'complete', 'drawings', 'partial', 'balanced', 'better', 'optimal']","['All syntax errors are not equal', 'Optimal dimension-exchange token distribution on complete binary trees']","Paraphrase the following paper title without adding any other information: "" The attribute of stability is regarded by some as an important attribute of software. Some claims regarding software design quality imply that what are called interfaces in Java are stable. This paper introduces some new metrics for investigating such claims, and presents some preliminary measurements from these metrics, which indicate that developers do not consistently develop stable interfaces."" based on the user's previous titles: ""['All syntax errors are not equal', 'Optimal dimension-exchange token distribution on complete binary trees']""."
72," Boltzmann machines are undirected graphical models with two-state stochastic variables, in which the logarithms of the clique potentials are quadratic functions of the node states. They have been widely studied in the neural computing literature, although their practical applicability has been limited by the difficulty of finding an effective learning algorithm. One well-established approach, known as mean field theory, represents the stochastic distribution using a factorized approximation. However, the corresponding learning algorithm often fails to find a good solution. We conjecture that this is due to the implicit uni-modality of the mean field approximation which is therefore unable to capture multi-modality in the true distribution. In this paper we use variational methods to approximate the stochastic distribution using multi-modal mixtures of factorized distributions. We present results for both inference and learning to demonstrate the effectiveness of this approach.","['undirected', 'graphical', 'twostate', 'stochastic', 'quadratic', 'node', 'neural', 'practical', 'effective', 'wellestablished', 'mean', 'stochastic', 'factorized', 'good', 'due', 'implicit', 'mean', 'unable', 'true', 'variational', 'stochastic', 'multimodal', 'factorized', 'present']","['multigraphs', 'undirected', 'graphs', 'Multicommodity', 'edgedisjoint', 'vertexrankings', 'combinatorial', 'linear', 'linear', 'bipartition', 'subgraphs', 'treewidth', 'algorithms', 'circuits', 'compact', 'necessary', 'optimal', 'possible', 'Efficient', 'necessary', 'optimal', 'bounded', 'combinatorial', 'protocols', 'And', 'are', 'simply', 'combinatorial', 'linear', 'multigraphs', 'symmetric', 'best', 'better', 'for', 'necessary', 'sufficient', 'Given', 'the', 'embedding', 'generalized', 'partial', 'reducible', 'And', 'are', 'simply', 'necessary', 'simply', 'sufficient', 'to', 'best', 'is', 'possible', 'Eulerian', 'Hamiltonian', 'approximation', 'generalized', 'combinatorial', 'linear', 'combinatorial', 'linear', 'multigraphs', 'symmetric', 'the']","['Learning to learn with the informative vector machine', 'Joint Modelling Of Confounding Factors And Prominent Genetic Regulators Provides Increased Accuracy In Genetical Genomics Studies']","Paraphrase the following paper title without adding any other information: "" Boltzmann machines are undirected graphical models with two-state stochastic variables, in which the logarithms of the clique potentials are quadratic functions of the node states. They have been widely studied in the neural computing literature, although their practical applicability has been limited by the difficulty of finding an effective learning algorithm. One well-established approach, known as mean field theory, represents the stochastic distribution using a factorized approximation. However, the corresponding learning algorithm often fails to find a good solution. We conjecture that this is due to the implicit uni-modality of the mean field approximation which is therefore unable to capture multi-modality in the true distribution. In this paper we use variational methods to approximate the stochastic distribution using multi-modal mixtures of factorized distributions. We present results for both inference and learning to demonstrate the effectiveness of this approach."" based on the user's previous titles: ""['Learning to learn with the informative vector machine', 'Joint Modelling Of Confounding Factors And Prominent Genetic Regulators Provides Increased Accuracy In Genetical Genomics Studies']""."
73," We establish the fundamental limits of DNA shotgun sequencing under noisy reads. We show a surprising result: for the i.i.d. DNA model, noisy reads are as good as noiseless reads, provided that the noise level is below a certain threshold which can be surprisingly high. As an example, for a uniformly distributed DNA sequence and a symmetric substitution noisy read channel, the threshold is as high as 19%.","['fundamental', 'noisy', 'surprising', 'noisy', 'good', 'noiseless', 'certain', 'high', 'symmetric', 'read', 'high']","['key', 'necessary', 'problem', 'Efficient', 'compact', 'corners', 'And', 'Given', 'finding', 'Efficient', 'compact', 'corners', 'best', 'better', 'for', 'necessary', 'sufficient', 'Efficient', 'compact', 'optimal', 'symmetric', 'the', 'Lower', 'demand', 'maximum', 'orthogonal', 'planar', 'symmetric', 'And', 'simply', 'to', 'Lower', 'demand', 'maximum']","['Efficient file synchronization: A distributed source coding approach', 'Cooperative diversity in wireless networks: Efficient protocols and outage behavior']","Paraphrase the following paper title without adding any other information: "" We establish the fundamental limits of DNA shotgun sequencing under noisy reads. We show a surprising result: for the i.i.d. DNA model, noisy reads are as good as noiseless reads, provided that the noise level is below a certain threshold which can be surprisingly high. As an example, for a uniformly distributed DNA sequence and a symmetric substitution noisy read channel, the threshold is as high as 19%."" based on the user's previous titles: ""['Efficient file synchronization: A distributed source coding approach', 'Cooperative diversity in wireless networks: Efficient protocols and outage behavior']""."
74," 
 In the last few years, an increasing number of massively distributed systems with millions of participants has emerged within
 very short time frames. Applications, such as instant messaging, file-sharing, and content distribution have attracted countless
 numbers of users. For example, Skype gained more than 2.5 millions of users within twelve months, and more than 50% of Internet
 traffic is originated by BitTorrent. These very large and still rapidly growing systems attest to a new era for the design
 and deployment of distributed systems [52]. In particular, they reflect what the major challenges are today for designing
 and implementing distributed systems: scalability, flexibility, and instant deployment.
 
 ","['last', 'few', 'short', 'such', 'instant', 'filesharing', 'content', 'countless', 'more', 'twelve', 'more', 'large', 'new', 'distributed', 'particular', 'major', 'instant']","['consecutive', 'the', 'a', 'Shortest', 'consecutive', 'shortest', 'are', 'necessary', 'possible', 'without', 'almost', 'complete', 'maximum', 'bandwidth', 'computing', 'embedding', 'networks', 'sharing', 'Web', 'bandwidth', 'embedding', 'networks', 'partitioning', 'Various', 'almost', 'simply', 'For', 'better', 'for', 'than', 'Four', 'consecutive', 'the', 'total', 'For', 'better', 'for', 'than', 'Small', 'a', 'size', 'the', 'Distribution', 'designated', 'networks', 'weighted', 'the', 'key', 'the', 'almost', 'complete', 'maximum']","['Brief announcement: lightweight key agreement and digital certificates for wireless sensor networks', 'Is the web ready for HTTP/2 server push?']","Paraphrase the following paper title without adding any other information: "" 
 In the last few years, an increasing number of massively distributed systems with millions of participants has emerged within
 very short time frames. Applications, such as instant messaging, file-sharing, and content distribution have attracted countless
 numbers of users. For example, Skype gained more than 2.5 millions of users within twelve months, and more than 50% of Internet
 traffic is originated by BitTorrent. These very large and still rapidly growing systems attest to a new era for the design
 and deployment of distributed systems [52]. In particular, they reflect what the major challenges are today for designing
 and implementing distributed systems: scalability, flexibility, and instant deployment.
 
 "" based on the user's previous titles: ""['Brief announcement: lightweight key agreement and digital certificates for wireless sensor networks', 'Is the web ready for HTTP/2 server push?']""."
75," Activity classification from smart environment data is typically done employing ad hoc solutions customised to the particular dataset at hand. In this work we introduce a general purpose collection of features for recognising human activities across datasets of different type, size and nature. The first experimental test of our feature collection achieves state of the art results on well known datasets, and we provide a feature importance analysis in order to compare the potential relevance of features for activity classification in different datasets.","['smart', 'particular', 'general', 'human', 'different', 'first', 'experimental', 'potential', 'different']","['Efficient', 'Simple', 'compact', 'the', 'generalized', 'the', 'approximation', 'complexity', 'face', 'power', 'reducible', 'Various', 'are', 'better', 'than', 'the', 'algorithm', 'combinatorial', 'protocols', 'maximum', 'possible', 'problems', 'Various', 'are', 'better', 'than']","['SpiderCrab and the emergent object: designing for the twenty-first century', 'Modelling Scenes Using the Activity within Them']","Paraphrase the following paper title without adding any other information: "" Activity classification from smart environment data is typically done employing ad hoc solutions customised to the particular dataset at hand. In this work we introduce a general purpose collection of features for recognising human activities across datasets of different type, size and nature. The first experimental test of our feature collection achieves state of the art results on well known datasets, and we provide a feature importance analysis in order to compare the potential relevance of features for activity classification in different datasets."" based on the user's previous titles: ""['SpiderCrab and the emergent object: designing for the twenty-first century', 'Modelling Scenes Using the Activity within Them']""."
76," During the last decade, the number of functions of automotive user interfaces has increased rapidly. Besides traditional controls to drive a car, driver assistance, infotainment, entertainment, and comfort systems need to be controlled while driving. This does not only affect the driver's cognitive workload but also leads to increased complexity in designing automotive user interfaces. In this paper, we provide models and tools for rapid prototyping and the evaluation of user interfaces in this context. Usually, functional prototypes of user interfaces are implemented that allow the usability and quality to be assessed with time-consuming user studies. In contrast, in our approach we use an adapted Keystroke-Level Model (KLM) that is based on empirically collected data for typical operations in the car. It takes into account the aspect of attention switching in the car between primary tasks and other tasks. We present KLM operator times that we determined in a user study as well as a formula for estimating the task completion time. The presented model is the foundation for the MI-AUI prototyping tool that we implemented for permitting the creation of automotive interfaces using tangible controls. By demonstrating a typical operation with the MI-AUI prototype, the estimated task completion time can be calculated. MI-AUI is an evaluation tool that can be quickly and easily applied in early stages of the design process without the need to involve real drivers.","['last', 'automotive', 'traditional', 'cognitive', 'automotive', 'rapid', 'user', 'functional', 'timeconsuming', 'adapted', 'typical', 'primary', 'other', 'present', 'presented', 'automotive', 'tangible', 'typical', 'early', 'real']","['consecutive', 'the', 'circuits', 'computing', 'power', 'the', 'using', 'algorithms', 'complexity', 'computability', 'computing', 'circuits', 'computing', 'power', 'demand', 'for', 'optimal', 'Web', 'algorithm', 'computing', 'compact', 'functions', 'maximal', 'optimal', 'Minimizing', 'finding', 'necessary', 'simply', 'connected', 'set', 'using', 'the', 'key', 'the', 'the', 'the', 'are', 'balanced', 'set', 'simply', 'circuits', 'computing', 'power', 'better', 'necessary', 'possible', 'reducible', 'sufficient', 'the', 'the', 'best', 'better', 'possible', 'problem']","['Environment-mediated mobile computing', 'Situated interaction in ubiquitous computing']","Paraphrase the following paper title without adding any other information: "" During the last decade, the number of functions of automotive user interfaces has increased rapidly. Besides traditional controls to drive a car, driver assistance, infotainment, entertainment, and comfort systems need to be controlled while driving. This does not only affect the driver's cognitive workload but also leads to increased complexity in designing automotive user interfaces. In this paper, we provide models and tools for rapid prototyping and the evaluation of user interfaces in this context. Usually, functional prototypes of user interfaces are implemented that allow the usability and quality to be assessed with time-consuming user studies. In contrast, in our approach we use an adapted Keystroke-Level Model (KLM) that is based on empirically collected data for typical operations in the car. It takes into account the aspect of attention switching in the car between primary tasks and other tasks. We present KLM operator times that we determined in a user study as well as a formula for estimating the task completion time. The presented model is the foundation for the MI-AUI prototyping tool that we implemented for permitting the creation of automotive interfaces using tangible controls. By demonstrating a typical operation with the MI-AUI prototype, the estimated task completion time can be calculated. MI-AUI is an evaluation tool that can be quickly and easily applied in early stages of the design process without the need to involve real drivers."" based on the user's previous titles: ""['Environment-mediated mobile computing', 'Situated interaction in ubiquitous computing']""."
77," Video games are not just played for fun; they have become a handy instrument for the cognitive, emotional, and social development of children. However, several barriers prevent many children with disabilities from playing action-oriented video games, alone or with their peers. In particular, children with severe motor disabilities, who rely on one-switch interaction for accessing electronic devices, find fast-paced games that require rapid decision-making and timely responses, completely unplayable. This article contributes to lowering such barriers by presenting GNomon (Gaming NOMON), a software framework based on the NOMON mode of interaction that allows the creation of action-oriented single-switch video games. The article reports the results of two studies that evaluate the playability and rehabilitation suitability of GNomon-based video games. The playability of GNomon-based games is evaluated by assessing their learnability, effectiveness, errors, satisfaction, memorability, and enjoyability with a group of eight children with severe motor disabilities. The suitability for pediatric rehabilitation is determined by means of a focus group with a team of speech therapists, physiotherapists, and psychologists from a Local Health Agency in Turin, Italy. The results of the playability study are positive: All children had fun playing GNomon-based video games, and seven of eight were able to interact and play autonomously. The results of the rehabilitation-suitability study also entail that GNomon-based games can be exploited in training hand-eye coordination and maintenance of selective attention over time. The article finally offers critical hindsight and reflections and shows possible new future game concepts.","['handy', 'cognitive', 'emotional', 'social', 'several', 'many', 'actionoriented', 'particular', 'severe', 'oneswitch', 'electronic', 'fastpaced', 'rapid', 'decisionmaking', 'timely', 'unplayable', 'such', 'actionoriented', 'singleswitch', 'GNomonbased', 'GNomonbased', 'severe', 'pediatric', 'positive', 'GNomonbased', 'able', 'GNomonbased', 'handeye', 'selective', 'critical', 'possible', 'new', 'future']","['Simple', 'compact', 'key', 'necessary', 'algorithms', 'complexity', 'computability', 'computing', 'balanced', 'complexity', 'depth', 'computing', 'embedding', 'family', 'networks', 'sharing', 'Various', 'consecutive', 'the', 'Various', 'are', 'the', '5coloring', 'fivecoloring', 'fourconnected', 'vertexrankings', 'the', 'face', 'problem', 'problems', '5coloring', 'BendOptimal', 'fivecoloring', 'vertexrankings', 'Digital', 'circuits', 'computing', 'balanced', 'compact', 'linear', 'demand', 'for', 'optimal', 'complexity', 'computing', 'functions', 'power', 'tradeoffs', 'Efficient', 'balanced', 'necessary', 'possible', 'Almost', 'almost', 'solvable', 'are', 'necessary', 'possible', 'without', '5coloring', 'fivecoloring', 'fourconnected', 'vertexrankings', 'multicolorings', 'nonhamiltonian', 'vertexrankings', 'Lineartime', 'Noncrossing', 'VertexRankings', 'Lineartime', 'Noncrossing', 'VertexRankings', 'face', 'problem', 'problems', 'family', 'optimal', 'protocols', 'balanced', 'better', 'key', 'unate', 'Lineartime', 'Noncrossing', 'VertexRankings', 'to', 'Lineartime', 'Noncrossing', 'VertexRankings', 'edgecoloring', 'edgedisjoint', 'vertexrankings', 'characterization', 'combinatorial', 'partial', 'partitioning', 'key', 'necessary', 'optimal', 'best', 'necessary', 'optimal', 'possible', 'the', 'for', 'possible']","['Training Engineers for the Ambient Intelligence Challenge.', 'An experimental analysis of the effectiveness of the circular self-test path technique']","Paraphrase the following paper title without adding any other information: "" Video games are not just played for fun; they have become a handy instrument for the cognitive, emotional, and social development of children. However, several barriers prevent many children with disabilities from playing action-oriented video games, alone or with their peers. In particular, children with severe motor disabilities, who rely on one-switch interaction for accessing electronic devices, find fast-paced games that require rapid decision-making and timely responses, completely unplayable. This article contributes to lowering such barriers by presenting GNomon (Gaming NOMON), a software framework based on the NOMON mode of interaction that allows the creation of action-oriented single-switch video games. The article reports the results of two studies that evaluate the playability and rehabilitation suitability of GNomon-based video games. The playability of GNomon-based games is evaluated by assessing their learnability, effectiveness, errors, satisfaction, memorability, and enjoyability with a group of eight children with severe motor disabilities. The suitability for pediatric rehabilitation is determined by means of a focus group with a team of speech therapists, physiotherapists, and psychologists from a Local Health Agency in Turin, Italy. The results of the playability study are positive: All children had fun playing GNomon-based video games, and seven of eight were able to interact and play autonomously. The results of the rehabilitation-suitability study also entail that GNomon-based games can be exploited in training hand-eye coordination and maintenance of selective attention over time. The article finally offers critical hindsight and reflections and shows possible new future game concepts."" based on the user's previous titles: ""['Training Engineers for the Ambient Intelligence Challenge.', 'An experimental analysis of the effectiveness of the circular self-test path technique']""."
78," Considering that security education needs to train students to deal with security problems in real environments, we developed new media for teaching applied cryptography and network security. They permit students to gain real-life security experience from virtual laboratories on CD, DVD, or online. The application of those media is helpful to reduce investment and administration costs as well as effectively support security experiments which previously had to be done in conventional security laboratories.","['real', 'new', 'reallife', 'virtual', 'helpful', 'support', 'conventional']","['best', 'better', 'possible', 'problem', 'the', 'Dealing', 'approximation', 'optimal', 'solvable', 'computing', 'exchange', 'networks', 'parallel', 'Efficient', 'best', 'finding', 'necessary', 'for', 'necessary', 'sufficient', 'linear']","['Privacy and security in IPv6 networks: challenges and possible solutions', 'A Flexible and Efficient Alert Correlation Platform for Distributed IDS']","Paraphrase the following paper title without adding any other information: "" Considering that security education needs to train students to deal with security problems in real environments, we developed new media for teaching applied cryptography and network security. They permit students to gain real-life security experience from virtual laboratories on CD, DVD, or online. The application of those media is helpful to reduce investment and administration costs as well as effectively support security experiments which previously had to be done in conventional security laboratories."" based on the user's previous titles: ""['Privacy and security in IPv6 networks: challenges and possible solutions', 'A Flexible and Efficient Alert Correlation Platform for Distributed IDS']""."
79," Monitoring aquatic debris is of great interest to the ecosystems, marine life, human health, and water transport. This paper presents the design and implementation of SOAR-a vision-based surveillance robot system that integrates an off-the-shelf Android smartphone and a gliding robotic fish for debris monitoring in relatively calm waters. SOAR features real-time debris detection and coverage-based rotation scheduling algorithms. The image processing algorithms for debris detection are specifically designed to address the unique challenges in aquatic environments. The rotation scheduling algorithm provides effective coverage for sporadic debris arrivals despite camera's limited angular view. Moreover, SOAR is able to dynamically offload compute-intensive processing tasks to the cloud for battery power conservation. We have implemented a SOAR prototype and conducted extensive experimental evaluation. The results show that SOAR can accurately detect debris in the presence of various environment and system dynamics, and the rotation scheduling algorithm enables SOAR to capture debris arrivals with reduced energy consumption.","['aquatic', 'great', 'marine', 'human', 'visionbased', 'offtheshelf', 'robotic', 'debris', 'calm', 'realtime', 'debris', 'coveragebased', 'debris', 'unique', 'aquatic', 'algorithm', 'effective', 'sporadic', 'limited', 'angular', 'able', 'computeintensive', 'extensive', 'experimental', 'various', 'algorithm', 'debris', 'reduced']","['decomposition', 'forests', 'undirected', 'a', 'best', 'better', 'Forests', 'forests', 'approximation', 'complexity', 'face', 'power', 'reducible', 'Multicolorings', 'Triconnected', 'BendOptimal', 'edgedisjoint', 'fourconnected', 'vertexrankings', 'algorithms', 'linear', 'areas', 'decomposition', 'trees', 'Orderly', 'and', 'balanced', 'algorithm', 'algorithms', 'areas', 'decomposition', 'trees', 'distanceedgecolorings', 'edgecoloring', 'areas', 'decomposition', 'trees', 'balanced', 'best', 'optimal', 'decomposition', 'forests', 'undirected', 'algorithm', 'Efficient', 'necessary', 'optimal', 'almost', 'consecutive', 'regular', 'maximum', 'partial', 'sufficient', 'linear', 'orthogonal', 'rectangular', 'symmetric', 'to', 'MultipleValued', 'SiteOriented', 'edgedisjoint', 'vertexrankings', 'complete', 'depth', 'partial', 'algorithm', 'combinatorial', 'protocols', 'Various', 'the', 'algorithm', 'areas', 'decomposition', 'trees', 'Improved', 'Reduction', 'maximal', 'maximum']","['Eigenspace updating for non-stationary process and its application to face recognition', 'Joint Face Alignment and 3D Face Reconstruction with Application to Face Recognition.']","Paraphrase the following paper title without adding any other information: "" Monitoring aquatic debris is of great interest to the ecosystems, marine life, human health, and water transport. This paper presents the design and implementation of SOAR-a vision-based surveillance robot system that integrates an off-the-shelf Android smartphone and a gliding robotic fish for debris monitoring in relatively calm waters. SOAR features real-time debris detection and coverage-based rotation scheduling algorithms. The image processing algorithms for debris detection are specifically designed to address the unique challenges in aquatic environments. The rotation scheduling algorithm provides effective coverage for sporadic debris arrivals despite camera's limited angular view. Moreover, SOAR is able to dynamically offload compute-intensive processing tasks to the cloud for battery power conservation. We have implemented a SOAR prototype and conducted extensive experimental evaluation. The results show that SOAR can accurately detect debris in the presence of various environment and system dynamics, and the rotation scheduling algorithm enables SOAR to capture debris arrivals with reduced energy consumption."" based on the user's previous titles: ""['Eigenspace updating for non-stationary process and its application to face recognition', 'Joint Face Alignment and 3D Face Reconstruction with Application to Face Recognition.']""."
80," Two natural decision problems regarding the XML query language XQuery are well-definedness and semantic type-checking. We study these problems in the setting of a relational fragment of XQuery. We show that well-definedness and semantic type-checking are undecidable, even in the positive-existential case. Nevertheless, for a “pure” variant of XQuery, in which no identification is made between an item and the singleton containing that item, the problems become decidable. We also consider the analogous problems in the setting of the nested relational calculus.","['natural', 'welldefinedness', 'semantic', 'relational', 'semantic', 'undecidable', 'positiveexistential', 'decidable', 'analogous', 'nested', 'relational']","['colorings', 'forests', 'undirected', 'kconnectivity', 'multicolorings', 'vertexrankings', 'Boolean', 'algorithms', 'computability', 'Boolean', 'Cardinality', 'Hierarchical', 'cardinality', 'computability', 'Boolean', 'algorithms', 'computability', 'Boolean', 'combinatorial', 'computability', 'reducible', 'solvable', 'edgedisjoint', 'seriesparallel', 'vertexrankings', 'Boolean', 'cardinality', 'combinatorial', 'computability', 'solvable', 'approximation', 'generalized', 'parallel', 'reducible', 'Hierarchical', 'cardinality', 'subgraphs', 'Boolean', 'Cardinality', 'Hierarchical', 'cardinality', 'computability']","['A Framework for Comparing Query Languages in Their Ability to Express Boolean Queries.', 'A Monotone Preservation Result for Boolean Queries Expressed as a Containment of Conjunctive Queries.']","Paraphrase the following paper title without adding any other information: "" Two natural decision problems regarding the XML query language XQuery are well-definedness and semantic type-checking. We study these problems in the setting of a relational fragment of XQuery. We show that well-definedness and semantic type-checking are undecidable, even in the positive-existential case. Nevertheless, for a “pure” variant of XQuery, in which no identification is made between an item and the singleton containing that item, the problems become decidable. We also consider the analogous problems in the setting of the nested relational calculus."" based on the user's previous titles: ""['A Framework for Comparing Query Languages in Their Ability to Express Boolean Queries.', 'A Monotone Preservation Result for Boolean Queries Expressed as a Containment of Conjunctive Queries.']""."
81," Advances in computer networks are rapidly changing the way engineering is performed, making computer-aided design the front door to a web of interconnected software, information, and human resources. CAD software is the “browser” through which design interfaces with these resources. For engineering design, the network promises to integrate activities across enterprises and throughout product life cycles. This article describes some of the technology trends influencing network-enabled CAD, with a particular focus on how companies are assimilating new Internet and object-oriented concepts. It reflects nearly two years of discussions with many people in both the software and engineering industries, including discussions held at industry-focused panels and workshops sponsored by the National Institute of Standards and Technology","['computeraided', 'front', 'interconnected', 'human', 'networkenabled', 'particular', 'new', 'objectoriented', 'many', 'industryfocused']","['4Connected', '4connected', 'fourconnected', 'seriesparallel', 'edge', 'the', 'connected', 'networks', 'parallel', 'approximation', 'complexity', 'face', 'power', 'reducible', '4Connected', 'FourConnected', 'edgedisjoint', 'vertexrankings', 'the', 'the', 'Boolean', 'algorithms', 'combinatorial', 'Various', 'are', 'the', 'FourConnected', 'SiteOriented', 'degreeconstrained', 'lineartime']","['Mathematical representation of the vascular structure and applications', 'Functional Modeling of Engineering Designs for the Semantic Web']","Paraphrase the following paper title without adding any other information: "" Advances in computer networks are rapidly changing the way engineering is performed, making computer-aided design the front door to a web of interconnected software, information, and human resources. CAD software is the “browser” through which design interfaces with these resources. For engineering design, the network promises to integrate activities across enterprises and throughout product life cycles. This article describes some of the technology trends influencing network-enabled CAD, with a particular focus on how companies are assimilating new Internet and object-oriented concepts. It reflects nearly two years of discussions with many people in both the software and engineering industries, including discussions held at industry-focused panels and workshops sponsored by the National Institute of Standards and Technology"" based on the user's previous titles: ""['Mathematical representation of the vascular structure and applications', 'Functional Modeling of Engineering Designs for the Semantic Web']""."
82," Large-scale donation-based distributed infrastructures need to cope with the inherent unreliability of participant nodes. A widely-used work scheduling technique in such en- vironments is to redundantly schedule the outsourced com- putations to a number of nodes. We present the design and implementation of RIDGE, a reliability-aware system which uses a node's prior performance and behavior to make more effective scheduling decisions. We have imple- mented RIDGE on top of the BOINC distributed comput- ing infrastructure and have evaluated its performance on a live PlanetLab testbed. Our experimental results show that RIDGE is able to match or surpass the throughput of the best BOINC configuration by automatically adapting to the characteristics of the underlying environment. In additio n, RIDGE is able to provide much lower workunit makespans compared to BOINC. RIDGE is also able to produce signif- icantly lower communication makespans for downloading clients. Collectively, the results suggest that RIDGE has great promise for service-oriented environments with time constraints. putation and communication performance of the nodes. These problems arise as nodes have dynamically changing workloads, may leave and join unexpectedly, and may be- have maliciously. When nodes are serving data, their ex- treme time-varying heterogeneity in terms of different ca- pacity, bandwidth, and latency can also compromise perfor- mance. For the first problem, we present a scheme that can dynamically adjust the degree of replication based on the current node behavior. In an earlier paper, we showed how intelligent replication can improve performance through a simulation study (9). This paper focuses on the implemen- tation and deployment of the proposed ideas in a live envi- ronment. For the second problem, we present a scheme that can dynamically select the most appropriate nodes for data download based on current network dynamics. The key contribution is that clients can make independent download decisions from replicated data nodes without direct intera c- tion and minimal state. We have deployed a prototype of RIDGE and evaluated it on a live distributed testbed on PlanetLab (3), using the BLAST (4) bioinformatics application. The results show that RIDGE can automatically match, and in some cases surpass, the best static BOINC performance (which requires knowing the dynamics of the environment) in terms of re- liability, and can achieve far better computation makespan. The results also show that communication makespan can be significantly reduced using our schemes (17-43% improve- ment over existing heuristics). Collectively, these resul ts indicate that RIDGE is well-suited to service-oriented envi- ronments with time constraints.","['Largescale', 'donationbased', 'inherent', 'participant', 'widelyused', 'such', 'en', 'outsourced', 'com', 'prior', 'effective', 'imple', 'comput', 'ing', 'live', 'experimental', 'able', 'best', 'underlying', 'additio', 'able', 'lower', 'able', 'signif', 'lower', 'great', 'serviceoriented', 'ex', 'timevarying', 'different', 'ca', 'perfor', 'first', 'current', 'earlier', 'intelligent', 'implemen', 'live', 'envi', 'second', 'appropriate', 'current', 'key', 'independent', 'direct', 'c', 'minimal', 'live', 'best', 'static', 'better', 'improve', 'wellsuited', 'serviceoriented', 'envi']","['Partitioning', '5coloring', 'Triconnected', 'fivecoloring', 'vertexrankings', 'complexity', 'problem', 'problems', 'tradeoffs', 'designated', 'exchange', 'regular', 'the', '4connected', 'degreeconstrained', 'fourconnected', 'seriesparallel', 'are', 'necessary', 'possible', 'without', 'An', 'in', 'almost', 'demand', 'functions', 'An', 'Links', 'Web', 'unate', 'the', 'without', 'Efficient', 'necessary', 'optimal', 'Efficient', 'Simple', 'unate', 'Computing', 'algorithms', 'computing', 'and', 'unate', 'around', 'family', 'in', 'algorithm', 'combinatorial', 'protocols', 'to', 'Best', 'best', 'better', 'possible', 'shortest', 'complexity', 'problem', 'problems', 'for', 'to', 'Lower', 'than', 'the', 'to', 'Given', 'than', 'unate', 'without', 'Lower', 'than', 'the', 'a', 'best', 'better', '4Connected', '4connected', 'SiteOriented', 'vertexrankings', 'to', 'edgedisjoint', 'edgerankings', 'seriesparallel', 'vertexrankings', 'Various', 'are', 'better', 'than', 'in', 'best', 'computing', 'functions', 'optimal', 'the', 'the', 'than', 'the', 'Efficient', 'balanced', 'undirected', 'Framework', 'embedding', 'functions', 'protocols', 'around', 'family', 'in', 'Framework', 'computing', 'forests', 'unate', 'the', 'necessary', 'optimal', 'possible', 'sufficient', 'the', 'Key', 'key', 'Independent', 'connected', 'sufficient', 'parallel', 'partial', 'possible', 'A', 'maximal', 'maximum', 'sufficient', 'without', 'around', 'family', 'in', 'Best', 'best', 'better', 'possible', 'shortest', 'linear', 'best', 'better', 'optimal', 'than', 'Improved', 'Improvements', 'better', 'optimal', '4Connected', '4connected', 'Triconnected', 'degreeconstrained', '4Connected', '4connected', 'SiteOriented', 'vertexrankings', 'Framework', 'computing', 'forests', 'unate']","['Exploring the throughput-fairness tradeoff of deadline scheduling in heterogeneous computing environments', 'STEAMEngine: Driving MapReduce provisioning in the cloud']","Paraphrase the following paper title without adding any other information: "" Large-scale donation-based distributed infrastructures need to cope with the inherent unreliability of participant nodes. A widely-used work scheduling technique in such en- vironments is to redundantly schedule the outsourced com- putations to a number of nodes. We present the design and implementation of RIDGE, a reliability-aware system which uses a node's prior performance and behavior to make more effective scheduling decisions. We have imple- mented RIDGE on top of the BOINC distributed comput- ing infrastructure and have evaluated its performance on a live PlanetLab testbed. Our experimental results show that RIDGE is able to match or surpass the throughput of the best BOINC configuration by automatically adapting to the characteristics of the underlying environment. In additio n, RIDGE is able to provide much lower workunit makespans compared to BOINC. RIDGE is also able to produce signif- icantly lower communication makespans for downloading clients. Collectively, the results suggest that RIDGE has great promise for service-oriented environments with time constraints. putation and communication performance of the nodes. These problems arise as nodes have dynamically changing workloads, may leave and join unexpectedly, and may be- have maliciously. When nodes are serving data, their ex- treme time-varying heterogeneity in terms of different ca- pacity, bandwidth, and latency can also compromise perfor- mance. For the first problem, we present a scheme that can dynamically adjust the degree of replication based on the current node behavior. In an earlier paper, we showed how intelligent replication can improve performance through a simulation study (9). This paper focuses on the implemen- tation and deployment of the proposed ideas in a live envi- ronment. For the second problem, we present a scheme that can dynamically select the most appropriate nodes for data download based on current network dynamics. The key contribution is that clients can make independent download decisions from replicated data nodes without direct intera c- tion and minimal state. We have deployed a prototype of RIDGE and evaluated it on a live distributed testbed on PlanetLab (3), using the BLAST (4) bioinformatics application. The results show that RIDGE can automatically match, and in some cases surpass, the best static BOINC performance (which requires knowing the dynamics of the environment) in terms of re- liability, and can achieve far better computation makespan. The results also show that communication makespan can be significantly reduced using our schemes (17-43% improve- ment over existing heuristics). Collectively, these resul ts indicate that RIDGE is well-suited to service-oriented envi- ronments with time constraints."" based on the user's previous titles: ""['Exploring the throughput-fairness tradeoff of deadline scheduling in heterogeneous computing environments', 'STEAMEngine: Driving MapReduce provisioning in the cloud']""."
83," In this paper we explore avenues for improving the reliability of dimensionality reduction methods such as Non-Negative Matrix Factorization (NMF) as interpretive exploratory data analysis tools. We flrst explore the di-culties of the op- timization problem underlying NMF, showing for the flrst time that non-trivial NMF solutions always exist and that the optimization problem is actually convex, by using the theory of Completely Positive Factorization. We subse- quently explore four novel approaches to flnding globally- optimal NMF solutions using various ideas from convex op- timization. We then develop a new method, isometric NMF (isoNMF), which preserves non-negativity while also provid- ing an isometric embedding, simultaneously achieving two properties which are helpful for interpretation. Though it results in a more di-cult optimization problem, we show ex- perimentally that the resulting method is scalable and even achieves more compact spectra than standard NMF.","['such', 'NonNegative', 'interpretive', 'op', 'flrst', 'nontrivial', 'convex', 'globally', 'optimal', 'various', 'convex', 'op', 'new', 'isometric', 'provid', 'isometric', 'helpful', 'dicult', 'ex', 'scalable', 'more', 'compact', 'standard']","['are', 'necessary', 'possible', 'without', 'Multicommodity', 'Noncrossing', 'vertexrankings', 'linear', 'Of', 'is', 'unate', 'consecutive', 'the', 'unate', 'combinatorial', 'maximal', 'orthogonal', 'solvable', 'symmetric', 'Convex', 'almost', 'around', 'demand', 'networks', 'Optimal', 'maximal', 'maximum', 'optimal', 'Various', 'the', 'Convex', 'Of', 'is', 'unate', 'the', 'planar', 'and', 'planar', 'Efficient', 'best', 'finding', 'necessary', 'multigraphs', 'unate', 'to', 'Efficient', 'computing', 'optimal', 'For', 'better', 'for', 'than', 'compact', 'rectangular', 'size', 'compact', 'protocols', 'regular', 'set']","['On the Sample Complexity of Predictive Sparse Coding', 'New Algorithms for Efficient High-Dimensional Nonparametric Classification']","Paraphrase the following paper title without adding any other information: "" In this paper we explore avenues for improving the reliability of dimensionality reduction methods such as Non-Negative Matrix Factorization (NMF) as interpretive exploratory data analysis tools. We flrst explore the di-culties of the op- timization problem underlying NMF, showing for the flrst time that non-trivial NMF solutions always exist and that the optimization problem is actually convex, by using the theory of Completely Positive Factorization. We subse- quently explore four novel approaches to flnding globally- optimal NMF solutions using various ideas from convex op- timization. We then develop a new method, isometric NMF (isoNMF), which preserves non-negativity while also provid- ing an isometric embedding, simultaneously achieving two properties which are helpful for interpretation. Though it results in a more di-cult optimization problem, we show ex- perimentally that the resulting method is scalable and even achieves more compact spectra than standard NMF."" based on the user's previous titles: ""['On the Sample Complexity of Predictive Sparse Coding', 'New Algorithms for Efficient High-Dimensional Nonparametric Classification']""."
84," Autonomy and flexibility are two major requirements for modern robots. In particular, humanoid robots should learn new skills incrementally through autonomous exploration, and adapt to different contexts. In this paper we consider the problem of learning forward models for task space control under dynamically varying kinematic contexts: the robot learns incrementally and autonomously its forward kinematics under different contexts, represented by the inclusion of different tools, and exploits the learned model to realize reaching with those tools. We model the forward kinematics as a multi-valued function, in which different outputs for the same input query are related to different tools (i.e. contexts). The model is estimated using IMLE, a recent online learning algorithm for multi-valued regression, and used for control. No information is given about the tool changes, nor any assumption is made about the tool kinematics. Results are provided both in simulation and with a full-body humanoid. In the latter case we show how the robot successfully performs reaching using a flexible tool, a clear example of complex kinematics.","['major', 'modern', 'particular', 'humanoid', 'new', 'autonomous', 'different', 'forward', 'task', 'kinematic', 'different', 'different', 'learned', 'multivalued', 'different', 'same', 'different', 'recent', 'multivalued', 'fullbody', 'latter', 'flexible', 'clear', 'complex']","['key', 'the', 'compact', 'the', 'the', 'planar', 'the', 'Connected', 'computing', 'connected', 'functions', 'Various', 'are', 'better', 'than', 'for', 'into', 'complete', 'complexity', 'finding', 'necessary', 'linear', 'Various', 'are', 'better', 'than', 'Various', 'are', 'better', 'than', 'And', 'using', 'Boolean', 'cardinality', 'linear', 'symmetric', 'Various', 'are', 'better', 'than', 'the', 'Various', 'are', 'better', 'than', 'Given', 'consecutive', 'the', 'Boolean', 'cardinality', 'linear', 'symmetric', 'drawings', 'the', 'balanced', 'compact', 'optimal', 'better', 'necessary', 'possible', 'simply', 'sufficient', 'combinatorial', 'complexity', 'problems', 'solvable']","['Reactive navigation for non-holonomic robots using the ego-kinematic space', 'Avoiding moving obstacles: the forbidden velocity map']","Paraphrase the following paper title without adding any other information: "" Autonomy and flexibility are two major requirements for modern robots. In particular, humanoid robots should learn new skills incrementally through autonomous exploration, and adapt to different contexts. In this paper we consider the problem of learning forward models for task space control under dynamically varying kinematic contexts: the robot learns incrementally and autonomously its forward kinematics under different contexts, represented by the inclusion of different tools, and exploits the learned model to realize reaching with those tools. We model the forward kinematics as a multi-valued function, in which different outputs for the same input query are related to different tools (i.e. contexts). The model is estimated using IMLE, a recent online learning algorithm for multi-valued regression, and used for control. No information is given about the tool changes, nor any assumption is made about the tool kinematics. Results are provided both in simulation and with a full-body humanoid. In the latter case we show how the robot successfully performs reaching using a flexible tool, a clear example of complex kinematics."" based on the user's previous titles: ""['Reactive navigation for non-holonomic robots using the ego-kinematic space', 'Avoiding moving obstacles: the forbidden velocity map']""."
85," Computing the Euler genus of a graph is a fundamental problem in graph theory and topology. It has been shown to be NP-hard by Thomassen [27] and a linear-time fixed-parameter algorithm has been obtained by Mohar [20]. Despite extensive study, the approximability of the Euler genus remains wide open. While the existence of a constant factor approximation is not ruled out, the currently best-known upper bound is a trivial O(n/g)-approximation that follows from bounds on the Euler characteristic. In this paper, we give the first non-trivial approximation algorithm for this problem. Specifically, we present a polynomial-time algorithm which given a graph G of Euler genus g outputs an embedding of G into a surface of Euler genus gO(1). Combined with the above O(n/g)-approximation, our result also implies a O(n1-α)-approximation, for some universal constant α> 0. Our approximation algorithm also has implications for the design of algorithms on graphs of small genus. Several of these algorithms require that an embedding of the graph into a surface of small genus is given as part of the input. Our result implies that many of these algorithms can be implemented even when the embedding of the input graph is unknown.","['fundamental', 'NPhard', 'lineartime', 'extensive', 'wide', 'open', 'constant', 'bestknown', 'upper', 'trivial', 'first', 'nontrivial', 'above', 'n1', 'universal', 'constant', 'small', 'Several', 'small', 'many', 'unknown']","['key', 'necessary', 'problem', 'PQtrees', 'SeriesParallel', 'Lineartime', 'lineartime', 'seriesparallel', 'complete', 'depth', 'partial', 'Spanning', 'depth', 'around', 'exchange', 'into', 'the', 'almost', 'linear', 'maximal', 'regular', 'Various', 'best', 'key', 'shortest', 'Lower', 'the', 'generalized', 'necessary', 'reducible', 'simply', 'solvable', 'the', 'combinatorial', 'maximal', 'orthogonal', 'solvable', 'symmetric', 'the', 'threshold', 'Combinatorial', 'matchings', 'orthogonal', 'treewidth', 'unate', 'almost', 'compact', 'generalized', 'reducible', 'almost', 'linear', 'maximal', 'regular', 'Small', 'a', 'size', 'For', 'Four', 'Various', 'Small', 'a', 'size', 'Various', 'are', 'the', 'Given', 'Various', 'possible', 'secret']","['Acute triangles in 4-connected maximal plane graphs', 'A linear time algorithm for the induced disjoint paths problem in planar graphs']","Paraphrase the following paper title without adding any other information: "" Computing the Euler genus of a graph is a fundamental problem in graph theory and topology. It has been shown to be NP-hard by Thomassen [27] and a linear-time fixed-parameter algorithm has been obtained by Mohar [20]. Despite extensive study, the approximability of the Euler genus remains wide open. While the existence of a constant factor approximation is not ruled out, the currently best-known upper bound is a trivial O(n/g)-approximation that follows from bounds on the Euler characteristic. In this paper, we give the first non-trivial approximation algorithm for this problem. Specifically, we present a polynomial-time algorithm which given a graph G of Euler genus g outputs an embedding of G into a surface of Euler genus gO(1). Combined with the above O(n/g)-approximation, our result also implies a O(n1-α)-approximation, for some universal constant α> 0. Our approximation algorithm also has implications for the design of algorithms on graphs of small genus. Several of these algorithms require that an embedding of the graph into a surface of small genus is given as part of the input. Our result implies that many of these algorithms can be implemented even when the embedding of the input graph is unknown."" based on the user's previous titles: ""['Acute triangles in 4-connected maximal plane graphs', 'A linear time algorithm for the induced disjoint paths problem in planar graphs']""."
86,"  We investigate the potential of the analysis of noisy non-stationary time series by quantizing it into streams of discrete symbols and applying finite-memory symbolic predictors. Careful quantization can reduce the noise in the time series to make model estimation more amenable. We apply the quantization strategy in a realistic setting involving financial forecasting and trading. In particular, using historical data, we simulate the trading of straddles on the financial indexes DAX and FTSE 100 ... ","['noisy', 'nonstationary', 'discrete', 'finitememory', 'symbolic', 'Careful', 'amenable', 'realistic', 'financial', 'particular', 'historical', 'financial']","['Efficient', 'compact', 'corners', 'linear', 'linear', 'Approximability', 'edgedisjoint', 'nonhamiltonian', 'vertexrankings', 'functions', 'key', 'power', 'reducible', 'Necessary', 'Nicely', 'With', 'Without', 'better', 'necessary', 'reducible', 'simply', 'sufficient', 'approximation', 'balanced', 'better', 'possible', 'computing', 'exchange', 'family', 'problems', 'the', 'characterization', 'depth', 'graphs', 'computing', 'exchange', 'family', 'problems']","['Incremental probabilistic classification vector machine with linear costs', 'Short term memory in input-driven linear dynamical systems']","Paraphrase the following paper title without adding any other information: ""  We investigate the potential of the analysis of noisy non-stationary time series by quantizing it into streams of discrete symbols and applying finite-memory symbolic predictors. Careful quantization can reduce the noise in the time series to make model estimation more amenable. We apply the quantization strategy in a realistic setting involving financial forecasting and trading. In particular, using historical data, we simulate the trading of straddles on the financial indexes DAX and FTSE 100 ... "" based on the user's previous titles: ""['Incremental probabilistic classification vector machine with linear costs', 'Short term memory in input-driven linear dynamical systems']""."
87," This paper presents a study, based on conic correspondences, on the relationship between two perspective images acquired by an uncalibrated camera. We show that for a pair of corresponding conics, the parameters representing the conics satisfy a linear constraint. To be more specific, the parameters that represent a conic in one image are transformed by a five-dimensional projective transformation to the parameters that represent the corresponding conic in another image. We also show that this transformation is expressed as the symmetric component of the tensor product of the transformation based on point/line correspondences and itself. In addition, we present a linear algorithm for uniquely determining the corresponding point-based transformation from a given conic-based transformation up to a scale factor. Accordingly, conic correspondences enable us to easily handle both points and lines in uncalibrated images of a planar object.","['conic', 'perspective', 'uncalibrated', 'linear', 'specific', 'fivedimensional', 'projective', 'symmetric', 'tensor', 'linear', 'corresponding', 'pointbased', 'conicbased', 'scale', 'conic', 'uncalibrated']","['planar', 'balanced', 'characterization', 'depth', 'approximation', 'linear', 'linear', 'key', 'optimal', 'fourconnected', 'lineartime', 'vertexrankings', 'planar', 'orthogonal', 'planar', 'symmetric', 'Hamiltonian', 'decomposition', 'orthogonal', 'symmetric', 'linear', 'designated', 'the', 'FourConnected', 'Multicommodity', 'vertexrankings', '4Connected', '4connected', 'MultipleValued', 'fourconnected', 'seriesparallel', 'complexity', 'graph', 'size', 'planar', 'approximation', 'linear']","['Digital planar surface segmentation using local geometric patterns', 'An approximation algorithm for the two-layered graph drawing problem']","Paraphrase the following paper title without adding any other information: "" This paper presents a study, based on conic correspondences, on the relationship between two perspective images acquired by an uncalibrated camera. We show that for a pair of corresponding conics, the parameters representing the conics satisfy a linear constraint. To be more specific, the parameters that represent a conic in one image are transformed by a five-dimensional projective transformation to the parameters that represent the corresponding conic in another image. We also show that this transformation is expressed as the symmetric component of the tensor product of the transformation based on point/line correspondences and itself. In addition, we present a linear algorithm for uniquely determining the corresponding point-based transformation from a given conic-based transformation up to a scale factor. Accordingly, conic correspondences enable us to easily handle both points and lines in uncalibrated images of a planar object."" based on the user's previous titles: ""['Digital planar surface segmentation using local geometric patterns', 'An approximation algorithm for the two-layered graph drawing problem']""."
88," Ethnographic studies of the home revealed the fundamental roles that physical locations and context play in how household members understand and manage conventional information. Yet we also know that digital information is becoming increasingly important to households. The problem is that this digital information is almost always tied to traditional computer displays, which inhibits its incorporation into household routines. Our solution, location-dependent information appliances, exploit both home location and context (as articulated in ethnographic studies) to enhance the role of ambient displays in the home setting; these displays provide home occupants with both background awareness of an information source and foreground methods to gain further details if desired. The novel aspect is that home occupants assign particular information to locations within a home in a way that makes sense to them. As a device is moved to a particular home location, information is automatically mapped to that device along with hints on how it should be displayed.","['Ethnographic', 'fundamental', 'physical', 'context', 'household', 'conventional', 'digital', 'important', 'digital', 'traditional', 'locationdependent', 'ethnographic', 'ambient', 'novel', 'assign', 'particular', 'particular']","['Abstract', 'Drawing', 'Mining', 'key', 'necessary', 'problem', 'complexity', 'computing', 'necessary', 'reducible', 'regular', 'characterization', 'complexity', 'the', 'family', 'size', 'supply', 'linear', 'Digital', 'computing', 'linear', 'best', 'key', 'necessary', 'Digital', 'computing', 'linear', 'the', 'using', '4connected', 'FourConnected', 'edgedisjoint', 'seriesparallel', 'Drawing', 'characterization', 'generalized', 'optimal', 'planar', 'Characterization', 'algorithm', 'characterization', 'combinatorial', 'designated', 'to', 'the', 'the']","['StickySpots: using location to embed technology in the social practices of the home', '""LINC-ing"" the family: the participatory design of an inkable family calendar']","Paraphrase the following paper title without adding any other information: "" Ethnographic studies of the home revealed the fundamental roles that physical locations and context play in how household members understand and manage conventional information. Yet we also know that digital information is becoming increasingly important to households. The problem is that this digital information is almost always tied to traditional computer displays, which inhibits its incorporation into household routines. Our solution, location-dependent information appliances, exploit both home location and context (as articulated in ethnographic studies) to enhance the role of ambient displays in the home setting; these displays provide home occupants with both background awareness of an information source and foreground methods to gain further details if desired. The novel aspect is that home occupants assign particular information to locations within a home in a way that makes sense to them. As a device is moved to a particular home location, information is automatically mapped to that device along with hints on how it should be displayed."" based on the user's previous titles: ""['StickySpots: using location to embed technology in the social practices of the home', '""LINC-ing"" the family: the participatory design of an inkable family calendar']""."
89, Evolutionary multiobjective optimization (EMO) is an active research area in the field of evolutionary computation. EMO algorithms are designed to find a non-dominated solution set that approximates the entire Pareto front of a multiobjective optimization ...,"['Evolutionary', 'multiobjective', 'active', 'evolutionary', 'nondominated', 'entire', 'multiobjective']","['Algorithms', 'Hierarchical', 'Optimal', 'algorithm', 'connected', 'designated', 'optimal', 'regular', 'algorithm', 'complexity', 'undirected', 'algorithm', 'combinatorial', 'orthogonal', 'symmetric', 'the', 'algorithm']","['Maximizing the performance of advertisements diffusion: A simulation study of the dynamics of viral advertising in social networks', 'Generalized Hierarchical Sparse Model for Arbitrary-Order Interactive Antigenic Sites Identification in Flu Virus Data.']","Paraphrase the following paper title without adding any other information: "" Evolutionary multiobjective optimization (EMO) is an active research area in the field of evolutionary computation. EMO algorithms are designed to find a non-dominated solution set that approximates the entire Pareto front of a multiobjective optimization ..."" based on the user's previous titles: ""['Maximizing the performance of advertisements diffusion: A simulation study of the dynamics of viral advertising in social networks', 'Generalized Hierarchical Sparse Model for Arbitrary-Order Interactive Antigenic Sites Identification in Flu Virus Data.']""."
90," We propose a novel probabilistic model for collaborative filtering (CF), called SRMCoFi, which seamlessly integrates both linear and bilinear random effects into a principled framework. The formulation of SRMCoFi is supported by both social psychological experiments and statistical theories. Not only can many existing CF methods be seen as special cases of SRMCoFi, but it also integrates their advantages while simultaneously overcoming their disadvantages. The solid theoretical foundation of SRMCoFi is further supported by promising empirical results obtained in extensive experiments using real CF data sets on movie ratings. Copyright © 2011, Association for the Advancement of Artificial Intelligence. All rights reserved.","['novel', 'probabilistic', 'collaborative', 'linear', 'bilinear', 'principled', 'social', 'psychological', 'statistical', 'many', 'special', 'solid', 'theoretical', 'empirical', 'extensive', 'real']","['Characterization', 'algorithm', 'characterization', 'combinatorial', 'algorithm', 'algorithms', 'combinatorial', 'Sharing', 'balanced', 'sharing', 'linear', 'linear', 'orthogonal', 'symmetric', 'balanced', 'best', 'characterization', 'tradeoffs', 'computing', 'embedding', 'family', 'networks', 'sharing', 'complexity', 'depth', 'problems', 'graphs', 'Various', 'are', 'the', 'for', 'regular', 'secret', 'balanced', 'compact', 'rectangular', 'sufficient', 'approximation', 'combinatorial', 'computability', 'generalized', 'maximal', 'approximation', 'characterization', 'combinatorial', 'generalized', 'graphs', 'complete', 'depth', 'partial', 'best', 'better', 'possible', 'problem']","['Robust locally linear embedding', 'Semisupervised generalized discriminant analysis.']","Paraphrase the following paper title without adding any other information: "" We propose a novel probabilistic model for collaborative filtering (CF), called SRMCoFi, which seamlessly integrates both linear and bilinear random effects into a principled framework. The formulation of SRMCoFi is supported by both social psychological experiments and statistical theories. Not only can many existing CF methods be seen as special cases of SRMCoFi, but it also integrates their advantages while simultaneously overcoming their disadvantages. The solid theoretical foundation of SRMCoFi is further supported by promising empirical results obtained in extensive experiments using real CF data sets on movie ratings. Copyright © 2011, Association for the Advancement of Artificial Intelligence. All rights reserved."" based on the user's previous titles: ""['Robust locally linear embedding', 'Semisupervised generalized discriminant analysis.']""."
91," Model-checkers have recently been suggested for automated software test-case generation. Several works have presented methods that create efficient test-suites using model-checkers. Ease of use and complete automation are major advantages of such approaches. However, the use of a model-checker comes at the price of potential performance problems. If the model used for test-case generation is complex, then model-checker based approaches can be very slow, or even not applicable at all. In this paper, we identify that unnecessary, redundant calls to the model-checker are one of the causes of bad performance. To overcome this problem, we suggest the use of temporal logic rewriting techniques, which originate from runtime verification research. This achieves a significant increase in the performance, and improves the applicability of model-checker based test-case generation approaches in general. At the same time, the suggested techniques achieve a reduction of the resulting test-suite sizes without degradation of the fault sensitivity. This helps to reduce the costs of the test-case execution.","['automated', 'testcase', 'Several', 'efficient', 'complete', 'major', 'such', 'potential', 'testcase', 'complex', 'slow', 'applicable', 'unnecessary', 'redundant', 'bad', 'temporal', 'logic', 'runtime', 'significant', 'testcase', 'general', 'same', 'testsuite', 'testcase']","['algorithms', 'using', 'graph', 'graphs', 'For', 'Four', 'Various', 'Efficient', 'optimal', 'Partial', 'complete', 'partial', 'total', 'key', 'the', 'are', 'necessary', 'possible', 'without', 'maximum', 'possible', 'problems', 'graph', 'graphs', 'combinatorial', 'complexity', 'problems', 'solvable', 'linear', 'approximation', 'are', 'designated', 'necessary', 'prescribed', 'Minimizing', 'necessary', 'simply', 'without', 'almost', 'functions', 'necessary', 'simply', 'best', 'better', 'problem', 'problems', 'linear', 'Boolean', 'Logic', 'circuits', 'combinatorial', 'Queue', 'algorithms', 'complexity', 'functions', 'key', 'necessary', 'sufficient', 'graph', 'graphs', 'generalized', 'the', 'the', 'computability', 'multigraphs', 'treewidth', 'graph', 'graphs']","[""On the complexity of program debugging using constraints for modeling the program's syntax and semantics"", 'Attack pattern-based combinatorial testing']","Paraphrase the following paper title without adding any other information: "" Model-checkers have recently been suggested for automated software test-case generation. Several works have presented methods that create efficient test-suites using model-checkers. Ease of use and complete automation are major advantages of such approaches. However, the use of a model-checker comes at the price of potential performance problems. If the model used for test-case generation is complex, then model-checker based approaches can be very slow, or even not applicable at all. In this paper, we identify that unnecessary, redundant calls to the model-checker are one of the causes of bad performance. To overcome this problem, we suggest the use of temporal logic rewriting techniques, which originate from runtime verification research. This achieves a significant increase in the performance, and improves the applicability of model-checker based test-case generation approaches in general. At the same time, the suggested techniques achieve a reduction of the resulting test-suite sizes without degradation of the fault sensitivity. This helps to reduce the costs of the test-case execution."" based on the user's previous titles: ""[""On the complexity of program debugging using constraints for modeling the program's syntax and semantics"", 'Attack pattern-based combinatorial testing']""."
92," Several recently-proposed architectures for high-performance object recognition are composed of two main stages: a feature extraction stage that extracts locally-invariant feature vectors from regularly spaced image patches, and a somewhat generic supervised classifier. The first stage is often composed of three main modules: (1) a bank of filters (often oriented edge detectors); (2) a non-linear transform, such as a point-wise squashing functions, quantization, or normalization; (3) a spatial pooling operation which combines the outputs of similar filters over neighboring regions. We propose a method that automatically learns such feature extractors in an unsupervised fashion by simultaneously learning the filters and the pooling units that combine multiple filter outputs together. The method automatically generates topographic maps of similar filters that extract features of orientations, scales, and positions. These similar filters are pooled together, producing locally-invariant outputs. The learned feature descriptors give comparable results as SIFT on image recognition tasks for which SIFT is well suited, and better results than SIFT on tasks for which SIFT is less well suited.","['Several', 'recentlyproposed', 'object', 'main', 'locallyinvariant', 'generic', 'first', 'main', 'nonlinear', 'such', 'pointwise', 'spatial', 'similar', 'neighboring', 'such', 'unsupervised', 'multiple', 'topographic', 'similar', 'similar', 'locallyinvariant', 'learned', 'comparable', 'better']","['For', 'Four', 'Various', '4Connected', '4connected', 'Lineartime', 'SiteOriented', 'lineartime', 'plane', 'rectangle', 'key', 'the', 'fourconnected', 'seriesparallel', 'generalized', 'linear', 'prescribed', 'simply', 'the', 'key', 'the', 'linear', 'are', 'necessary', 'possible', 'without', 'Eulerian', 'cardinality', 'multigraphs', 'orthogonal', 'symmetric', 'linear', 'parallel', 'using', 'with', 'areas', 'the', 'are', 'necessary', 'possible', 'without', 'algorithms', 'subgraphs', 'undirected', 'Various', 'consecutive', 'parallel', 'contours', 'planar', 'parallel', 'using', 'with', 'parallel', 'using', 'with', 'fourconnected', 'seriesparallel', 'And', 'using', 'approximation', 'better', 'optimal', 'than', 'best', 'better', 'optimal', 'than']","['Fast Image Deconvolution using Hyper-Laplacian Priors.', 'Blind deconvolution using a normalized sparsity measure']","Paraphrase the following paper title without adding any other information: "" Several recently-proposed architectures for high-performance object recognition are composed of two main stages: a feature extraction stage that extracts locally-invariant feature vectors from regularly spaced image patches, and a somewhat generic supervised classifier. The first stage is often composed of three main modules: (1) a bank of filters (often oriented edge detectors); (2) a non-linear transform, such as a point-wise squashing functions, quantization, or normalization; (3) a spatial pooling operation which combines the outputs of similar filters over neighboring regions. We propose a method that automatically learns such feature extractors in an unsupervised fashion by simultaneously learning the filters and the pooling units that combine multiple filter outputs together. The method automatically generates topographic maps of similar filters that extract features of orientations, scales, and positions. These similar filters are pooled together, producing locally-invariant outputs. The learned feature descriptors give comparable results as SIFT on image recognition tasks for which SIFT is well suited, and better results than SIFT on tasks for which SIFT is less well suited."" based on the user's previous titles: ""['Fast Image Deconvolution using Hyper-Laplacian Priors.', 'Blind deconvolution using a normalized sparsity measure']""."
93," The emergence of the World Wide Web and its influence on the fields of industry, commerce, healthcare and so on, has led to an innovative, dynamic, open, collaborative and interactive environment – the digital ecosystem. Whereas service plays an important role in digital ecosystems, there is no technology available to retrieve heterogeneous and geographically dispersed services. Additionally, no methodology has been proposed in the literature that can distinguish or rank the services based on the Quality of Services (QoS). In order to address these issues, we propose a semantic service retrieval engine which incorporates a novel semantic QoS evaluation methodology. The salient feature of this methodology that sets it apart from any other QoS evaluation methodologies is that the evaluation of the service and the subsequent quantification of QoS is a combination of subjective and objective QoS measures. Another salient feature of our proposed methodology is that it enables the domain-specific ranking of services. Finally, an online prototype is implemented to evaluate our methodology, the results of which are discussed in this paper.","['innovative', 'dynamic', 'open', 'collaborative', 'interactive', 'digital', 'important', 'digital', 'available', 'heterogeneous', 'semantic', 'novel', 'semantic', 'salient', 'other', 'subsequent', 'subjective', 'objective', 'salient', 'domainspecific']","['Efficient', 'best', 'compact', 'balanced', 'linear', 'around', 'exchange', 'into', 'the', 'Sharing', 'balanced', 'sharing', 'linear', 'Digital', 'computing', 'linear', 'best', 'key', 'necessary', 'Digital', 'computing', 'linear', 'are', 'for', 'necessary', 'possible', 'Hierarchical', 'combinatorial', 'generalized', 'partitioning', 'symmetric', 'Boolean', 'algorithms', 'computability', 'Characterization', 'algorithm', 'characterization', 'combinatorial', 'Boolean', 'algorithms', 'computability', 'key', 'the', 'tradeoffs', 'the', 'consecutive', 'the', 'generalized', 'reducible', 'simply', 'tradeoffs', 'weighted', 'necessary', 'optimal', 'possible', 'sufficient', 'key', 'the', 'tradeoffs', 'VertexRankings', 'edgedisjoint', 'vertexrankings']","['A service computing manifesto: the next 10 years.', 'A Human-Centered Semantic Service Platform for the Digital Ecosystems Environment']","Paraphrase the following paper title without adding any other information: "" The emergence of the World Wide Web and its influence on the fields of industry, commerce, healthcare and so on, has led to an innovative, dynamic, open, collaborative and interactive environment – the digital ecosystem. Whereas service plays an important role in digital ecosystems, there is no technology available to retrieve heterogeneous and geographically dispersed services. Additionally, no methodology has been proposed in the literature that can distinguish or rank the services based on the Quality of Services (QoS). In order to address these issues, we propose a semantic service retrieval engine which incorporates a novel semantic QoS evaluation methodology. The salient feature of this methodology that sets it apart from any other QoS evaluation methodologies is that the evaluation of the service and the subsequent quantification of QoS is a combination of subjective and objective QoS measures. Another salient feature of our proposed methodology is that it enables the domain-specific ranking of services. Finally, an online prototype is implemented to evaluate our methodology, the results of which are discussed in this paper."" based on the user's previous titles: ""['A service computing manifesto: the next 10 years.', 'A Human-Centered Semantic Service Platform for the Digital Ecosystems Environment']""."
94," The growing number of cloud services has made service selection a challenging decision-making problem by offering wide ranging choices for cloud service consumers. This necessitates the use of formal decision making methodologies to assist a decision maker in selecting the service that best fulfills the user's requirements. In this paper, we present a cloud service selection methodology that utilizes quality of service history of cloud services over different time periods and performs parallel multi-criteria decision analysis to rank all cloud services in each time period in accordance with user preferences before aggregating the results to determine the overall rank of all the available options for cloud service selection. This methodology assists the cloud service user to select the best possible available service according to the requirements. The multi-criteria decision making processes used for each time period are independent of the other time periods and are executed in parallel.","['wide', 'cloud', 'formal', 'best', 'different', 'multicriteria', 'user', 'overall', 'available', 'cloud', 'best', 'possible', 'available', 'multicriteria', 'independent', 'other']","['Spanning', 'depth', 'Computing', 'computing', 'forests', 'regular', 'uniform', 'without', 'Best', 'best', 'better', 'possible', 'shortest', 'Various', 'are', 'better', 'than', 'algorithm', 'Web', 'algorithm', 'computing', 'complexity', 'optimal', 'the', 'total', 'are', 'for', 'necessary', 'possible', 'Computing', 'computing', 'forests', 'Best', 'best', 'better', 'possible', 'shortest', 'best', 'necessary', 'optimal', 'possible', 'are', 'for', 'necessary', 'possible', 'algorithm', 'Independent', 'connected', 'sufficient', 'the']","['Support vector regression with chaos-based firefly algorithm for stock market price forecasting', 'A framework for SLA management in cloud computing for informed decision making']","Paraphrase the following paper title without adding any other information: "" The growing number of cloud services has made service selection a challenging decision-making problem by offering wide ranging choices for cloud service consumers. This necessitates the use of formal decision making methodologies to assist a decision maker in selecting the service that best fulfills the user's requirements. In this paper, we present a cloud service selection methodology that utilizes quality of service history of cloud services over different time periods and performs parallel multi-criteria decision analysis to rank all cloud services in each time period in accordance with user preferences before aggregating the results to determine the overall rank of all the available options for cloud service selection. This methodology assists the cloud service user to select the best possible available service according to the requirements. The multi-criteria decision making processes used for each time period are independent of the other time periods and are executed in parallel."" based on the user's previous titles: ""['Support vector regression with chaos-based firefly algorithm for stock market price forecasting', 'A framework for SLA management in cloud computing for informed decision making']""."
95," This study describes how a wiki platform worked as a resource in a university course on applied ethnographic research method. The platform was primarily used for uploading field notes from students' ethnographic work. We describe the use of the wiki in terms of how it supported orientations among students towards relevant competencies involved in fieldwork, and how teachers used it as a way of gaining access to students' work. We discuss these functionalities in relation to ethnomethodological work on learning-and-instruction, showing how wiki entries were used as references in students' and teachers' talk. Distributed activities were thereby made available for instructive practices, and the competencies involved in note taking and observation could be collaboratively oriented to. We thus show that although the wiki was a web based distributed tool, its primary pedagogical functionality lay in its being used as a resource in co-located face-to-face talk.","['wiki', 'applied', 'ethnographic', 'uploading', 'ethnographic', 'relevant', 'ethnomethodological', 'wiki', 'available', 'instructive', 'distributed', 'primary', 'pedagogical', 'colocated', 'facetoface']","['Computing', 'File', 'Index', 'Links', 'Web', 'designated', 'prescribed', 'using', 'weighted', 'Drawing', 'characterization', 'generalized', 'Sharing', 'embedding', 'sharing', 'using', 'Drawing', 'characterization', 'generalized', 'key', 'necessary', 'sufficient', 'characterization', 'combinatorial', 'computability', 'generalized', 'orthogonal', 'Computing', 'File', 'Index', 'Links', 'Web', 'are', 'for', 'necessary', 'possible', 'graphs', 'necessary', 'possible', 'Distribution', 'designated', 'networks', 'weighted', 'key', 'the', 'combinatorial', 'complexity', 'computability', 'computing', 'embedding', 'connected', 'networks', 'parallel', 'with']","[""Exploring Users' Experiences of the Web"", 'Gaming on the edge: using seams in ubicomp games']","Paraphrase the following paper title without adding any other information: "" This study describes how a wiki platform worked as a resource in a university course on applied ethnographic research method. The platform was primarily used for uploading field notes from students' ethnographic work. We describe the use of the wiki in terms of how it supported orientations among students towards relevant competencies involved in fieldwork, and how teachers used it as a way of gaining access to students' work. We discuss these functionalities in relation to ethnomethodological work on learning-and-instruction, showing how wiki entries were used as references in students' and teachers' talk. Distributed activities were thereby made available for instructive practices, and the competencies involved in note taking and observation could be collaboratively oriented to. We thus show that although the wiki was a web based distributed tool, its primary pedagogical functionality lay in its being used as a resource in co-located face-to-face talk."" based on the user's previous titles: ""[""Exploring Users' Experiences of the Web"", 'Gaming on the edge: using seams in ubicomp games']""."
96, Two new approaches to designing positive linear observers for positive linear systems are proposed. The first one employs a coordinates transformation and the second relies on the theory of positive realization. These approaches allow one to enlarge the class of positive systems that admit positive linear observers. New results on compartmental systems are also presented.,"['new', 'positive', 'linear', 'positive', 'linear', 'first', 'second', 'positive', 'positive', 'positive', 'linear', 'compartmental']","['the', 'balanced', 'better', 'key', 'unate', 'linear', 'balanced', 'better', 'key', 'unate', 'linear', 'the', 'the', 'balanced', 'better', 'key', 'unate', 'balanced', 'better', 'key', 'unate', 'balanced', 'better', 'key', 'unate', 'linear', 'linear', 'partitioning']","['Bounds related to the Riccati difference equation for linear time varying systems', 'Output-feedback shared-control for fully actuated linear mechanical systems']","Paraphrase the following paper title without adding any other information: "" Two new approaches to designing positive linear observers for positive linear systems are proposed. The first one employs a coordinates transformation and the second relies on the theory of positive realization. These approaches allow one to enlarge the class of positive systems that admit positive linear observers. New results on compartmental systems are also presented."" based on the user's previous titles: ""['Bounds related to the Riccati difference equation for linear time varying systems', 'Output-feedback shared-control for fully actuated linear mechanical systems']""."
97," We address the problem of instance classification: our goal is to annotate images with tags corresponding to objects classes which exhibit small intra-class variations such as logos, products or landmarks. We propose a novel algorithm for the selection of class-specific prototypes which are used in a voting-based classification scheme. We show significant improvements over two state-of-the-art methods, namely the Fisher vector and Hamming Embedding, on two challenging methods of logos and vehicles.","['small', 'intraclass', 'such', 'novel', 'classspecific', 'votingbased', 'significant', 'stateoftheart']","['Small', 'a', 'size', 'Generalized', 'are', 'necessary', 'possible', 'without', 'Characterization', 'algorithm', 'characterization', 'combinatorial', 'edgedisjoint', 'lineartime', 'vertexrankings', '4connected', '5coloring', 'Noncrossing', 'fColoring', 'key', 'necessary', 'sufficient', 'Improvements', 'Sufficient', 'algorithms', 'computing']","['Generalized Max Pooling', 'Textual Similarity with a Bag-of-Embedded-Words Model']","Paraphrase the following paper title without adding any other information: "" We address the problem of instance classification: our goal is to annotate images with tags corresponding to objects classes which exhibit small intra-class variations such as logos, products or landmarks. We propose a novel algorithm for the selection of class-specific prototypes which are used in a voting-based classification scheme. We show significant improvements over two state-of-the-art methods, namely the Fisher vector and Hamming Embedding, on two challenging methods of logos and vehicles."" based on the user's previous titles: ""['Generalized Max Pooling', 'Textual Similarity with a Bag-of-Embedded-Words Model']""."
98," The fast global k-means (FGKM) clustering algorithm is one of the most effective approaches for resolving the local convergence of the k-means clustering algorithm. Numerical experiments show that it can effectively determine a global or near global minimizer of the cost function. However, the FGKM algorithm needs a large amount of computational time or storage space when handling large data sets. To overcome this deficiency, a more efficient FGKM algorithm, namely FGKM+A, is developed in this paper. In the development, we first apply local geometrical information to describe approximately the set of objects represented by a candidate cluster center. On the basis of the approximate description, we then propose an acceleration mechanism for the production of new cluster centers. As a result of the acceleration, the FGKM+A algorithm not only yields the same clustering results as that of the FGKM algorithm but also requires less computational time and fewer distance calculations than the FGKM algorithm and its existing modifications. The efficiency of the FGKM+A algorithm is further confirmed by experimental studies on several UCI data sets.","['fast', 'global', 'effective', 'local', 'Numerical', 'global', 'near', 'global', 'large', 'computational', 'large', 'efficient', 'local', 'geometrical', 'approximate', 'new', 'same', 'computational', 'fewer', 'existing', 'experimental', 'several']","['Efficient', 'Simple', 'compact', 'possible', 'complexity', 'computing', 'demand', 'networks', 'Efficient', 'necessary', 'optimal', 'the', 'Algorithms', 'Parametric', 'complexity', 'computing', 'demand', 'networks', 'around', 'in', 'complexity', 'computing', 'demand', 'networks', 'Small', 'a', 'size', 'algorithms', 'computability', 'computing', 'Small', 'a', 'size', 'Efficient', 'optimal', 'the', 'planar', 'approximation', 'maximum', 'optimal', 'the', 'the', 'algorithms', 'computability', 'computing', 'better', 'consecutive', 'numbers', 'than', 'the', 'using', 'algorithm', 'combinatorial', 'protocols', 'Various', 'consecutive', 'the']","['A FPTAS for computing a symmetric Leontief competitive economy equilibrium', ""The complexity of determining the uniqueness of Tarski's fixed point under the lexicographic ordering""]","Paraphrase the following paper title without adding any other information: "" The fast global k-means (FGKM) clustering algorithm is one of the most effective approaches for resolving the local convergence of the k-means clustering algorithm. Numerical experiments show that it can effectively determine a global or near global minimizer of the cost function. However, the FGKM algorithm needs a large amount of computational time or storage space when handling large data sets. To overcome this deficiency, a more efficient FGKM algorithm, namely FGKM+A, is developed in this paper. In the development, we first apply local geometrical information to describe approximately the set of objects represented by a candidate cluster center. On the basis of the approximate description, we then propose an acceleration mechanism for the production of new cluster centers. As a result of the acceleration, the FGKM+A algorithm not only yields the same clustering results as that of the FGKM algorithm but also requires less computational time and fewer distance calculations than the FGKM algorithm and its existing modifications. The efficiency of the FGKM+A algorithm is further confirmed by experimental studies on several UCI data sets."" based on the user's previous titles: ""['A FPTAS for computing a symmetric Leontief competitive economy equilibrium', ""The complexity of determining the uniqueness of Tarski's fixed point under the lexicographic ordering""]""."
99," Despite an increase in the number of knowledge bases published according to Semantic Web W3C standards, many of those consist primarily of instance data and lack sophisticated schemata, although the availability of such schemata would allow more powerful querying, consistency checking and debugging as well as improved inference. One of the reasons why schemata are still rare is the effort required to create them. Consequently, numerous ontology learning approaches have been developed to simplify the creation of schemata. Those approaches usually either learn structures from text or existing RDF data. In this submission, we present the first approach combining both sources of evidence, in particular we combine an existing logical learning approach with statistical relevance measures applied on textual resources. We perform an experiment involving a manual evaluation on 100 classes of the DBpedia 3.9 dataset and show that the inclusion of relevance measures leads to a significant improvement of the accuracy over the baseline algorithm.","['many', 'consist', 'sophisticated', 'such', 'powerful', 'rare', 'numerous', 'learn', 'first', 'particular', 'logical', 'statistical', 'textual', 'manual', 'significant']","['Various', 'are', 'the', 'are', 'complete', 'of', 'Simple', 'algorithms', 'compact', 'complexity', 'are', 'necessary', 'possible', 'without', 'balanced', 'best', 'compact', 'power', 'almost', 'are', 'best', 'possible', 'Various', 'with', 'to', 'the', 'the', 'Logic', 'necessary', 'parallel', 'possible', 'graphs', 'combinatorial', 'drawings', 'complete', 'functions', 'protocols', 'simply', 'key', 'necessary', 'sufficient']","['Managing the life-cycle of linked data with the LOD2 stack', 'Ideal Downward Refinement in the EL\\mathcal{EL} Description Logic']","Paraphrase the following paper title without adding any other information: "" Despite an increase in the number of knowledge bases published according to Semantic Web W3C standards, many of those consist primarily of instance data and lack sophisticated schemata, although the availability of such schemata would allow more powerful querying, consistency checking and debugging as well as improved inference. One of the reasons why schemata are still rare is the effort required to create them. Consequently, numerous ontology learning approaches have been developed to simplify the creation of schemata. Those approaches usually either learn structures from text or existing RDF data. In this submission, we present the first approach combining both sources of evidence, in particular we combine an existing logical learning approach with statistical relevance measures applied on textual resources. We perform an experiment involving a manual evaluation on 100 classes of the DBpedia 3.9 dataset and show that the inclusion of relevance measures leads to a significant improvement of the accuracy over the baseline algorithm."" based on the user's previous titles: ""['Managing the life-cycle of linked data with the LOD2 stack', 'Ideal Downward Refinement in the EL\\mathcal{EL} Description Logic']""."
