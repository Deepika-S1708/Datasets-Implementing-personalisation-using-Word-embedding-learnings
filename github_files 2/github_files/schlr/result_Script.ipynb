{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d968edf-743d-4de1-ac77-5638a8b46ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-12 16:56:13--  https://ciir.cs.umass.edu/downloads/LaMP/LaMP_5/train/train_outputs.json\n",
      "Resolving ciir.cs.umass.edu (ciir.cs.umass.edu)... 128.119.246.154\n",
      "Connecting to ciir.cs.umass.edu (ciir.cs.umass.edu)|128.119.246.154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1030905 (1007K) [application/json]\n",
      "Saving to: ‘train_outputs.json’\n",
      "\n",
      "train_outputs.json  100%[===================>]   1007K  4.05MB/s    in 0.2s    \n",
      "\n",
      "2023-12-12 16:56:14 (4.05 MB/s) - ‘train_outputs.json’ saved [1030905/1030905]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://ciir.cs.umass.edu/downloads/LaMP/LaMP_5/train/train_outputs.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b832b0d-c790-4fd9-955d-c4b0cdfba49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting absl-py (from rouge-score)\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: nltk in /Users/deeparm/anaconda3/envs/ir_project/lib/python3.8/site-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /Users/deeparm/anaconda3/envs/ir_project/lib/python3.8/site-packages (from rouge-score) (1.24.3)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/deeparm/anaconda3/envs/ir_project/lib/python3.8/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/deeparm/anaconda3/envs/ir_project/lib/python3.8/site-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/deeparm/anaconda3/envs/ir_project/lib/python3.8/site-packages (from nltk->rouge-score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/deeparm/anaconda3/envs/ir_project/lib/python3.8/site-packages (from nltk->rouge-score) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Users/deeparm/anaconda3/envs/ir_project/lib/python3.8/site-packages (from nltk->rouge-score) (4.66.1)\n",
      "Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24932 sha256=33740223b78e6f914f4131e13c3b86b4636fc5614471102a42b64f2afb63ab2e\n",
      "  Stored in directory: /Users/deeparm/Library/Caches/pip/wheels/24/55/6f/ebfc4cb176d1c9665da4e306e1705496206d08215c1acd9dde\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: absl-py, rouge-score\n",
      "Successfully installed absl-py-2.0.0 rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e04321e7-6cc7-469c-8243-225e6417e094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schlr set\n",
      "ROUGE-1: Precision = 0.43041117822959174, Recall = 0.41814368672263413, F1 = 0.4006117967053601\n",
      "ROUGE-L: Precision = 0.3847882466376199, Recall = 0.3711822139848456, F1 = 0.3562092440647113\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from rouge import Rouge\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = \"train_outputs.json\"\n",
    "json_file_path_llm = \"schlr_LLM_output.json\"\n",
    "\n",
    "# Read the JSON file\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "with open(json_file_path_llm, \"r\") as json_file:\n",
    "    data_llm_out = json.load(json_file)\n",
    "\n",
    "# Now 'data' contains the content of the JSON file as a Python dictionary\n",
    "precision = {'r1':[], 'rl':[]}\n",
    "recall = {'r1':[], 'rl':[]}\n",
    "f1score = {'r1':[], 'rl':[]}\n",
    "for idx, i in enumerate(data['golds'][:100]):\n",
    "    ref = i['output']\n",
    "    pred = data_llm_out['llm_output'][idx]\n",
    "    scores = scorer.score(ref, pred)\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    recall['r1'].append(scores['rouge1'].recall)\n",
    "    precision['r1'].append(scores['rouge1'].precision)\n",
    "    f1score['r1'].append(scores['rouge1'].fmeasure)\n",
    "\n",
    "    recall['rl'].append(scores['rougeL'].recall)\n",
    "    precision['rl'].append(scores['rougeL'].precision)\n",
    "    f1score['rl'].append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "print(\"Schlr set\")\n",
    "print(f\"ROUGE-1: Precision = {np.mean(precision['r1'])}, Recall = {np.mean(recall['r1'])}, F1 = {np.mean(f1score['r1'])}\")\n",
    "print(f\"ROUGE-L: Precision = {np.mean(precision['rl'])}, Recall = {np.mean(recall['rl'])}, F1 = {np.mean(f1score['rl'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ad48b62-2797-4bac-980c-5788e74443b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'dict' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf1score\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/ir_project/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3461\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3462\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 3464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3465\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ir_project/lib/python3.8/site-packages/numpy/core/_methods.py:194\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    192\u001b[0m         ret \u001b[38;5;241m=\u001b[39m ret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype(ret \u001b[38;5;241m/\u001b[39m rcount)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 194\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mret\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrcount\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'dict' and 'int'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8159550-0808-4fcf-9ab7-ff16d4af4d6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m hypothesis \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA fast brown fox jumps over a lazy canine.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize the Rouge object\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# rouge = Rouge()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Compute ROUGE scores\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mscorer\u001b[49m\u001b[38;5;241m.\u001b[39mscore(reference, hypothesis)\n\u001b[1;32m     12\u001b[0m scorer \u001b[38;5;241m=\u001b[39m rouge_scorer\u001b[38;5;241m.\u001b[39mRougeScorer([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrougeL\u001b[39m\u001b[38;5;124m'\u001b[39m], use_stemmer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scorer' is not defined"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Sample reference and hypothesis summaries\n",
    "reference = \"The quick brown fox jumps over the lazy dog.\"\n",
    "hypothesis = \"A fast brown fox jumps over a lazy canine.\"\n",
    "\n",
    "# Initialize the Rouge object\n",
    "# rouge = Rouge()\n",
    "\n",
    "# Compute ROUGE scores\n",
    "scores = scorer.score(reference, hypothesis)\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df5088e5-6dbd-4794-91de-6315b1599889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores:\n",
      "ROUGE-1: Precision = 0.5555555555555556, Recall = 0.5555555555555556, F1 = 0.5555555555555556\n",
      "ROUGE-2: Precision = 0.375, Recall = 0.375, F1 = 0.375\n",
      "ROUGE-L: Precision = 0.5555555555555556, Recall = 0.5555555555555556, F1 = 0.5555555555555556\n"
     ]
    }
   ],
   "source": [
    "print(\"ROUGE Scores:\")\n",
    "print(f\"ROUGE-1: Precision = {scores['rouge1'].precision}, Recall = {scores['rouge1'].recall}, F1 = {scores['rouge1'].fmeasure}\")\n",
    "print(f\"ROUGE-2: Precision = {scores['rouge2'].precision}, Recall = {scores['rouge2'].recall}, F1 = {scores['rouge2'].fmeasure}\")\n",
    "print(f\"ROUGE-L: Precision = {scores['rougeL'].precision}, Recall = {scores['rougeL'].recall}, F1 = {scores['rougeL'].fmeasure}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
